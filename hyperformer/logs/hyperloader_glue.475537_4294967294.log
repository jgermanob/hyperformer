06/13/2024 19:44:47 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
06/13/2024 19:44:47 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
06/13/2024 19:44:47 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
06/13/2024 19:44:47 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
06/13/2024 19:44:47 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='outputs/hyperLoader_glue/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0003, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100, max_steps=65536, warmup_steps=500, logging_dir='runs/Jun13_19-44-41_gpu-05', logging_first_step=True, logging_steps=200, save_steps=1000, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='outputs/hyperLoader_glue/', disable_tqdm=True, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=True, label_smoothing=0.1, predict_with_generate=True, adafactor=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear', temperature=10, train_adapters=True, do_test=True, eval_output_dir=None, generate_classifier_weights=False, optimize_from_scratch=False, optimize_from_scratch_with_loading_model=False, split_validation_test=True, print_num_parameters=True, compute_memory=False, compute_time=False)
06/13/2024 19:44:48 - WARNING - __main__ -   model path loaded from : t5-base
06/13/2024 19:44:49 - WARNING - __main__ -   model path loaded from : t5-base
06/13/2024 19:44:49 - WARNING - __main__ -   model path loaded from : t5-base
06/13/2024 19:44:49 - WARNING - __main__ -   model path loaded from : t5-base
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.rte', 'task_embedding_controller.task_to_embeddings.sst2', 'task_embedding_controller.task_to_embeddings.mrpc', 'task_embedding_controller.task_to_embeddings.stsb', 'task_embedding_controller.task_to_embeddings.qqp', 'task_embedding_controller.task_to_embeddings.mnli', 'task_embedding_controller.task_to_embeddings.qnli', 'task_embedding_controller.task_to_embeddings.cola', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/13/2024 19:44:53 - INFO - __main__ -   T5ForConditionalGeneration(
  (task_embedding_controller): TaskEmbeddingController(
    (task_to_embeddings): ParameterDict(
        (rte): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (sst2): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (mrpc): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (stsb): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (qqp): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (mnli): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (qnli): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (cola): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
    )
  )
  (shared): Embedding(32128, 768)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 768)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (9): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (10): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (11): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (adapter_layers_hyper_net): AdapterLayersOneHyperNetController(
      (layer_id_embeddings): Embedding(12, 64)
      (adapters_block_type): Embedding(6, 64)
      (task_hypernet): TaskHyperNet(
        (task_embeding_generator): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): ReLU()
          (2): Linear(in_features=128, out_features=64, bias=True)
        )
      )
      (LayerNorm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (up_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=768, bias=True)
        )
      )
      (down_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=24, bias=True)
        )
      )
      (lora_a_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (lora_b_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (post_layernorm_hypernet): LayerNormHyperNet(
        (weight_generator): Linear(in_features=64, out_features=768, bias=True)
        (bias_generator): Linear(in_features=64, out_features=768, bias=True)
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(32128, 768)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (9): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (10): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (11): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (adapter_layers_hyper_net): AdapterLayersOneHyperNetController(
      (layer_id_embeddings): Embedding(12, 64)
      (adapters_block_type): Embedding(6, 64)
      (task_hypernet): TaskHyperNet(
        (task_embeding_generator): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): ReLU()
          (2): Linear(in_features=128, out_features=64, bias=True)
        )
      )
      (LayerNorm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (up_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=768, bias=True)
        )
      )
      (down_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=24, bias=True)
        )
      )
      (lora_a_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (lora_b_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (post_layernorm_hypernet): LayerNormHyperNet(
        (weight_generator): Linear(in_features=64, out_features=768, bias=True)
        (bias_generator): Linear(in_features=64, out_features=768, bias=True)
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=768, out_features=32128, bias=False)
)
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.rte
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.sst2
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.mrpc
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.stsb
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.qqp
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.mnli
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.qnli
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.cola
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.0.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.0.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.1.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.1.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.2.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.2.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.3.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.3.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.4.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.4.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.5.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.5.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.6.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.6.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.7.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.7.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.8.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.8.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.9.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.9.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.10.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.10.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.11.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.block.11.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.layer_id_embeddings.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.adapters_block_type.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.LayerNorm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.LayerNorm.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name encoder.final_layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.0.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.0.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.0.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.1.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.1.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.1.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.2.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.2.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.2.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.3.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.3.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.3.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.4.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.4.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.4.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.5.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.5.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.5.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.6.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.6.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.6.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.7.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.7.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.7.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.8.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.8.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.8.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.9.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.9.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.9.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.10.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.10.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.10.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.11.layer.0.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.11.layer.1.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.block.11.layer.2.layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.layer_id_embeddings.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.adapters_block_type.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.LayerNorm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.LayerNorm.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias
06/13/2024 19:44:53 - INFO - __main__ -   Parameter name decoder.final_layer_norm.weight
06/13/2024 19:44:53 - INFO - __main__ -   Total trainable parameters 6784432
06/13/2024 19:44:53 - INFO - __main__ -   Total parameters 229640752
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.rte', 'task_embedding_controller.task_to_embeddings.sst2', 'task_embedding_controller.task_to_embeddings.mrpc', 'task_embedding_controller.task_to_embeddings.stsb', 'task_embedding_controller.task_to_embeddings.qqp', 'task_embedding_controller.task_to_embeddings.mnli', 'task_embedding_controller.task_to_embeddings.qnli', 'task_embedding_controller.task_to_embeddings.cola', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.rte', 'task_embedding_controller.task_to_embeddings.sst2', 'task_embedding_controller.task_to_embeddings.mrpc', 'task_embedding_controller.task_to_embeddings.stsb', 'task_embedding_controller.task_to_embeddings.qqp', 'task_embedding_controller.task_to_embeddings.mnli', 'task_embedding_controller.task_to_embeddings.qnli', 'task_embedding_controller.task_to_embeddings.cola', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.rte', 'task_embedding_controller.task_to_embeddings.sst2', 'task_embedding_controller.task_to_embeddings.mrpc', 'task_embedding_controller.task_to_embeddings.stsb', 'task_embedding_controller.task_to_embeddings.qqp', 'task_embedding_controller.task_to_embeddings.mnli', 'task_embedding_controller.task_to_embeddings.qnli', 'task_embedding_controller.task_to_embeddings.cola', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading:   0%|          | 0.00/7.83k [00:00<?, ?B/s]Downloading: 28.7kB [00:00, 9.03MB/s]                   
Downloading:   0%|          | 0.00/7.83k [00:00<?, ?B/s]Downloading: 28.7kB [00:00, 11.7MB/s]                   
Downloading:   0%|          | 0.00/7.83k [00:00<?, ?B/s]Downloading: 28.7kB [00:00, 18.4MB/s]                   
Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]Downloading: 28.7kB [00:00, 21.0MB/s]                   
Downloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]Downloading: 28.7kB [00:00, 16.5MB/s]                   
Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]Downloading: 28.7kB [00:00, 20.5MB/s]                   
Downloading:   0%|          | 0.00/697k [00:00<?, ?B/s]Downloading:   0%|          | 1.02k/697k [00:00<02:45, 4.20kB/s]Downloading:   5%|▍         | 34.8k/697k [00:00<01:51, 5.92kB/s]Downloading:  15%|█▍        | 104k/697k [00:00<01:10, 8.38kB/s] Downloading:  30%|██▉       | 209k/697k [00:00<00:41, 11.9kB/s]Downloading:  65%|██████▍   | 453k/697k [00:01<00:14, 16.9kB/s]Downloading: 100%|██████████| 697k/697k [00:01<00:00, 565kB/s] 
0 examples [00:00, ? examples/s]                                0 examples [00:00, ? examples/s]                                0 examples [00:00, ? examples/s]                                Dataset glue downloaded and prepared to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.
  0%|          | 0/2490 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/2490 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/2490 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/2490 [00:00<?, ?ex/s]100%|██████████| 2490/2490 [00:00<00:00, 31870.74ex/s]
100%|██████████| 2490/2490 [00:00<00:00, 29633.06ex/s]
100%|██████████| 2490/2490 [00:00<00:00, 29226.64ex/s]
100%|██████████| 2490/2490 [00:00<00:00, 30857.15ex/s]
Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
Downloading:   0%|          | 0.00/7.44M [00:00<?, ?B/s]Downloading:   0%|          | 2.05k/7.44M [00:00<21:32, 5.75kB/s]Downloading:   0%|          | 19.5k/7.44M [00:00<15:48, 7.83kB/s]Downloading:   0%|          | 34.8k/7.44M [00:01<11:53, 10.4kB/s]Downloading:   1%|          | 52.2k/7.44M [00:01<09:03, 13.6kB/s]Downloading:   1%|          | 69.6k/7.44M [00:01<06:50, 18.0kB/s]Downloading:   1%|          | 87.0k/7.44M [00:02<05:31, 22.2kB/s]Downloading:   1%|▏         | 104k/7.44M [00:02<04:21, 28.0kB/s] Downloading:   2%|▏         | 122k/7.44M [00:02<03:32, 34.4kB/s]Downloading:   2%|▏         | 138k/7.44M [00:02<02:44, 44.5kB/s]Downloading:   2%|▏         | 147k/7.44M [00:02<02:23, 50.9kB/s]Downloading:   2%|▏         | 157k/7.44M [00:02<02:08, 56.8kB/s]Downloading:   2%|▏         | 174k/7.44M [00:03<01:59, 60.8kB/s]Downloading:   3%|▎         | 191k/7.44M [00:03<01:53, 64.1kB/s]Downloading:   3%|▎         | 226k/7.44M [00:03<01:33, 77.1kB/s]Downloading:   4%|▎         | 261k/7.44M [00:03<01:19, 89.9kB/s]Downloading:   4%|▍         | 296k/7.44M [00:03<01:05, 109kB/s] Downloading:   5%|▍         | 347k/7.44M [00:04<00:53, 132kB/s]Downloading:   5%|▍         | 366k/7.44M [00:04<00:51, 138kB/s]Downloading:   5%|▌         | 400k/7.44M [00:04<00:50, 140kB/s]Downloading:   6%|▌         | 453k/7.44M [00:04<00:39, 177kB/s]Downloading:   6%|▋         | 478k/7.44M [00:04<00:37, 186kB/s]Downloading:   7%|▋         | 522k/7.44M [00:04<00:31, 219kB/s]Downloading:   7%|▋         | 551k/7.44M [00:04<00:30, 225kB/s]Downloading:   8%|▊         | 592k/7.44M [00:05<00:27, 252kB/s]Downloading:   8%|▊         | 622k/7.44M [00:05<00:27, 251kB/s]Downloading:   9%|▉         | 679k/7.44M [00:05<00:23, 294kB/s]Downloading:  10%|▉         | 714k/7.44M [00:05<00:22, 293kB/s]Downloading:  11%|█         | 783k/7.44M [00:05<00:19, 345kB/s]Downloading:  11%|█         | 824k/7.44M [00:05<00:19, 344kB/s]Downloading:  12%|█▏        | 888k/7.44M [00:05<00:16, 386kB/s]Downloading:  13%|█▎        | 932k/7.44M [00:05<00:17, 380kB/s]Downloading:  14%|█▎        | 1.01M/7.44M [00:06<00:14, 436kB/s]Downloading:  14%|█▍        | 1.06M/7.44M [00:06<00:14, 428kB/s]Downloading:  15%|█▌        | 1.15M/7.44M [00:06<00:12, 493kB/s]Downloading:  16%|█▌        | 1.21M/7.44M [00:06<00:12, 486kB/s]Downloading:  18%|█▊        | 1.31M/7.44M [00:06<00:10, 558kB/s]Downloading:  18%|█▊        | 1.37M/7.44M [00:06<00:11, 548kB/s]Downloading:  20%|█▉        | 1.48M/7.44M [00:06<00:09, 627kB/s]Downloading:  21%|██        | 1.55M/7.44M [00:06<00:09, 614kB/s]Downloading:  22%|██▏       | 1.67M/7.44M [00:07<00:08, 699kB/s]Downloading:  24%|██▎       | 1.75M/7.44M [00:07<00:08, 682kB/s]Downloading:  25%|██▌       | 1.88M/7.44M [00:07<00:07, 772kB/s]Downloading:  26%|██▋       | 1.97M/7.44M [00:07<00:07, 754kB/s]Downloading:  28%|██▊       | 2.11M/7.44M [00:07<00:06, 848kB/s]Downloading:  30%|██▉       | 2.20M/7.44M [00:07<00:06, 824kB/s]Downloading:  32%|███▏      | 2.37M/7.44M [00:07<00:05, 945kB/s]Downloading:  33%|███▎      | 2.47M/7.44M [00:07<00:05, 923kB/s]Downloading:  36%|███▌      | 2.64M/7.44M [00:07<00:04, 1.07MB/s]Downloading:  37%|███▋      | 2.77M/7.44M [00:08<00:04, 1.01MB/s]Downloading:  40%|███▉      | 2.94M/7.44M [00:08<00:04, 1.12MB/s]Downloading:  41%|████      | 3.06M/7.44M [00:08<00:04, 1.09MB/s]Downloading:  44%|████▍     | 3.27M/7.44M [00:08<00:03, 1.23MB/s]Downloading:  46%|████▌     | 3.41M/7.44M [00:08<00:03, 1.19MB/s]Downloading:  49%|████▉     | 3.64M/7.44M [00:08<00:02, 1.35MB/s]Downloading:  51%|█████     | 3.79M/7.44M [00:08<00:02, 1.31MB/s]Downloading:  54%|█████▍    | 4.02M/7.44M [00:08<00:02, 1.47MB/s]Downloading:  56%|█████▌    | 4.18M/7.44M [00:09<00:02, 1.42MB/s]Downloading:  59%|█████▉    | 4.42M/7.44M [00:09<00:01, 1.57MB/s]Downloading:  62%|██████▏   | 4.59M/7.44M [00:09<00:01, 1.51MB/s]Downloading:  66%|██████▌   | 4.87M/7.44M [00:09<00:01, 1.70MB/s]Downloading:  68%|██████▊   | 5.06M/7.44M [00:09<00:01, 1.65MB/s]Downloading:  72%|███████▏  | 5.36M/7.44M [00:09<00:01, 1.85MB/s]Downloading:  75%|███████▍  | 5.56M/7.44M [00:09<00:01, 1.79MB/s]Downloading:  79%|███████▉  | 5.88M/7.44M [00:09<00:00, 2.00MB/s]Downloading:  82%|████████▏ | 6.10M/7.44M [00:09<00:00, 1.93MB/s]Downloading:  87%|████████▋ | 6.46M/7.44M [00:10<00:00, 2.17MB/s]Downloading:  90%|████████▉ | 6.69M/7.44M [00:10<00:00, 2.09MB/s]Downloading:  95%|█████████▍| 7.05M/7.44M [00:10<00:00, 2.32MB/s]Downloading:  98%|█████████▊| 7.30M/7.44M [00:10<00:00, 2.23MB/s]Downloading: 100%|██████████| 7.44M/7.44M [00:10<00:00, 711kB/s] 
0 examples [00:00, ? examples/s]6559 examples [00:00, 65583.18 examples/s]8527 examples [00:00, 38015.58 examples/s]13791 examples [00:00, 41470.97 examples/s]19744 examples [00:00, 45622.30 examples/s]27278 examples [00:00, 51745.51 examples/s]34840 examples [00:00, 57158.72 examples/s]42407 examples [00:00, 61684.75 examples/s]49918 examples [00:00, 65177.92 examples/s]56854 examples [00:00, 66376.50 examples/s]64411 examples [00:01, 68889.22 examples/s]                                           0 examples [00:00, ? examples/s]                                0 examples [00:00, ? examples/s]                                Dataset glue downloaded and prepared to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.
  0%|          | 0/67349 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/67349 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/67349 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/67349 [00:00<?, ?ex/s]  6%|▋         | 4338/67349 [00:00<00:01, 43377.87ex/s]  5%|▍         | 3346/67349 [00:00<00:01, 33458.92ex/s]  6%|▌         | 4021/67349 [00:00<00:01, 40202.95ex/s]  6%|▌         | 3717/67349 [00:00<00:01, 37169.86ex/s] 12%|█▏        | 8230/67349 [00:00<00:01, 41934.43ex/s]  7%|▋         | 4529/67349 [00:00<00:02, 21605.78ex/s] 13%|█▎        | 8606/67349 [00:00<00:01, 41743.37ex/s] 12%|█▏        | 8179/67349 [00:00<00:01, 39127.85ex/s] 19%|█▉        | 12772/67349 [00:00<00:01, 42920.18ex/s] 13%|█▎        | 9087/67349 [00:00<00:02, 25653.73ex/s] 20%|█▉        | 13199/67349 [00:00<00:01, 42915.23ex/s] 19%|█▉        | 12800/67349 [00:00<00:01, 41012.80ex/s] 26%|██▌       | 17271/67349 [00:00<00:01, 43518.54ex/s] 20%|██        | 13613/67349 [00:00<00:01, 29485.55ex/s] 24%|██▍       | 16131/67349 [00:00<00:01, 33034.03ex/s] 23%|██▎       | 15670/67349 [00:00<00:01, 31945.49ex/s] 32%|███▏      | 21677/67349 [00:00<00:01, 43677.83ex/s] 25%|██▌       | 17136/67349 [00:00<00:01, 31000.38ex/s] 31%|███       | 20748/67349 [00:00<00:01, 36115.81ex/s] 30%|███       | 20213/67349 [00:00<00:01, 35067.05ex/s] 39%|███▉      | 26183/67349 [00:00<00:00, 44082.18ex/s] 32%|███▏      | 21513/67349 [00:00<00:01, 33972.62ex/s] 38%|███▊      | 25336/67349 [00:00<00:01, 38577.94ex/s] 37%|███▋      | 24754/67349 [00:00<00:01, 37638.00ex/s] 45%|████▌     | 30562/67349 [00:00<00:00, 43992.96ex/s] 38%|███▊      | 25676/67349 [00:00<00:01, 35955.65ex/s] 44%|████▍     | 29966/67349 [00:00<00:00, 40609.17ex/s] 43%|████▎     | 29138/67349 [00:00<00:00, 39304.20ex/s] 52%|█████▏    | 35053/67349 [00:00<00:00, 44262.25ex/s] 45%|████▍     | 30232/67349 [00:00<00:00, 38382.78ex/s] 51%|█████▏    | 34543/67349 [00:00<00:00, 42029.54ex/s] 50%|█████     | 33678/67349 [00:00<00:00, 40952.64ex/s] 59%|█████▉    | 39576/67349 [00:00<00:00, 44547.99ex/s] 52%|█████▏    | 34787/67349 [00:00<00:00, 40283.56ex/s] 58%|█████▊    | 39000/67349 [00:00<00:00, 42715.14ex/s] 57%|█████▋    | 38204/67349 [00:00<00:00, 42153.92ex/s] 65%|██████▌   | 44067/67349 [00:01<00:00, 44654.82ex/s] 58%|█████▊    | 39318/67349 [00:01<00:00, 41667.60ex/s] 65%|██████▍   | 43607/67349 [00:01<00:00, 43668.96ex/s] 63%|██████▎   | 42748/67349 [00:01<00:00, 43088.36ex/s] 72%|███████▏  | 48582/67349 [00:01<00:00, 44800.27ex/s] 65%|██████▌   | 43869/67349 [00:01<00:00, 42747.02ex/s] 72%|███████▏  | 48187/67349 [00:01<00:00, 44286.21ex/s] 70%|███████   | 47258/67349 [00:01<00:00, 43672.53ex/s] 79%|███████▊  | 52978/67349 [00:01<00:00, 44070.97ex/s] 72%|███████▏  | 48385/67349 [00:01<00:00, 43441.14ex/s] 78%|███████▊  | 52784/67349 [00:01<00:00, 44776.30ex/s] 77%|███████▋  | 51802/67349 [00:01<00:00, 44186.64ex/s] 85%|████████▌ | 57329/67349 [00:01<00:00, 43446.63ex/s] 79%|███████▊  | 52914/67349 [00:01<00:00, 43978.75ex/s] 85%|████████▌ | 57325/67349 [00:01<00:00, 44963.99ex/s] 83%|████████▎ | 56219/67349 [00:01<00:00, 43480.00ex/s] 92%|█████████▏| 61799/67349 [00:01<00:00, 43812.38ex/s] 85%|████████▌ | 57423/67349 [00:01<00:00, 44304.48ex/s] 92%|█████████▏| 61816/67349 [00:01<00:00, 44461.74ex/s] 98%|█████████▊| 66250/67349 [00:01<00:00, 44016.82ex/s] 92%|█████████▏| 61877/67349 [00:01<00:00, 43591.28ex/s] 90%|████████▉ | 60568/67349 [00:01<00:00, 40860.73ex/s]100%|██████████| 67349/67349 [00:01<00:00, 43963.89ex/s]
train
 99%|█████████▊| 66366/67349 [00:01<00:00, 44766.55ex/s]100%|██████████| 67349/67349 [00:01<00:00, 43107.56ex/s]
 97%|█████████▋| 65051/67349 [00:01<00:00, 41974.55ex/s] 98%|█████████▊| 66255/67349 [00:01<00:00, 42513.40ex/s]100%|██████████| 67349/67349 [00:01<00:00, 41048.11ex/s]
100%|██████████| 67349/67349 [00:01<00:00, 41720.88ex/s]
train
train
train
Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
Downloading: 0.00B [00:00, ?B/s]Downloading: 6.22kB [00:00, 4.08MB/s]
Downloading: 0.00B [00:00, ?B/s]Downloading: 21.6kB [00:00, 88.7kB/s]Downloading: 54.4kB [00:00, 98.8kB/s]Downloading: 136kB [00:00, 125kB/s]  Downloading: 316kB [00:00, 167kB/s]Downloading: 651kB [00:01, 227kB/s]Downloading: 1.05MB [00:01, 841kB/s]
Downloading: 0.00B [00:00, ?B/s]Downloading: 2.05kB [00:00, 8.70kB/s]Downloading: 37.0kB [00:00, 12.1kB/s]Downloading: 106kB [00:00, 17.0kB/s] Downloading: 211kB [00:00, 23.9kB/s]Downloading: 441kB [00:01, 435kB/s] 
0 examples [00:00, ? examples/s]3173 examples [00:00, 31729.73 examples/s]                                          0 examples [00:00, ? examples/s]                                0 examples [00:00, ? examples/s]                                Dataset glue downloaded and prepared to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.
  0%|          | 0/3668 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/3668 [00:00<?, ?ex/s]  0%|          | 0/3668 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/3668 [00:00<?, ?ex/s] 87%|████████▋ | 3194/3668 [00:00<00:00, 31932.95ex/s] 82%|████████▏ | 3000/3668 [00:00<00:00, 29892.27ex/s]100%|██████████| 3668/3668 [00:00<00:00, 32477.81ex/s]
 81%|████████  | 2971/3668 [00:00<00:00, 29706.70ex/s]100%|██████████| 3668/3668 [00:00<00:00, 31076.51ex/s]
100%|██████████| 3668/3668 [00:00<00:00, 30615.42ex/s]
 73%|███████▎  | 2692/3668 [00:00<00:00, 26911.88ex/s]100%|██████████| 3668/3668 [00:00<00:00, 27705.22ex/s]
Downloading and preparing dataset glue/stsb (download: 784.05 KiB, generated: 1.09 MiB, post-processed: Unknown size, total: 1.86 MiB) to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
Downloading:   0%|          | 0.00/803k [00:00<?, ?B/s]Downloading:   0%|          | 2.05k/803k [00:00<01:33, 8.61kB/s]Downloading:   4%|▍         | 34.8k/803k [00:00<01:04, 12.0kB/s]Downloading:  13%|█▎        | 104k/803k [00:00<00:41, 16.8kB/s] Downloading:  26%|██▌       | 209k/803k [00:00<00:25, 23.6kB/s]Downloading:  56%|█████▋    | 453k/803k [00:01<00:10, 33.4kB/s]Downloading: 100%|██████████| 803k/803k [00:01<00:00, 669kB/s] 
0 examples [00:00, ? examples/s]4932 examples [00:00, 49315.70 examples/s]                                          0 examples [00:00, ? examples/s]                                0 examples [00:00, ? examples/s]                                Dataset glue downloaded and prepared to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.
  0%|          | 0/5749 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/5749 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/5749 [00:00<?, ?ex/s]  0%|          | 0/5749 [00:00<?, ?ex/s] 32%|███▏      | 1850/5749 [00:00<00:00, 18498.74ex/s] 37%|███▋      | 2119/5749 [00:00<00:00, 21182.85ex/s] 35%|███▌      | 2021/5749 [00:00<00:00, 20204.09ex/s] 39%|███▊      | 2227/5749 [00:00<00:00, 22265.40ex/s] 75%|███████▍  | 4294/5749 [00:00<00:00, 19953.23ex/s] 79%|███████▉  | 4550/5749 [00:00<00:00, 22032.48ex/s] 78%|███████▊  | 4458/5749 [00:00<00:00, 21295.96ex/s] 81%|████████  | 4657/5749 [00:00<00:00, 22838.72ex/s]100%|██████████| 5749/5749 [00:00<00:00, 22208.42ex/s]
100%|██████████| 5749/5749 [00:00<00:00, 22590.73ex/s]
100%|██████████| 5749/5749 [00:00<00:00, 23448.09ex/s]
100%|██████████| 5749/5749 [00:00<00:00, 22725.03ex/s]
Downloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
Downloading:   0%|          | 0.00/41.7M [00:00<?, ?B/s]Downloading:   0%|          | 17.4k/41.7M [00:00<09:21, 74.3kB/s]Downloading:   0%|          | 34.8k/41.7M [00:00<09:22, 74.0kB/s]Downloading:   0%|          | 69.6k/41.7M [00:00<07:58, 86.9kB/s]Downloading:   0%|          | 104k/41.7M [00:00<06:59, 99.1kB/s] Downloading:   0%|          | 126k/41.7M [00:01<07:11, 96.3kB/s]Downloading:   0%|          | 159k/41.7M [00:01<06:31, 106kB/s] Downloading:   0%|          | 198k/41.7M [00:01<05:49, 119kB/s]Downloading:   1%|          | 230k/41.7M [00:01<05:34, 124kB/s]Downloading:   1%|          | 279k/41.7M [00:02<04:55, 140kB/s]Downloading:   1%|          | 314k/41.7M [00:02<04:48, 144kB/s]Downloading:   1%|          | 359k/41.7M [00:02<04:27, 155kB/s]Downloading:   1%|          | 403k/41.7M [00:02<03:44, 184kB/s]Downloading:   1%|          | 425k/41.7M [00:02<03:36, 191kB/s]Downloading:   1%|          | 450k/41.7M [00:03<04:30, 153kB/s]Downloading:   1%|          | 492k/41.7M [00:03<04:19, 159kB/s]Downloading:   1%|▏         | 540k/41.7M [00:03<04:01, 170kB/s]Downloading:   1%|▏         | 601k/41.7M [00:03<03:15, 210kB/s]Downloading:   2%|▏         | 629k/41.7M [00:03<03:03, 224kB/s]Downloading:   2%|▏         | 656k/41.7M [00:04<03:53, 176kB/s]Downloading:   2%|▏         | 711k/41.7M [00:04<03:13, 212kB/s]Downloading:   2%|▏         | 739k/41.7M [00:04<02:59, 228kB/s]Downloading:   2%|▏         | 782k/41.7M [00:04<03:12, 212kB/s]Downloading:   2%|▏         | 863k/41.7M [00:04<02:50, 239kB/s]Downloading:   2%|▏         | 935k/41.7M [00:04<02:22, 286kB/s]Downloading:   2%|▏         | 971k/41.7M [00:04<02:14, 304kB/s]Downloading:   3%|▎         | 1.04M/41.7M [00:05<02:13, 305kB/s]Downloading:   3%|▎         | 1.14M/41.7M [00:05<02:03, 329kB/s]Downloading:   3%|▎         | 1.26M/41.7M [00:05<01:39, 406kB/s]Downloading:   3%|▎         | 1.31M/41.7M [00:05<01:33, 432kB/s]Downloading:   3%|▎         | 1.39M/41.7M [00:05<01:40, 399kB/s]Downloading:   4%|▎         | 1.53M/41.7M [00:06<01:30, 442kB/s]Downloading:   4%|▍         | 1.69M/41.7M [00:06<01:13, 542kB/s]Downloading:   4%|▍         | 1.76M/41.7M [00:06<01:09, 577kB/s]Downloading:   4%|▍         | 1.86M/41.7M [00:06<01:03, 625kB/s]Downloading:   5%|▍         | 1.93M/41.7M [00:06<01:01, 646kB/s]Downloading:   5%|▍         | 2.05M/41.7M [00:06<00:56, 704kB/s]Downloading:   5%|▌         | 2.13M/41.7M [00:06<00:55, 716kB/s]Downloading:   5%|▌         | 2.26M/41.7M [00:07<00:50, 780kB/s]Downloading:   6%|▌         | 2.34M/41.7M [00:07<00:49, 788kB/s]Downloading:   6%|▌         | 2.49M/41.7M [00:07<00:45, 855kB/s]Downloading:   6%|▌         | 2.58M/41.7M [00:07<00:45, 862kB/s]Downloading:   7%|▋         | 2.75M/41.7M [00:07<00:40, 952kB/s]Downloading:   7%|▋         | 2.85M/41.7M [00:07<00:40, 961kB/s]Downloading:   7%|▋         | 3.01M/41.7M [00:07<00:37, 1.02MB/s]Downloading:   7%|▋         | 3.12M/41.7M [00:07<00:37, 1.03MB/s]Downloading:   8%|▊         | 3.31M/41.7M [00:07<00:34, 1.12MB/s]Downloading:   8%|▊         | 3.42M/41.7M [00:08<00:34, 1.12MB/s]Downloading:   9%|▊         | 3.64M/41.7M [00:08<00:30, 1.23MB/s]Downloading:   9%|▉         | 3.77M/41.7M [00:08<00:30, 1.24MB/s]Downloading:  10%|▉         | 3.99M/41.7M [00:08<00:28, 1.34MB/s]Downloading:  10%|▉         | 4.12M/41.7M [00:08<00:28, 1.34MB/s]Downloading:  10%|█         | 4.37M/41.7M [00:08<00:25, 1.46MB/s]Downloading:  11%|█         | 4.52M/41.7M [00:08<00:25, 1.46MB/s]Downloading:  11%|█▏        | 4.77M/41.7M [00:08<00:23, 1.56MB/s]Downloading:  12%|█▏        | 4.93M/41.7M [00:09<00:23, 1.56MB/s]Downloading:  13%|█▎        | 5.22M/41.7M [00:09<00:21, 1.70MB/s]Downloading:  13%|█▎        | 5.40M/41.7M [00:09<00:21, 1.70MB/s]Downloading:  14%|█▎        | 5.69M/41.7M [00:09<00:19, 1.83MB/s]Downloading:  14%|█▍        | 5.88M/41.7M [00:09<00:19, 1.83MB/s]Downloading:  15%|█▍        | 6.20M/41.7M [00:09<00:18, 1.96MB/s]Downloading:  15%|█▌        | 6.40M/41.7M [00:09<00:18, 1.92MB/s]Downloading:  16%|█▌        | 6.74M/41.7M [00:09<00:16, 2.10MB/s]Downloading:  17%|█▋        | 6.95M/41.7M [00:09<00:16, 2.10MB/s]Downloading:  18%|█▊        | 7.33M/41.7M [00:10<00:15, 2.27MB/s]Downloading:  18%|█▊        | 7.56M/41.7M [00:10<00:15, 2.27MB/s]Downloading:  19%|█▉        | 7.95M/41.7M [00:10<00:13, 2.43MB/s]Downloading:  20%|█▉        | 8.20M/41.7M [00:10<00:13, 2.43MB/s]Downloading:  21%|██        | 8.62M/41.7M [00:10<00:12, 2.61MB/s]Downloading:  21%|██▏       | 8.89M/41.7M [00:10<00:12, 2.60MB/s]Downloading:  22%|██▏       | 9.35M/41.7M [00:10<00:11, 2.80MB/s]Downloading:  23%|██▎       | 9.63M/41.7M [00:10<00:11, 2.78MB/s]Downloading:  24%|██▍       | 10.1M/41.7M [00:11<00:10, 2.98MB/s]Downloading:  25%|██▍       | 10.4M/41.7M [00:11<00:10, 2.95MB/s]Downloading:  26%|██▌       | 10.9M/41.7M [00:11<00:09, 3.18MB/s]Downloading:  27%|██▋       | 11.2M/41.7M [00:11<00:09, 3.16MB/s]Downloading:  28%|██▊       | 11.8M/41.7M [00:11<00:08, 3.37MB/s]Downloading:  29%|██▉       | 12.1M/41.7M [00:11<00:08, 3.34MB/s]Downloading:  30%|███       | 12.7M/41.7M [00:11<00:08, 3.59MB/s]Downloading:  31%|███▏      | 13.1M/41.7M [00:11<00:08, 3.56MB/s]Downloading:  33%|███▎      | 13.7M/41.7M [00:11<00:07, 3.83MB/s]Downloading:  34%|███▎      | 14.1M/41.7M [00:12<00:07, 3.80MB/s]Downloading:  35%|███▌      | 14.7M/41.7M [00:12<00:06, 4.06MB/s]Downloading:  36%|███▋      | 15.1M/41.7M [00:12<00:06, 4.03MB/s]Downloading:  38%|███▊      | 15.8M/41.7M [00:12<00:06, 4.29MB/s]Downloading:  39%|███▉      | 16.2M/41.7M [00:12<00:05, 4.25MB/s]Downloading:  41%|████      | 16.9M/41.7M [00:12<00:05, 4.56MB/s]Downloading:  42%|████▏     | 17.4M/41.7M [00:12<00:05, 4.52MB/s]Downloading:  44%|████▎     | 18.2M/41.7M [00:12<00:04, 4.82MB/s]Downloading:  45%|████▍     | 18.7M/41.7M [00:13<00:04, 4.75MB/s]Downloading:  47%|████▋     | 19.4M/41.7M [00:13<00:04, 5.08MB/s]Downloading:  48%|████▊     | 20.0M/41.7M [00:13<00:04, 4.99MB/s]Downloading:  50%|████▉     | 20.8M/41.7M [00:13<00:03, 5.37MB/s]Downloading:  51%|█████     | 21.3M/41.7M [00:13<00:03, 5.28MB/s]Downloading:  53%|█████▎    | 22.2M/41.7M [00:13<00:03, 5.68MB/s]Downloading:  55%|█████▍    | 22.8M/41.7M [00:13<00:03, 5.60MB/s]Downloading:  57%|█████▋    | 23.7M/41.7M [00:13<00:03, 5.97MB/s]Downloading:  58%|█████▊    | 24.3M/41.7M [00:13<00:02, 5.86MB/s]Downloading:  61%|██████    | 25.3M/41.7M [00:14<00:02, 6.29MB/s]Downloading:  62%|██████▏   | 26.0M/41.7M [00:14<00:02, 6.19MB/s]Downloading:  65%|██████▍   | 27.0M/41.7M [00:14<00:02, 6.63MB/s]Downloading:  66%|██████▋   | 27.7M/41.7M [00:14<00:02, 6.50MB/s]Downloading:  69%|██████▉   | 28.7M/41.7M [00:14<00:01, 6.96MB/s]Downloading:  71%|███████   | 29.4M/41.7M [00:14<00:01, 6.81MB/s]Downloading:  73%|███████▎  | 30.6M/41.7M [00:14<00:01, 7.31MB/s]Downloading:  75%|███████▌  | 31.3M/41.7M [00:14<00:01, 7.14MB/s]Downloading:  78%|███████▊  | 32.5M/41.7M [00:15<00:01, 7.69MB/s]Downloading:  80%|███████▉  | 33.3M/41.7M [00:15<00:01, 7.51MB/s]Downloading:  83%|████████▎ | 34.5M/41.7M [00:15<00:00, 8.08MB/s]Downloading:  85%|████████▍ | 35.4M/41.7M [00:15<00:00, 7.88MB/s]Downloading:  88%|████████▊ | 36.7M/41.7M [00:15<00:00, 8.49MB/s]Downloading:  90%|█████████ | 37.5M/41.7M [00:15<00:00, 8.28MB/s]Downloading:  93%|█████████▎| 38.9M/41.7M [00:15<00:00, 8.91MB/s]Downloading:  95%|█████████▌| 39.8M/41.7M [00:15<00:00, 8.71MB/s]Downloading:  99%|█████████▉| 41.2M/41.7M [00:16<00:00, 9.36MB/s]Downloading: 100%|██████████| 41.7M/41.7M [00:16<00:00, 2.59MB/s]
0 examples [00:00, ? examples/s]4864 examples [00:00, 48631.81 examples/s]10271 examples [00:00, 50144.28 examples/s]16072 examples [00:00, 52270.41 examples/s]20906 examples [00:00, 51023.56 examples/s]26231 examples [00:00, 51671.63 examples/s]31589 examples [00:00, 52229.58 examples/s]37299 examples [00:00, 53597.45 examples/s]42846 examples [00:00, 54144.72 examples/s]48655 examples [00:00, 55270.13 examples/s]54144 examples [00:01, 55154.80 examples/s]59840 examples [00:01, 55683.86 examples/s]65382 examples [00:01, 55602.62 examples/s]70882 examples [00:01, 54706.58 examples/s]76434 examples [00:01, 54947.83 examples/s]81977 examples [00:01, 55090.28 examples/s]87534 examples [00:01, 55231.65 examples/s]93061 examples [00:01, 55240.64 examples/s]98794 examples [00:01, 55848.61 examples/s]104375 examples [00:01, 54261.79 examples/s]110000 examples [00:02, 54560.95 examples/s]115462 examples [00:02, 54126.52 examples/s]120879 examples [00:02, 53111.17 examples/s]126666 examples [00:02, 54452.01 examples/s]132200 examples [00:02, 54713.13 examples/s]137682 examples [00:02, 54340.33 examples/s]143252 examples [00:02, 54738.72 examples/s]149069 examples [00:02, 55724.39 examples/s]154691 examples [00:02, 55870.27 examples/s]160285 examples [00:02, 54560.89 examples/s]165753 examples [00:03, 54448.82 examples/s]171206 examples [00:03, 50741.78 examples/s]177075 examples [00:03, 52889.31 examples/s]182638 examples [00:03, 53681.79 examples/s]188422 examples [00:03, 54864.18 examples/s]194057 examples [00:03, 55299.90 examples/s]199895 examples [00:03, 56186.45 examples/s]205537 examples [00:03, 56160.36 examples/s]211170 examples [00:03, 54687.32 examples/s]216659 examples [00:03, 52952.90 examples/s]222231 examples [00:04, 53752.30 examples/s]228047 examples [00:04, 55001.14 examples/s]233647 examples [00:04, 55296.23 examples/s]239193 examples [00:04, 53468.02 examples/s]244800 examples [00:04, 54221.41 examples/s]250242 examples [00:04, 53312.79 examples/s]255591 examples [00:04, 52858.59 examples/s]261150 examples [00:04, 53649.27 examples/s]266987 examples [00:04, 54982.19 examples/s]272565 examples [00:05, 55218.95 examples/s]278350 examples [00:05, 55980.51 examples/s]283959 examples [00:05, 55812.21 examples/s]289548 examples [00:05, 55212.86 examples/s]295077 examples [00:05, 55076.25 examples/s]300590 examples [00:05, 53927.15 examples/s]306444 examples [00:05, 55231.80 examples/s]312045 examples [00:05, 55460.68 examples/s]317700 examples [00:05, 55779.91 examples/s]323301 examples [00:05, 55847.84 examples/s]329171 examples [00:06, 56671.58 examples/s]334845 examples [00:06, 55116.00 examples/s]340371 examples [00:06, 49213.56 examples/s]346202 examples [00:06, 51623.30 examples/s]351793 examples [00:06, 52838.43 examples/s]357596 examples [00:06, 54294.71 examples/s]363178 examples [00:06, 54740.91 examples/s]                                            0 examples [00:00, ? examples/s]5325 examples [00:00, 53243.20 examples/s]10794 examples [00:00, 53668.32 examples/s]16591 examples [00:00, 54888.87 examples/s]22144 examples [00:00, 55076.93 examples/s]27489 examples [00:00, 54577.64 examples/s]32604 examples [00:00, 53500.40 examples/s]37484 examples [00:00, 51995.75 examples/s]                                           0 examples [00:00, ? examples/s]5341 examples [00:00, 53406.10 examples/s]11156 examples [00:00, 54745.35 examples/s]17321 examples [00:00, 56647.57 examples/s]23226 examples [00:00, 57345.26 examples/s]29372 examples [00:00, 58519.23 examples/s]34824 examples [00:00, 57256.88 examples/s]40755 examples [00:00, 57857.46 examples/s]46960 examples [00:00, 59054.28 examples/s]52868 examples [00:00, 59059.86 examples/s]59076 examples [00:01, 59933.12 examples/s]64930 examples [00:01, 56941.96 examples/s]70855 examples [00:01, 57614.76 examples/s]77074 examples [00:01, 58912.75 examples/s]82942 examples [00:01, 57559.52 examples/s]89103 examples [00:01, 58715.83 examples/s]95047 examples [00:01, 58928.65 examples/s]100940 examples [00:01, 58501.44 examples/s]107097 examples [00:01, 59388.03 examples/s]113040 examples [00:01, 59322.89 examples/s]119202 examples [00:02, 59992.68 examples/s]125205 examples [00:02, 59870.09 examples/s]131195 examples [00:02, 58200.66 examples/s]137041 examples [00:02, 58275.21 examples/s]142976 examples [00:02, 58592.68 examples/s]149147 examples [00:02, 59492.92 examples/s]155111 examples [00:02, 59535.58 examples/s]161070 examples [00:02, 59307.77 examples/s]167266 examples [00:02, 60078.68 examples/s]173280 examples [00:02, 59228.57 examples/s]179210 examples [00:03, 57678.84 examples/s]185148 examples [00:03, 58177.42 examples/s]191072 examples [00:03, 58491.73 examples/s]197322 examples [00:03, 59638.09 examples/s]203310 examples [00:03, 59707.92 examples/s]209480 examples [00:03, 60290.70 examples/s]215516 examples [00:03, 58435.10 examples/s]221403 examples [00:03, 58563.00 examples/s]227272 examples [00:03, 58130.44 examples/s]233095 examples [00:03, 54317.60 examples/s]238924 examples [00:04, 55449.52 examples/s]244850 examples [00:04, 56538.98 examples/s]250788 examples [00:04, 57361.11 examples/s]256989 examples [00:04, 58680.56 examples/s]262945 examples [00:04, 58940.92 examples/s]269147 examples [00:04, 59829.93 examples/s]275148 examples [00:04, 57003.04 examples/s]280889 examples [00:04, 56616.39 examples/s]287110 examples [00:04, 58183.45 examples/s]293028 examples [00:05, 58476.86 examples/s]299212 examples [00:05, 59445.71 examples/s]305176 examples [00:05, 56149.04 examples/s]311052 examples [00:05, 56906.86 examples/s]316782 examples [00:05, 56863.28 examples/s]322496 examples [00:05, 56534.41 examples/s]328680 examples [00:05, 58026.20 examples/s]334625 examples [00:05, 58444.21 examples/s]340487 examples [00:05, 57878.67 examples/s]346289 examples [00:05, 57574.74 examples/s]352154 examples [00:06, 57893.05 examples/s]358325 examples [00:06, 58985.50 examples/s]364234 examples [00:06, 56714.16 examples/s]370000 examples [00:06, 56755.62 examples/s]376137 examples [00:06, 58063.76 examples/s]381992 examples [00:06, 58206.81 examples/s]388116 examples [00:06, 59084.31 examples/s]                                            Dataset glue downloaded and prepared to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/363846 [00:00<?, ?ex/s]  0%|          | 0/363846 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/363846 [00:00<?, ?ex/s]  0%|          | 0/363846 [00:00<?, ?ex/s]  1%|          | 2611/363846 [00:00<00:13, 26103.49ex/s]  1%|          | 3186/363846 [00:00<00:11, 31859.57ex/s]  1%|          | 3237/363846 [00:00<00:11, 32368.26ex/s]  1%|          | 3213/363846 [00:00<00:11, 32125.28ex/s]  2%|▏         | 6227/363846 [00:00<00:12, 28479.30ex/s]  2%|▏         | 6874/363846 [00:00<00:10, 33214.71ex/s]  2%|▏         | 6789/363846 [00:00<00:10, 33251.32ex/s]  2%|▏         | 6882/363846 [00:00<00:10, 33369.61ex/s]  3%|▎         | 9912/363846 [00:00<00:11, 30561.94ex/s]  3%|▎         | 10547/363846 [00:00<00:10, 34194.50ex/s]  3%|▎         | 10480/363846 [00:00<00:10, 34268.07ex/s]  3%|▎         | 10448/363846 [00:00<00:10, 34023.65ex/s]  4%|▎         | 13553/363846 [00:00<00:10, 32108.48ex/s]  4%|▍         | 14215/363846 [00:00<00:10, 34902.56ex/s]  4%|▍         | 14162/363846 [00:00<00:09, 34993.85ex/s]  4%|▍         | 14127/363846 [00:00<00:10, 34807.58ex/s]  5%|▍         | 17183/363846 [00:00<00:10, 33258.52ex/s]  5%|▍         | 17901/363846 [00:00<00:09, 35466.47ex/s]  5%|▍         | 17857/363846 [00:00<00:09, 35557.48ex/s]  5%|▍         | 17823/363846 [00:00<00:09, 35423.93ex/s]  6%|▌         | 20827/363846 [00:00<00:10, 34151.11ex/s]  6%|▌         | 20926/363846 [00:00<00:10, 33646.94ex/s]  6%|▌         | 21529/363846 [00:00<00:09, 35898.14ex/s]  6%|▌         | 21488/363846 [00:00<00:09, 35782.96ex/s]  7%|▋         | 24440/363846 [00:00<00:09, 34720.47ex/s]  7%|▋         | 24588/363846 [00:00<00:09, 34484.38ex/s]  7%|▋         | 25138/363846 [00:00<00:09, 35994.50ex/s]  7%|▋         | 24747/363846 [00:00<00:10, 33498.22ex/s]  8%|▊         | 27785/363846 [00:00<00:09, 34326.57ex/s]  8%|▊         | 28257/363846 [00:00<00:09, 35117.70ex/s]  8%|▊         | 28813/363846 [00:00<00:09, 36217.34ex/s]  8%|▊         | 28426/363846 [00:00<00:09, 34419.72ex/s]  9%|▊         | 31397/363846 [00:00<00:09, 34843.68ex/s]  9%|▉         | 31938/363846 [00:00<00:09, 35607.28ex/s]  9%|▉         | 32458/363846 [00:00<00:09, 36285.72ex/s]  9%|▉         | 32111/363846 [00:00<00:09, 35112.66ex/s] 10%|▉         | 35004/363846 [00:01<00:09, 35200.70ex/s] 10%|▉         | 35453/363846 [00:01<00:09, 35467.79ex/s] 10%|▉         | 36103/363846 [00:01<00:09, 36333.44ex/s] 10%|▉         | 35806/363846 [00:01<00:09, 35643.09ex/s] 11%|█         | 38611/363846 [00:01<00:09, 35456.55ex/s] 11%|█         | 38922/363846 [00:01<00:09, 35160.05ex/s] 11%|█         | 39776/363846 [00:01<00:08, 36449.42ex/s] 11%|█         | 39485/363846 [00:01<00:09, 35978.18ex/s] 12%|█▏        | 42209/363846 [00:01<00:09, 35609.97ex/s] 12%|█▏        | 42558/363846 [00:01<00:09, 35509.46ex/s] 12%|█▏        | 43387/363846 [00:01<00:08, 36345.05ex/s] 12%|█▏        | 43034/363846 [00:01<00:08, 35781.89ex/s] 13%|█▎        | 45743/363846 [00:01<00:09, 35173.73ex/s] 13%|█▎        | 46197/363846 [00:01<00:08, 35768.93ex/s] 13%|█▎        | 46979/363846 [00:01<00:08, 36058.16ex/s] 13%|█▎        | 46701/363846 [00:01<00:08, 36041.78ex/s] 14%|█▎        | 49337/363846 [00:01<00:08, 35399.89ex/s] 14%|█▎        | 49843/363846 [00:01<00:08, 35972.68ex/s] 14%|█▍        | 50601/363846 [00:01<00:08, 36106.54ex/s] 14%|█▍        | 50359/363846 [00:01<00:08, 36198.95ex/s] 15%|█▍        | 52943/363846 [00:01<00:08, 35593.10ex/s] 15%|█▍        | 53458/363846 [00:01<00:08, 36023.16ex/s] 15%|█▍        | 54212/363846 [00:01<00:08, 36104.85ex/s] 15%|█▍        | 54000/363846 [00:01<00:08, 36222.95ex/s] 16%|█▌        | 56522/363846 [00:01<00:08, 35650.40ex/s] 16%|█▌        | 57089/363846 [00:01<00:08, 36106.36ex/s] 16%|█▌        | 57864/363846 [00:01<00:08, 36225.84ex/s] 16%|█▌        | 57660/363846 [00:01<00:08, 36334.10ex/s] 17%|█▋        | 60085/363846 [00:01<00:08, 35643.09ex/s] 17%|█▋        | 60721/363846 [00:01<00:08, 36168.37ex/s] 17%|█▋        | 61477/363846 [00:01<00:08, 36188.52ex/s] 17%|█▋        | 61288/363846 [00:01<00:08, 36315.08ex/s] 18%|█▊        | 63674/363846 [00:01<00:08, 35716.54ex/s] 18%|█▊        | 64332/363846 [00:01<00:08, 36136.85ex/s] 18%|█▊        | 65089/363846 [00:01<00:08, 36138.20ex/s] 18%|█▊        | 64929/363846 [00:01<00:08, 36342.55ex/s] 19%|█▊        | 67963/363846 [00:01<00:08, 36188.17ex/s] 18%|█▊        | 67243/363846 [00:01<00:08, 35384.68ex/s] 19%|█▉        | 68698/363846 [00:01<00:08, 36095.70ex/s] 19%|█▉        | 68560/363846 [00:01<00:08, 36312.34ex/s] 20%|█▉        | 71579/363846 [00:02<00:08, 35811.19ex/s] 19%|█▉        | 70781/363846 [00:02<00:08, 34879.10ex/s] 20%|█▉        | 72305/363846 [00:02<00:08, 36000.74ex/s] 20%|█▉        | 72189/363846 [00:02<00:08, 36290.82ex/s] 21%|██        | 75173/363846 [00:02<00:08, 35849.62ex/s] 20%|██        | 74321/363846 [00:02<00:08, 35033.37ex/s] 21%|██        | 75910/363846 [00:02<00:07, 36015.00ex/s] 21%|██        | 75817/363846 [00:02<00:07, 36259.67ex/s] 22%|██▏       | 78786/363846 [00:02<00:07, 35931.52ex/s] 21%|██▏       | 77826/363846 [00:02<00:08, 34649.40ex/s] 22%|██▏       | 79442/363846 [00:02<00:07, 36238.19ex/s] 22%|██▏       | 79510/363846 [00:02<00:08, 35445.54ex/s] 23%|██▎       | 82379/363846 [00:02<00:07, 35894.28ex/s] 22%|██▏       | 81369/363846 [00:02<00:08, 34878.50ex/s] 23%|██▎       | 83065/363846 [00:02<00:07, 36220.22ex/s] 23%|██▎       | 83091/363846 [00:02<00:07, 35553.85ex/s] 24%|██▎       | 85986/363846 [00:02<00:07, 35946.01ex/s] 23%|██▎       | 84908/363846 [00:02<00:07, 35030.28ex/s] 24%|██▍       | 86687/363846 [00:02<00:07, 36204.50ex/s] 24%|██▍       | 86688/363846 [00:02<00:07, 35676.38ex/s] 25%|██▍       | 89581/363846 [00:02<00:07, 35880.83ex/s] 24%|██▍       | 88416/363846 [00:02<00:07, 35044.26ex/s] 25%|██▍       | 90308/363846 [00:02<00:07, 36090.06ex/s] 25%|██▍       | 90273/363846 [00:02<00:07, 35727.25ex/s] 26%|██▌       | 93169/363846 [00:02<00:07, 35797.55ex/s] 25%|██▌       | 91974/363846 [00:02<00:07, 35202.55ex/s] 26%|██▌       | 93917/363846 [00:02<00:07, 36080.63ex/s] 26%|██▌       | 93847/363846 [00:02<00:07, 35648.98ex/s] 27%|██▋       | 96752/363846 [00:02<00:07, 35806.45ex/s] 26%|██▌       | 95495/363846 [00:02<00:07, 35165.08ex/s] 27%|██▋       | 97525/363846 [00:02<00:07, 36024.00ex/s] 27%|██▋       | 97413/363846 [00:02<00:07, 35490.25ex/s] 28%|██▊       | 100333/363846 [00:02<00:07, 35777.61ex/s] 27%|██▋       | 99013/363846 [00:02<00:07, 34561.12ex/s] 28%|██▊       | 101128/363846 [00:02<00:07, 35311.36ex/s] 28%|██▊       | 100963/363846 [00:02<00:07, 34910.91ex/s] 29%|██▊       | 103911/363846 [00:02<00:07, 34774.79ex/s] 28%|██▊       | 102550/363846 [00:02<00:07, 34797.94ex/s] 29%|██▉       | 104732/363846 [00:02<00:07, 35524.79ex/s] 29%|██▊       | 104520/363846 [00:02<00:07, 35103.79ex/s] 30%|██▉       | 107457/363846 [00:03<00:07, 34975.44ex/s] 29%|██▉       | 106033/363846 [00:03<00:07, 33809.12ex/s] 30%|██▉       | 108295/363846 [00:03<00:07, 35554.19ex/s] 30%|██▉       | 108081/363846 [00:03<00:07, 35252.92ex/s] 31%|███       | 110990/363846 [00:03<00:07, 35080.87ex/s] 30%|███       | 109565/363846 [00:03<00:07, 34247.22ex/s] 31%|███       | 111887/363846 [00:03<00:07, 35660.93ex/s] 31%|███       | 111622/363846 [00:03<00:07, 35299.63ex/s] 31%|███▏      | 114502/363846 [00:03<00:07, 35092.54ex/s] 31%|███       | 113056/363846 [00:03<00:07, 34442.97ex/s] 32%|███▏      | 115455/363846 [00:03<00:07, 35294.94ex/s] 32%|███▏      | 115154/363846 [00:03<00:07, 34920.56ex/s] 32%|███▏      | 118018/363846 [00:03<00:07, 35109.62ex/s] 32%|███▏      | 116569/363846 [00:03<00:07, 34645.47ex/s] 33%|███▎      | 118987/363846 [00:03<00:07, 34689.11ex/s] 33%|███▎      | 118648/363846 [00:03<00:07, 34469.34ex/s] 33%|███▎      | 121564/363846 [00:03<00:06, 35212.09ex/s] 33%|███▎      | 120043/363846 [00:03<00:07, 34673.16ex/s] 34%|███▎      | 122532/363846 [00:03<00:06, 34913.78ex/s] 34%|███▎      | 122138/363846 [00:03<00:06, 34597.33ex/s] 34%|███▍      | 125090/363846 [00:03<00:06, 35224.12ex/s] 34%|███▍      | 123549/363846 [00:03<00:06, 34786.62ex/s] 35%|███▍      | 126088/363846 [00:03<00:06, 35102.63ex/s] 35%|███▍      | 125671/363846 [00:03<00:06, 34812.73ex/s] 35%|███▌      | 128639/363846 [00:03<00:06, 35301.62ex/s] 35%|███▍      | 127032/363846 [00:03<00:06, 34798.22ex/s] 36%|███▌      | 129643/363846 [00:03<00:06, 35235.46ex/s] 36%|███▌      | 129182/363846 [00:03<00:06, 34900.62ex/s] 36%|███▋      | 132170/363846 [00:03<00:06, 35195.08ex/s] 36%|███▌      | 130526/363846 [00:03<00:06, 34840.27ex/s] 37%|███▋      | 133177/363846 [00:03<00:06, 35265.53ex/s] 36%|███▋      | 132690/363846 [00:03<00:06, 34953.43ex/s] 37%|███▋      | 135704/363846 [00:03<00:06, 35237.99ex/s] 37%|███▋      | 134011/363846 [00:03<00:06, 34801.39ex/s] 38%|███▊      | 136710/363846 [00:03<00:06, 35283.10ex/s] 37%|███▋      | 136187/363846 [00:03<00:06, 34931.81ex/s] 38%|███▊      | 139229/363846 [00:03<00:06, 34760.11ex/s] 38%|███▊      | 137498/363846 [00:03<00:06, 34819.99ex/s] 39%|███▊      | 140247/363846 [00:03<00:06, 35308.75ex/s] 38%|███▊      | 139687/363846 [00:03<00:06, 34951.27ex/s] 39%|███▉      | 142747/363846 [00:04<00:06, 34882.38ex/s] 39%|███▊      | 140981/363846 [00:04<00:06, 34793.57ex/s] 40%|███▉      | 143786/363846 [00:04<00:06, 35332.67ex/s] 39%|███▉      | 143183/363846 [00:04<00:06, 34932.94ex/s] 40%|████      | 146239/363846 [00:04<00:06, 34891.97ex/s] 40%|███▉      | 144461/363846 [00:04<00:06, 34676.52ex/s] 40%|████      | 147322/363846 [00:04<00:06, 35338.37ex/s] 40%|████      | 146680/363846 [00:04<00:06, 34941.89ex/s] 41%|████      | 149730/363846 [00:04<00:06, 34379.27ex/s] 41%|████      | 147929/363846 [00:04<00:06, 34270.67ex/s] 41%|████▏     | 150857/363846 [00:04<00:06, 35306.35ex/s] 41%|████▏     | 150179/363846 [00:04<00:06, 34954.32ex/s] 42%|████▏     | 153208/363846 [00:04<00:06, 34498.04ex/s] 42%|████▏     | 151372/363846 [00:04<00:06, 34317.74ex/s] 42%|████▏     | 154388/363846 [00:04<00:05, 35238.73ex/s] 42%|████▏     | 153675/363846 [00:04<00:06, 34518.27ex/s] 43%|████▎     | 156660/363846 [00:04<00:06, 34086.52ex/s] 43%|████▎     | 154857/363846 [00:04<00:06, 34474.88ex/s] 43%|████▎     | 157913/363846 [00:04<00:05, 34996.39ex/s] 43%|████▎     | 157129/363846 [00:04<00:05, 34488.05ex/s] 44%|████▍     | 160156/363846 [00:04<00:05, 34342.53ex/s] 44%|████▎     | 158318/363846 [00:04<00:05, 34514.95ex/s] 44%|████▍     | 161430/363846 [00:04<00:05, 35046.93ex/s] 44%|████▍     | 160579/363846 [00:04<00:05, 34489.97ex/s] 45%|████▍     | 163640/363846 [00:04<00:05, 34490.24ex/s] 44%|████▍     | 161780/363846 [00:04<00:05, 34545.17ex/s] 45%|████▌     | 164959/363846 [00:04<00:05, 35117.28ex/s] 45%|████▌     | 164029/363846 [00:04<00:05, 34401.30ex/s] 45%|████▌     | 165235/363846 [00:04<00:05, 34489.63ex/s] 46%|████▌     | 167091/363846 [00:04<00:05, 33947.54ex/s] 46%|████▋     | 168472/363846 [00:04<00:05, 35075.36ex/s] 46%|████▌     | 167470/363846 [00:04<00:05, 34326.57ex/s] 46%|████▋     | 168693/363846 [00:04<00:05, 34515.54ex/s] 47%|████▋     | 170588/363846 [00:04<00:05, 34245.94ex/s] 47%|████▋     | 170919/363846 [00:04<00:05, 34373.66ex/s] 47%|████▋     | 171980/363846 [00:04<00:05, 34524.56ex/s] 47%|████▋     | 172145/363846 [00:04<00:05, 34426.38ex/s] 48%|████▊     | 174016/363846 [00:04<00:05, 33713.90ex/s] 48%|████▊     | 174357/363846 [00:04<00:05, 34272.43ex/s] 48%|████▊     | 175463/363846 [00:04<00:05, 34613.37ex/s] 48%|████▊     | 175599/363846 [00:05<00:05, 34458.82ex/s] 49%|████▉     | 177392/363846 [00:05<00:05, 33609.09ex/s] 49%|████▉     | 177785/363846 [00:05<00:05, 34214.05ex/s] 49%|████▉     | 178968/363846 [00:05<00:05, 34742.16ex/s] 49%|████▉     | 179046/363846 [00:05<00:05, 34402.55ex/s] 50%|████▉     | 180873/363846 [00:05<00:05, 33959.20ex/s] 50%|████▉     | 181207/363846 [00:05<00:05, 34172.39ex/s] 50%|█████     | 182444/363846 [00:05<00:05, 34732.08ex/s] 50%|█████     | 182487/363846 [00:05<00:05, 34390.43ex/s] 51%|█████     | 184319/363846 [00:05<00:05, 34105.45ex/s] 51%|█████     | 185939/363846 [00:05<00:05, 34795.17ex/s] 51%|█████     | 184625/363846 [00:05<00:05, 34080.32ex/s] 51%|█████     | 185927/363846 [00:05<00:05, 34318.58ex/s] 52%|█████▏    | 187775/363846 [00:05<00:05, 34240.03ex/s] 52%|█████▏    | 188034/363846 [00:05<00:05, 34028.43ex/s] 52%|█████▏    | 189420/363846 [00:05<00:05, 32521.74ex/s] 52%|█████▏    | 189359/363846 [00:05<00:05, 34234.13ex/s] 53%|█████▎    | 191216/363846 [00:05<00:05, 34288.19ex/s] 53%|█████▎    | 191439/363846 [00:05<00:05, 34033.28ex/s] 53%|█████▎    | 192918/363846 [00:05<00:05, 33219.82ex/s] 53%|█████▎    | 192783/363846 [00:05<00:05, 33928.49ex/s] 53%|█████▎    | 194651/363846 [00:05<00:04, 34304.66ex/s] 54%|█████▎    | 194843/363846 [00:05<00:05, 33781.25ex/s] 54%|█████▍    | 196401/363846 [00:05<00:04, 33685.56ex/s] 54%|█████▍    | 196177/363846 [00:05<00:04, 33844.05ex/s] 54%|█████▍    | 198083/363846 [00:05<00:04, 33760.05ex/s] 54%|█████▍    | 198222/363846 [00:05<00:04, 33758.44ex/s] 55%|█████▍    | 199913/363846 [00:05<00:04, 34102.95ex/s] 55%|█████▍    | 199589/363846 [00:05<00:04, 33925.97ex/s] 55%|█████▌    | 201462/363846 [00:05<00:04, 33384.31ex/s] 55%|█████▌    | 201599/363846 [00:05<00:05, 32236.82ex/s] 56%|█████▌    | 203338/363846 [00:05<00:04, 33691.87ex/s] 56%|█████▌    | 202996/363846 [00:05<00:04, 33967.46ex/s] 56%|█████▋    | 204907/363846 [00:05<00:04, 33694.99ex/s] 56%|█████▋    | 204995/363846 [00:05<00:04, 32734.12ex/s] 57%|█████▋    | 206814/363846 [00:05<00:04, 34004.50ex/s] 57%|█████▋    | 206394/363846 [00:05<00:04, 33866.56ex/s] 57%|█████▋    | 208344/363846 [00:05<00:04, 33893.31ex/s] 57%|█████▋    | 208387/363846 [00:05<00:04, 33079.96ex/s] 58%|█████▊    | 210263/363846 [00:05<00:04, 34148.43ex/s] 58%|█████▊    | 209795/363846 [00:06<00:04, 33907.62ex/s] 58%|█████▊    | 211784/363846 [00:06<00:04, 34042.07ex/s] 58%|█████▊    | 211705/363846 [00:06<00:04, 32983.38ex/s] 59%|█████▊    | 213724/363846 [00:06<00:04, 34282.72ex/s] 59%|█████▊    | 213186/363846 [00:06<00:04, 33809.49ex/s] 59%|█████▉    | 215190/363846 [00:06<00:04, 33773.36ex/s] 59%|█████▉    | 215076/363846 [00:06<00:04, 33196.36ex/s] 60%|█████▉    | 217157/363846 [00:06<00:04, 34272.98ex/s] 60%|█████▉    | 216568/363846 [00:06<00:04, 33784.94ex/s] 60%|██████    | 218599/363846 [00:06<00:04, 33866.20ex/s] 60%|██████    | 218457/363846 [00:06<00:04, 33376.53ex/s] 61%|██████    | 220603/363846 [00:06<00:04, 34327.80ex/s] 60%|██████    | 219947/363846 [00:06<00:04, 33291.56ex/s] 61%|██████    | 222000/363846 [00:06<00:04, 33872.72ex/s] 61%|██████    | 221847/363846 [00:06<00:04, 33530.03ex/s] 62%|██████▏   | 224038/363846 [00:06<00:04, 34275.51ex/s] 61%|██████▏   | 223322/363846 [00:06<00:04, 33426.37ex/s] 62%|██████▏   | 225424/363846 [00:06<00:04, 33979.79ex/s] 62%|██████▏   | 225222/363846 [00:06<00:04, 33594.65ex/s] 63%|██████▎   | 227482/363846 [00:06<00:03, 34322.33ex/s] 62%|██████▏   | 226681/363846 [00:06<00:04, 33475.28ex/s] 63%|██████▎   | 228854/363846 [00:06<00:03, 34075.02ex/s] 63%|██████▎   | 228626/363846 [00:06<00:04, 33725.63ex/s] 63%|██████▎   | 230916/363846 [00:06<00:03, 34322.51ex/s] 63%|██████▎   | 230050/363846 [00:06<00:03, 33538.89ex/s] 64%|██████▍   | 232263/363846 [00:06<00:03, 33399.30ex/s] 64%|██████▍   | 232001/363846 [00:06<00:03, 33255.67ex/s] 64%|██████▍   | 234350/363846 [00:06<00:03, 34217.23ex/s] 64%|██████▍   | 233433/363846 [00:06<00:03, 33623.55ex/s] 65%|██████▍   | 235689/363846 [00:06<00:03, 33651.24ex/s] 65%|██████▍   | 235342/363846 [00:06<00:03, 33300.91ex/s] 65%|██████▌   | 237796/363846 [00:06<00:03, 34288.10ex/s] 65%|██████▌   | 236801/363846 [00:06<00:03, 33637.91ex/s] 66%|██████▌   | 239078/363846 [00:06<00:03, 33720.32ex/s] 66%|██████▌   | 238674/363846 [00:06<00:03, 32865.68ex/s] 66%|██████▋   | 241226/363846 [00:06<00:03, 34230.41ex/s] 66%|██████▌   | 240166/363846 [00:06<00:03, 33301.56ex/s] 67%|██████▋   | 242494/363846 [00:06<00:03, 33850.97ex/s] 67%|██████▋   | 242021/363846 [00:06<00:03, 33043.33ex/s] 67%|██████▋   | 244650/363846 [00:06<00:03, 34219.79ex/s] 67%|██████▋   | 243513/363846 [00:07<00:03, 33351.09ex/s] 68%|██████▊   | 245883/363846 [00:07<00:03, 33862.24ex/s] 67%|██████▋   | 245372/363846 [00:07<00:03, 33181.26ex/s] 68%|██████▊   | 248073/363846 [00:07<00:03, 34181.77ex/s] 68%|██████▊   | 246865/363846 [00:07<00:03, 33399.10ex/s] 69%|██████▊   | 249271/363846 [00:07<00:03, 33806.64ex/s] 68%|██████▊   | 248729/363846 [00:07<00:03, 33296.71ex/s] 69%|██████▉   | 251510/363846 [00:07<00:03, 34237.06ex/s] 69%|██████▉   | 250206/363846 [00:07<00:03, 32598.00ex/s] 69%|██████▉   | 252653/363846 [00:07<00:03, 33762.11ex/s] 70%|███████   | 254934/363846 [00:07<00:03, 34233.20ex/s] 69%|██████▉   | 252060/363846 [00:07<00:03, 32711.47ex/s] 70%|██████▉   | 253542/363846 [00:07<00:03, 32820.32ex/s] 70%|███████   | 256030/363846 [00:07<00:03, 33726.86ex/s] 71%|███████   | 258358/363846 [00:07<00:03, 34182.25ex/s] 70%|███████   | 255422/363846 [00:07<00:03, 32977.62ex/s] 71%|███████   | 256881/363846 [00:07<00:03, 32987.38ex/s] 71%|███████▏  | 259407/363846 [00:07<00:03, 33737.00ex/s] 72%|███████▏  | 261786/363846 [00:07<00:02, 34210.75ex/s] 71%|███████   | 258784/363846 [00:07<00:03, 33166.67ex/s] 72%|███████▏  | 262781/363846 [00:07<00:02, 33699.73ex/s] 72%|███████▏  | 260183/363846 [00:07<00:03, 32519.61ex/s] 73%|███████▎  | 265208/363846 [00:07<00:02, 34136.68ex/s] 72%|███████▏  | 262103/363846 [00:07<00:03, 33158.84ex/s] 73%|███████▎  | 266152/363846 [00:07<00:02, 33586.74ex/s] 72%|███████▏  | 263514/363846 [00:07<00:03, 32750.88ex/s] 74%|███████▍  | 268622/363846 [00:07<00:02, 34122.75ex/s] 73%|███████▎  | 265434/363846 [00:07<00:02, 33201.21ex/s] 74%|███████▍  | 269526/363846 [00:07<00:02, 33631.96ex/s] 73%|███████▎  | 266863/363846 [00:07<00:02, 32966.65ex/s] 74%|███████▍  | 268771/363846 [00:07<00:02, 33251.11ex/s] 75%|███████▍  | 272035/363846 [00:07<00:02, 33255.10ex/s] 75%|███████▌  | 272894/363846 [00:07<00:02, 33644.30ex/s] 74%|███████▍  | 270175/363846 [00:07<00:02, 33010.23ex/s] 75%|███████▍  | 272097/363846 [00:07<00:02, 33201.41ex/s] 76%|███████▌  | 275426/363846 [00:07<00:02, 33448.06ex/s] 75%|███████▌  | 273509/363846 [00:07<00:02, 33106.88ex/s] 76%|███████▌  | 276259/363846 [00:07<00:02, 33551.89ex/s] 76%|███████▌  | 275418/363846 [00:07<00:02, 33182.71ex/s] 77%|███████▋  | 278829/363846 [00:07<00:02, 33618.88ex/s] 76%|███████▌  | 276852/363846 [00:08<00:02, 33201.24ex/s] 77%|███████▋  | 279620/363846 [00:08<00:02, 33569.04ex/s] 77%|███████▋  | 278745/363846 [00:08<00:02, 33207.65ex/s] 78%|███████▊  | 282194/363846 [00:08<00:02, 33610.25ex/s] 77%|███████▋  | 280174/363846 [00:08<00:02, 33202.22ex/s] 78%|███████▊  | 282978/363846 [00:08<00:02, 32956.44ex/s] 78%|███████▊  | 282067/363846 [00:08<00:02, 33157.52ex/s] 78%|███████▊  | 285583/363846 [00:08<00:02, 33690.97ex/s] 78%|███████▊  | 283495/363846 [00:08<00:02, 33139.56ex/s] 79%|███████▊  | 286287/363846 [00:08<00:02, 32994.99ex/s] 78%|███████▊  | 285383/363846 [00:08<00:02, 33149.95ex/s] 79%|███████▉  | 288980/363846 [00:08<00:02, 33773.48ex/s] 79%|███████▉  | 286810/363846 [00:08<00:02, 32717.96ex/s] 80%|███████▉  | 289625/363846 [00:08<00:02, 33107.22ex/s] 79%|███████▉  | 288699/363846 [00:08<00:02, 33110.52ex/s] 80%|████████  | 292359/363846 [00:08<00:02, 33733.73ex/s] 80%|███████▉  | 290096/363846 [00:08<00:02, 32759.00ex/s] 81%|████████  | 292987/363846 [00:08<00:02, 33256.75ex/s] 80%|████████  | 292011/363846 [00:08<00:02, 33080.02ex/s] 81%|████████▏ | 295740/363846 [00:08<00:02, 33755.16ex/s] 81%|████████  | 293381/363846 [00:08<00:02, 32785.98ex/s] 81%|████████▏ | 296329/363846 [00:08<00:02, 33303.46ex/s] 81%|████████  | 295320/363846 [00:08<00:02, 33076.85ex/s] 82%|████████▏ | 299117/363846 [00:08<00:01, 33713.00ex/s] 82%|████████▏ | 296709/363846 [00:08<00:02, 32930.27ex/s] 82%|████████▏ | 299685/363846 [00:08<00:01, 33378.24ex/s] 82%|████████▏ | 298632/363846 [00:08<00:01, 33089.25ex/s] 83%|████████▎ | 302497/363846 [00:08<00:01, 33736.17ex/s] 82%|████████▏ | 300006/363846 [00:08<00:01, 32941.06ex/s] 83%|████████▎ | 303024/363846 [00:08<00:01, 33378.43ex/s] 83%|████████▎ | 301942/363846 [00:08<00:01, 33089.64ex/s] 84%|████████▍ | 305871/363846 [00:08<00:01, 33705.99ex/s] 83%|████████▎ | 303325/363846 [00:08<00:01, 33014.06ex/s] 84%|████████▍ | 306368/363846 [00:08<00:01, 33396.75ex/s] 84%|████████▍ | 305251/363846 [00:08<00:01, 33065.80ex/s] 85%|████████▍ | 309242/363846 [00:08<00:01, 33650.33ex/s] 84%|████████▍ | 306636/363846 [00:08<00:01, 33040.86ex/s] 85%|████████▌ | 309720/363846 [00:08<00:01, 33433.16ex/s] 85%|████████▍ | 308561/363846 [00:08<00:01, 33074.41ex/s] 86%|████████▌ | 312608/363846 [00:08<00:01, 33323.93ex/s] 85%|████████▌ | 309941/363846 [00:09<00:01, 32968.97ex/s] 86%|████████▌ | 313064/363846 [00:09<00:01, 32708.56ex/s] 86%|████████▌ | 311869/363846 [00:09<00:01, 33064.86ex/s] 87%|████████▋ | 315948/363846 [00:09<00:01, 33345.03ex/s] 86%|████████▌ | 313239/363846 [00:09<00:01, 32901.35ex/s] 87%|████████▋ | 316339/363846 [00:09<00:01, 32327.40ex/s] 87%|████████▋ | 315176/363846 [00:09<00:01, 32949.60ex/s] 88%|████████▊ | 319285/363846 [00:09<00:01, 33352.12ex/s] 87%|████████▋ | 316530/363846 [00:09<00:01, 32360.04ex/s] 88%|████████▊ | 319696/363846 [00:09<00:01, 32689.33ex/s] 88%|████████▊ | 318472/363846 [00:09<00:01, 32923.49ex/s] 89%|████████▊ | 322624/363846 [00:09<00:01, 33363.46ex/s] 88%|████████▊ | 319833/363846 [00:09<00:01, 32557.71ex/s] 89%|████████▉ | 323013/363846 [00:09<00:01, 32830.47ex/s] 88%|████████▊ | 321765/363846 [00:09<00:01, 32911.64ex/s] 90%|████████▉ | 325965/363846 [00:09<00:01, 33375.99ex/s] 89%|████████▉ | 323091/363846 [00:09<00:01, 32547.34ex/s] 90%|████████▉ | 326336/363846 [00:09<00:01, 32948.62ex/s] 89%|████████▉ | 325057/363846 [00:09<00:01, 32837.41ex/s] 91%|█████████ | 329303/363846 [00:09<00:01, 33366.80ex/s] 90%|████████▉ | 326377/363846 [00:09<00:01, 32637.88ex/s] 91%|█████████ | 329633/363846 [00:09<00:01, 32662.20ex/s] 90%|█████████ | 328342/363846 [00:09<00:01, 32839.41ex/s] 91%|█████████▏| 332640/363846 [00:09<00:00, 33347.78ex/s] 91%|█████████ | 329642/363846 [00:09<00:01, 32415.02ex/s] 92%|█████████▏| 332943/363846 [00:09<00:00, 32791.00ex/s] 91%|█████████ | 331627/363846 [00:09<00:00, 32840.48ex/s] 92%|█████████▏| 335975/363846 [00:09<00:00, 33347.34ex/s] 91%|█████████▏| 332890/363846 [00:09<00:00, 32434.24ex/s] 92%|█████████▏| 336238/363846 [00:09<00:00, 32836.68ex/s] 92%|█████████▏| 334912/363846 [00:09<00:00, 32813.69ex/s] 93%|█████████▎| 339310/363846 [00:09<00:00, 33273.13ex/s] 92%|█████████▏| 336157/363846 [00:09<00:00, 32502.65ex/s] 93%|█████████▎| 339523/363846 [00:09<00:00, 32766.53ex/s] 93%|█████████▎| 338194/363846 [00:09<00:00, 32745.17ex/s] 94%|█████████▍| 342638/363846 [00:09<00:00, 32647.91ex/s] 93%|█████████▎| 339408/363846 [00:09<00:00, 32029.80ex/s] 94%|█████████▍| 342801/363846 [00:09<00:00, 32029.83ex/s] 94%|█████████▍| 341469/363846 [00:09<00:00, 32741.64ex/s] 95%|█████████▌| 345934/363846 [00:09<00:00, 32739.13ex/s] 94%|█████████▍| 342639/363846 [00:10<00:00, 32110.73ex/s] 95%|█████████▌| 346071/363846 [00:10<00:00, 32227.05ex/s] 95%|█████████▍| 344744/363846 [00:10<00:00, 32345.95ex/s] 96%|█████████▌| 349212/363846 [00:10<00:00, 32750.10ex/s] 95%|█████████▌| 345852/363846 [00:10<00:00, 31739.78ex/s] 96%|█████████▌| 349356/363846 [00:10<00:00, 32410.58ex/s] 96%|█████████▌| 348000/363846 [00:10<00:00, 32380.42ex/s] 97%|█████████▋| 352489/363846 [00:10<00:00, 32253.99ex/s] 96%|█████████▌| 349078/363846 [00:10<00:00, 31892.96ex/s] 97%|█████████▋| 352647/363846 [00:10<00:00, 32557.62ex/s] 97%|█████████▋| 351251/363846 [00:10<00:00, 32417.13ex/s] 98%|█████████▊| 355788/363846 [00:10<00:00, 32468.59ex/s] 97%|█████████▋| 352310/363846 [00:10<00:00, 32017.85ex/s] 98%|█████████▊| 355929/363846 [00:10<00:00, 32635.41ex/s] 97%|█████████▋| 354524/363846 [00:10<00:00, 32509.68ex/s] 99%|█████████▊| 359069/363846 [00:10<00:00, 32567.52ex/s] 98%|█████████▊| 355513/363846 [00:10<00:00, 31586.46ex/s] 99%|█████████▊| 359194/363846 [00:10<00:00, 32092.10ex/s] 98%|█████████▊| 357780/363846 [00:10<00:00, 32522.03ex/s]100%|█████████▉| 362368/363846 [00:10<00:00, 32690.33ex/s]100%|██████████| 363846/363846 [00:10<00:00, 34504.76ex/s]
 99%|█████████▊| 358747/363846 [00:10<00:00, 31806.99ex/s]100%|█████████▉| 362459/363846 [00:10<00:00, 32256.94ex/s] 99%|█████████▉| 361033/363846 [00:10<00:00, 32490.62ex/s]100%|██████████| 363846/363846 [00:10<00:00, 34180.47ex/s]
 99%|█████████▉| 361930/363846 [00:10<00:00, 31771.81ex/s]100%|██████████| 363846/363846 [00:10<00:00, 34107.34ex/s]
100%|██████████| 363846/363846 [00:10<00:00, 33819.05ex/s]
Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
Downloading:   0%|          | 0.00/313M [00:00<?, ?B/s]Downloading:   0%|          | 19.5k/313M [00:00<1:03:04, 82.6kB/s]Downloading:   0%|          | 62.5k/313M [00:00<52:45, 98.8kB/s]  Downloading:   0%|          | 135k/313M [00:00<42:00, 124kB/s]  Downloading:   0%|          | 314k/313M [00:00<31:26, 166kB/s]Downloading:   0%|          | 648k/313M [00:01<23:06, 225kB/s]Downloading:   0%|          | 1.34M/313M [00:01<16:40, 311kB/s]Downloading:   1%|          | 2.70M/313M [00:01<11:53, 435kB/s]Downloading:   2%|▏         | 5.43M/313M [00:01<08:22, 611kB/s]Downloading:   3%|▎         | 9.22M/313M [00:02<05:53, 859kB/s]Downloading:   4%|▍         | 13.1M/313M [00:02<04:09, 1.20MB/s]Downloading:   5%|▌         | 17.1M/313M [00:02<02:57, 1.66MB/s]Downloading:   7%|▋         | 21.1M/313M [00:02<02:08, 2.28MB/s]Downloading:   8%|▊         | 25.0M/313M [00:03<01:33, 3.07MB/s]Downloading:   9%|▉         | 29.3M/313M [00:03<01:09, 4.08MB/s]Downloading:  11%|█         | 33.2M/313M [00:03<00:53, 5.27MB/s]Downloading:  12%|█▏        | 37.3M/313M [00:03<00:41, 6.63MB/s]Downloading:  13%|█▎        | 41.2M/313M [00:04<00:33, 8.08MB/s]Downloading:  14%|█▍        | 45.1M/313M [00:04<00:28, 9.52MB/s]Downloading:  16%|█▌        | 49.1M/313M [00:04<00:24, 10.9MB/s]Downloading:  17%|█▋        | 53.0M/313M [00:04<00:21, 12.1MB/s]Downloading:  18%|█▊        | 57.2M/313M [00:05<00:19, 13.3MB/s]Downloading:  20%|█▉        | 61.1M/313M [00:05<00:17, 14.1MB/s]Downloading:  21%|██        | 65.1M/313M [00:05<00:16, 14.7MB/s]Downloading:  22%|██▏       | 69.0M/313M [00:05<00:16, 15.2MB/s]Downloading:  23%|██▎       | 72.8M/313M [00:05<00:15, 15.6MB/s]Downloading:  25%|██▍       | 76.9M/313M [00:06<00:14, 15.8MB/s]Downloading:  26%|██▌       | 80.8M/313M [00:06<00:14, 15.9MB/s]Downloading:  27%|██▋       | 84.7M/313M [00:06<00:14, 16.1MB/s]Downloading:  28%|██▊       | 88.9M/313M [00:06<00:13, 16.5MB/s]Downloading:  30%|██▉       | 92.8M/313M [00:07<00:13, 16.4MB/s]Downloading:  31%|███       | 96.7M/313M [00:07<00:13, 16.4MB/s]Downloading:  32%|███▏      | 101M/313M [00:07<00:12, 16.3MB/s] Downloading:  33%|███▎      | 105M/313M [00:07<00:12, 16.4MB/s]Downloading:  35%|███▍      | 109M/313M [00:08<00:12, 16.4MB/s]Downloading:  36%|███▌      | 113M/313M [00:08<00:12, 16.5MB/s]Downloading:  37%|███▋      | 117M/313M [00:08<00:11, 16.5MB/s]Downloading:  39%|███▊      | 121M/313M [00:08<00:11, 16.7MB/s]Downloading:  40%|███▉      | 125M/313M [00:09<00:11, 16.6MB/s]Downloading:  41%|████      | 129M/313M [00:09<00:10, 16.8MB/s]Downloading:  42%|████▏     | 133M/313M [00:09<00:10, 16.6MB/s]Downloading:  44%|████▎     | 137M/313M [00:09<00:10, 16.7MB/s]Downloading:  45%|████▍     | 141M/313M [00:10<00:10, 16.6MB/s]Downloading:  46%|████▌     | 145M/313M [00:10<00:10, 16.5MB/s]Downloading:  47%|████▋     | 147M/313M [00:10<00:11, 14.6MB/s]Downloading:  48%|████▊     | 151M/313M [00:10<00:10, 15.2MB/s]Downloading:  50%|████▉     | 155M/313M [00:11<00:13, 11.8MB/s]Downloading:  51%|█████     | 159M/313M [00:11<00:11, 13.0MB/s]Downloading:  52%|█████▏    | 163M/313M [00:11<00:10, 13.9MB/s]Downloading:  53%|█████▎    | 166M/313M [00:11<00:08, 17.1MB/s]Downloading:  54%|█████▍    | 169M/313M [00:11<00:09, 14.9MB/s]Downloading:  54%|█████▍    | 170M/313M [00:12<00:11, 12.3MB/s]Downloading:  56%|█████▌    | 174M/313M [00:12<00:11, 12.6MB/s]Downloading:  57%|█████▋    | 178M/313M [00:12<00:09, 13.6MB/s]Downloading:  58%|█████▊    | 181M/313M [00:12<00:09, 14.2MB/s]Downloading:  59%|█████▉    | 185M/313M [00:13<00:08, 14.8MB/s]Downloading:  61%|██████    | 189M/313M [00:13<00:08, 15.2MB/s]Downloading:  62%|██████▏   | 193M/313M [00:13<00:07, 15.6MB/s]Downloading:  63%|██████▎   | 197M/313M [00:13<00:07, 15.8MB/s]Downloading:  64%|██████▍   | 201M/313M [00:14<00:06, 16.0MB/s]Downloading:  66%|██████▌   | 205M/313M [00:14<00:06, 16.4MB/s]Downloading:  67%|██████▋   | 208M/313M [00:14<00:07, 14.9MB/s]Downloading:  68%|██████▊   | 212M/313M [00:14<00:06, 15.3MB/s]Downloading:  69%|██████▉   | 216M/313M [00:15<00:06, 15.8MB/s]Downloading:  70%|███████   | 220M/313M [00:15<00:05, 16.0MB/s]Downloading:  72%|███████▏  | 224M/313M [00:15<00:05, 16.2MB/s]Downloading:  73%|███████▎  | 228M/313M [00:15<00:05, 16.5MB/s]Downloading:  74%|███████▍  | 232M/313M [00:16<00:04, 16.5MB/s]Downloading:  75%|███████▌  | 236M/313M [00:16<00:04, 15.9MB/s]Downloading:  77%|███████▋  | 240M/313M [00:16<00:04, 16.5MB/s]Downloading:  78%|███████▊  | 244M/313M [00:16<00:04, 16.4MB/s]Downloading:  79%|███████▉  | 248M/313M [00:17<00:03, 16.4MB/s]Downloading:  81%|████████  | 252M/313M [00:17<00:03, 16.3MB/s]Downloading:  82%|████████▏ | 256M/313M [00:17<00:03, 16.3MB/s]Downloading:  83%|████████▎ | 260M/313M [00:17<00:03, 16.5MB/s]Downloading:  84%|████████▍ | 264M/313M [00:17<00:02, 16.5MB/s]Downloading:  86%|████████▌ | 268M/313M [00:18<00:02, 16.4MB/s]Downloading:  86%|████████▌ | 270M/313M [00:18<00:03, 12.8MB/s]Downloading:  87%|████████▋ | 273M/313M [00:18<00:02, 13.4MB/s]Downloading:  88%|████████▊ | 276M/313M [00:18<00:02, 13.1MB/s]Downloading:  89%|████████▉ | 279M/313M [00:19<00:02, 13.2MB/s]Downloading:  90%|█████████ | 283M/313M [00:19<00:02, 13.3MB/s]Downloading:  91%|█████████▏| 285M/313M [00:19<00:02, 12.9MB/s]Downloading:  92%|█████████▏| 289M/313M [00:19<00:01, 13.2MB/s]Downloading:  93%|█████████▎| 292M/313M [00:20<00:01, 13.4MB/s]Downloading:  94%|█████████▍| 295M/313M [00:20<00:01, 13.7MB/s]Downloading:  96%|█████████▌| 299M/313M [00:20<00:01, 13.9MB/s]Downloading:  97%|█████████▋| 302M/313M [00:20<00:00, 14.1MB/s]Downloading:  98%|█████████▊| 306M/313M [00:21<00:00, 14.1MB/s]Downloading:  99%|█████████▉| 309M/313M [00:21<00:00, 14.2MB/s]Downloading: 100%|█████████▉| 313M/313M [00:21<00:00, 14.3MB/s]Downloading: 100%|██████████| 313M/313M [00:21<00:00, 14.5MB/s]
0 examples [00:00, ? examples/s]2532 examples [00:00, 25315.74 examples/s]6081 examples [00:00, 27691.28 examples/s]9646 examples [00:00, 29677.34 examples/s]13075 examples [00:00, 30924.75 examples/s]16615 examples [00:00, 32142.65 examples/s]20036 examples [00:00, 32735.29 examples/s]23699 examples [00:00, 33812.39 examples/s]26915 examples [00:00, 32395.96 examples/s]30360 examples [00:00, 32984.55 examples/s]33998 examples [00:01, 33932.42 examples/s]37476 examples [00:01, 34178.42 examples/s]40865 examples [00:01, 32268.59 examples/s]44449 examples [00:01, 33261.39 examples/s]47784 examples [00:01, 33139.35 examples/s]51206 examples [00:01, 33455.27 examples/s]54866 examples [00:01, 34340.29 examples/s]58472 examples [00:01, 34836.43 examples/s]61964 examples [00:01, 34846.07 examples/s]65592 examples [00:01, 35261.25 examples/s]69183 examples [00:02, 35453.01 examples/s]72733 examples [00:02, 34930.37 examples/s]76334 examples [00:02, 35245.11 examples/s]79863 examples [00:02, 34575.13 examples/s]83327 examples [00:02, 34480.83 examples/s]86937 examples [00:02, 34950.89 examples/s]90437 examples [00:02, 34637.45 examples/s]94106 examples [00:02, 35226.89 examples/s]97720 examples [00:02, 35493.96 examples/s]101274 examples [00:02, 35192.50 examples/s]104797 examples [00:03, 34667.71 examples/s]108268 examples [00:03, 32946.49 examples/s]111712 examples [00:03, 33379.98 examples/s]115319 examples [00:03, 34143.37 examples/s]118896 examples [00:03, 34614.21 examples/s]122370 examples [00:03, 34416.34 examples/s]125963 examples [00:03, 34856.30 examples/s]129526 examples [00:03, 35083.68 examples/s]133041 examples [00:03, 34559.16 examples/s]136661 examples [00:03, 35033.31 examples/s]140170 examples [00:04, 33901.12 examples/s]143792 examples [00:04, 34564.63 examples/s]147411 examples [00:04, 35035.03 examples/s]150925 examples [00:04, 34777.38 examples/s]154556 examples [00:04, 35223.27 examples/s]158161 examples [00:04, 35465.57 examples/s]161713 examples [00:04, 35035.19 examples/s]165222 examples [00:04, 34522.11 examples/s]168680 examples [00:04, 33316.73 examples/s]172130 examples [00:05, 33661.37 examples/s]175750 examples [00:05, 34382.28 examples/s]179317 examples [00:05, 34758.25 examples/s]182802 examples [00:05, 32473.78 examples/s]186382 examples [00:05, 33404.54 examples/s]189983 examples [00:05, 34144.33 examples/s]193424 examples [00:05, 33454.83 examples/s]197050 examples [00:05, 34246.68 examples/s]200495 examples [00:05, 33652.11 examples/s]204122 examples [00:05, 34393.33 examples/s]207720 examples [00:06, 34854.38 examples/s]211218 examples [00:06, 34721.06 examples/s]214699 examples [00:06, 34602.53 examples/s]218315 examples [00:06, 35053.69 examples/s]221827 examples [00:06, 34872.96 examples/s]225319 examples [00:06, 34333.39 examples/s]228904 examples [00:06, 34772.97 examples/s]232387 examples [00:06, 34646.90 examples/s]236046 examples [00:06, 35206.71 examples/s]239661 examples [00:06, 35483.45 examples/s]243214 examples [00:07, 35279.76 examples/s]246745 examples [00:07, 34981.02 examples/s]250246 examples [00:07, 33831.54 examples/s]253640 examples [00:07, 32115.91 examples/s]257234 examples [00:07, 33174.11 examples/s]260637 examples [00:07, 33424.93 examples/s]264248 examples [00:07, 34185.37 examples/s]267840 examples [00:07, 34687.11 examples/s]271323 examples [00:07, 34487.76 examples/s]274951 examples [00:08, 35005.91 examples/s]278461 examples [00:08, 34319.57 examples/s]281915 examples [00:08, 34384.92 examples/s]285361 examples [00:08, 32837.36 examples/s]288974 examples [00:08, 33759.20 examples/s]292370 examples [00:08, 33384.70 examples/s]295999 examples [00:08, 34206.09 examples/s]299436 examples [00:08, 33579.38 examples/s]302846 examples [00:08, 33733.59 examples/s]306229 examples [00:08, 33507.96 examples/s]309748 examples [00:09, 33995.34 examples/s]313199 examples [00:09, 34146.48 examples/s]316806 examples [00:09, 34700.53 examples/s]320282 examples [00:09, 34437.50 examples/s]323731 examples [00:09, 33858.54 examples/s]327123 examples [00:09, 33865.89 examples/s]330514 examples [00:09, 33123.54 examples/s]333940 examples [00:09, 33454.79 examples/s]337504 examples [00:09, 34080.76 examples/s]340919 examples [00:09, 34006.11 examples/s]344470 examples [00:10, 34442.73 examples/s]347991 examples [00:10, 34666.34 examples/s]351462 examples [00:10, 30226.68 examples/s]354699 examples [00:10, 30838.43 examples/s]358268 examples [00:10, 32148.74 examples/s]361702 examples [00:10, 32774.76 examples/s]365343 examples [00:10, 33786.72 examples/s]368941 examples [00:10, 34414.49 examples/s]372416 examples [00:10, 34362.68 examples/s]376004 examples [00:11, 34803.18 examples/s]379502 examples [00:11, 33884.98 examples/s]382909 examples [00:11, 33192.22 examples/s]386517 examples [00:11, 34007.50 examples/s]390000 examples [00:11, 34031.33 examples/s]                                            0 examples [00:00, ? examples/s]3258 examples [00:00, 32575.68 examples/s]6748 examples [00:00, 33237.14 examples/s]                                          0 examples [00:00, ? examples/s]3511 examples [00:00, 35105.26 examples/s]6945 examples [00:00, 34870.08 examples/s]                                          0 examples [00:00, ? examples/s]3888 examples [00:00, 38875.50 examples/s]7748 examples [00:00, 38791.14 examples/s]                                          0 examples [00:00, ? examples/s]3838 examples [00:00, 38377.93 examples/s]7673 examples [00:00, 38369.04 examples/s]                                          Dataset glue downloaded and prepared to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/392702 [00:00<?, ?ex/s]  0%|          | 0/392702 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/392702 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/392702 [00:00<?, ?ex/s]  1%|          | 3237/392702 [00:00<00:12, 32368.87ex/s]  1%|          | 3048/392702 [00:00<00:12, 30474.80ex/s]  1%|          | 3558/392702 [00:00<00:10, 35579.10ex/s]  1%|          | 2957/392702 [00:00<00:13, 29565.45ex/s]  2%|▏         | 6626/392702 [00:00<00:11, 32810.51ex/s]  2%|▏         | 6735/392702 [00:00<00:12, 32146.55ex/s]  2%|▏         | 6997/392702 [00:00<00:10, 35211.00ex/s]  2%|▏         | 6606/392702 [00:00<00:12, 31350.10ex/s]  3%|▎         | 10000/392702 [00:00<00:11, 33065.48ex/s]  3%|▎         | 10401/392702 [00:00<00:11, 33379.36ex/s]  3%|▎         | 10602/392702 [00:00<00:10, 35458.36ex/s]  3%|▎         | 10036/392702 [00:00<00:11, 32179.65ex/s]  3%|▎         | 13560/392702 [00:00<00:11, 33784.53ex/s]  4%|▎         | 13882/392702 [00:00<00:11, 33793.95ex/s]  4%|▎         | 14215/392702 [00:00<00:10, 35655.33ex/s]  3%|▎         | 13664/392702 [00:00<00:11, 33307.98ex/s]  4%|▍         | 17146/392702 [00:00<00:10, 34380.71ex/s]  4%|▍         | 16966/392702 [00:00<00:11, 32849.44ex/s]  5%|▍         | 17826/392702 [00:00<00:10, 35790.33ex/s]  4%|▍         | 16508/392702 [00:00<00:11, 31679.90ex/s]  5%|▌         | 20752/392702 [00:00<00:10, 34867.76ex/s]  5%|▌         | 20134/392702 [00:00<00:11, 32487.79ex/s]  5%|▌         | 21236/392702 [00:00<00:10, 35263.82ex/s]  5%|▌         | 19881/392702 [00:00<00:11, 32266.54ex/s]  6%|▌         | 24049/392702 [00:00<00:10, 34275.20ex/s]  6%|▌         | 23794/392702 [00:00<00:10, 33620.41ex/s]  6%|▌         | 24358/392702 [00:00<00:10, 33941.27ex/s]  6%|▌         | 23223/392702 [00:00<00:11, 32603.98ex/s]  7%|▋         | 27630/392702 [00:00<00:10, 34719.83ex/s]  7%|▋         | 27466/392702 [00:00<00:10, 34492.88ex/s]  7%|▋         | 27472/392702 [00:00<00:11, 32714.19ex/s]  7%|▋         | 26881/392702 [00:00<00:10, 33701.14ex/s]  8%|▊         | 31214/392702 [00:00<00:10, 35046.82ex/s]  8%|▊         | 31133/392702 [00:00<00:10, 35117.19ex/s]  8%|▊         | 31068/392702 [00:00<00:10, 33623.53ex/s]  8%|▊         | 30319/392702 [00:00<00:10, 33901.52ex/s]  9%|▉         | 34758/392702 [00:01<00:10, 35163.32ex/s]  9%|▉         | 34814/392702 [00:01<00:10, 35606.08ex/s]  9%|▉         | 34676/392702 [00:01<00:10, 34322.22ex/s]  9%|▊         | 33972/392702 [00:01<00:10, 34647.29ex/s] 10%|▉         | 38217/392702 [00:01<00:10, 34987.22ex/s] 10%|▉         | 38470/392702 [00:01<00:09, 35886.06ex/s] 10%|▉         | 38263/392702 [00:01<00:10, 34769.78ex/s] 10%|▉         | 37585/392702 [00:01<00:10, 35077.30ex/s] 11%|█         | 41789/392702 [00:01<00:09, 35201.58ex/s] 11%|█         | 42113/392702 [00:01<00:09, 36045.12ex/s] 11%|█         | 41848/392702 [00:01<00:09, 35085.53ex/s] 10%|█         | 41206/392702 [00:01<00:09, 35409.59ex/s] 12%|█▏        | 45328/392702 [00:01<00:09, 35257.58ex/s] 12%|█▏        | 45772/392702 [00:01<00:09, 36204.60ex/s] 12%|█▏        | 45318/392702 [00:01<00:09, 34907.47ex/s] 11%|█▏        | 44830/392702 [00:01<00:09, 35654.57ex/s] 12%|█▏        | 48891/392702 [00:01<00:09, 35368.19ex/s] 13%|█▎        | 49412/392702 [00:01<00:09, 36262.16ex/s] 12%|█▏        | 48894/392702 [00:01<00:09, 35157.21ex/s] 12%|█▏        | 48450/392702 [00:01<00:09, 35814.58ex/s] 13%|█▎        | 52421/392702 [00:01<00:09, 35345.06ex/s] 14%|█▎        | 53022/392702 [00:01<00:09, 35991.26ex/s] 13%|█▎        | 52444/392702 [00:01<00:09, 35257.28ex/s] 13%|█▎        | 52048/392702 [00:01<00:09, 35861.37ex/s] 14%|█▍        | 56656/392702 [00:01<00:09, 36092.39ex/s] 14%|█▍        | 55941/392702 [00:01<00:09, 34149.15ex/s] 14%|█▍        | 56001/392702 [00:01<00:09, 35348.69ex/s] 14%|█▍        | 55660/392702 [00:01<00:09, 35938.45ex/s] 15%|█▌        | 60278/392702 [00:01<00:09, 36128.32ex/s] 15%|█▌        | 59483/392702 [00:01<00:09, 34520.37ex/s] 15%|█▌        | 59566/392702 [00:01<00:09, 35435.99ex/s] 15%|█▌        | 59250/392702 [00:01<00:09, 35925.66ex/s] 16%|█▋        | 63905/392702 [00:01<00:09, 36169.22ex/s] 16%|█▌        | 63030/392702 [00:01<00:09, 34798.32ex/s] 16%|█▌        | 63118/392702 [00:01<00:09, 35459.89ex/s] 16%|█▌        | 62850/392702 [00:01<00:09, 35947.84ex/s] 17%|█▋        | 67519/392702 [00:01<00:08, 36152.00ex/s] 17%|█▋        | 66581/392702 [00:01<00:09, 35006.38ex/s] 17%|█▋        | 66674/392702 [00:01<00:09, 35489.04ex/s] 17%|█▋        | 66442/392702 [00:01<00:09, 35902.70ex/s] 18%|█▊        | 71135/392702 [00:02<00:08, 36152.16ex/s] 18%|█▊        | 70108/392702 [00:02<00:09, 35083.66ex/s] 18%|█▊        | 70225/392702 [00:02<00:09, 35493.00ex/s] 18%|█▊        | 70030/392702 [00:02<00:09, 35813.89ex/s] 19%|█▉        | 74749/392702 [00:02<00:08, 36137.07ex/s] 19%|█▉        | 73651/392702 [00:02<00:09, 35184.39ex/s] 19%|█▉        | 73787/392702 [00:02<00:08, 35529.32ex/s] 19%|█▊        | 73610/392702 [00:02<00:08, 35482.48ex/s] 20%|█▉        | 78362/392702 [00:02<00:08, 36080.94ex/s] 20%|█▉        | 77170/392702 [00:02<00:08, 35170.11ex/s] 20%|█▉        | 77339/392702 [00:02<00:08, 35438.43ex/s] 20%|█▉        | 77158/392702 [00:02<00:08, 35470.66ex/s] 21%|██        | 81973/392702 [00:02<00:08, 36087.86ex/s] 21%|██        | 80699/392702 [00:02<00:08, 35203.71ex/s] 21%|██        | 80895/392702 [00:02<00:08, 35472.60ex/s] 21%|██        | 80727/392702 [00:02<00:08, 35533.94ex/s] 22%|██▏       | 85582/392702 [00:02<00:08, 35950.46ex/s] 21%|██▏       | 84220/392702 [00:02<00:08, 35195.72ex/s] 22%|██▏       | 84442/392702 [00:02<00:08, 35360.31ex/s] 21%|██▏       | 84281/392702 [00:02<00:08, 35528.45ex/s] 23%|██▎       | 89177/392702 [00:02<00:08, 35927.23ex/s] 22%|██▏       | 87740/392702 [00:02<00:08, 35196.71ex/s] 22%|██▏       | 87978/392702 [00:02<00:08, 35005.94ex/s] 22%|██▏       | 87834/392702 [00:02<00:08, 35235.88ex/s] 24%|██▎       | 92775/392702 [00:02<00:08, 35941.46ex/s] 23%|██▎       | 91260/392702 [00:02<00:08, 35174.64ex/s] 23%|██▎       | 91497/392702 [00:02<00:08, 35058.29ex/s] 23%|██▎       | 91359/392702 [00:02<00:08, 35095.47ex/s] 25%|██▍       | 96369/392702 [00:02<00:08, 35571.77ex/s] 24%|██▍       | 94778/392702 [00:02<00:08, 34014.80ex/s] 24%|██▍       | 95004/392702 [00:02<00:08, 35055.49ex/s] 24%|██▍       | 94941/392702 [00:02<00:08, 35308.67ex/s] 25%|██▌       | 99968/392702 [00:02<00:08, 35695.26ex/s] 25%|██▌       | 98276/392702 [00:02<00:08, 34296.71ex/s] 25%|██▌       | 98538/392702 [00:02<00:08, 35137.80ex/s] 25%|██▌       | 98476/392702 [00:02<00:08, 35319.81ex/s] 26%|██▋       | 103539/392702 [00:02<00:08, 35679.64ex/s] 26%|██▌       | 101795/392702 [00:02<00:08, 34557.99ex/s] 26%|██▌       | 102053/392702 [00:02<00:08, 35030.97ex/s] 26%|██▌       | 102009/392702 [00:02<00:08, 34735.98ex/s] 27%|██▋       | 107108/392702 [00:03<00:08, 34251.97ex/s] 27%|██▋       | 105256/392702 [00:03<00:08, 33753.73ex/s] 27%|██▋       | 105557/392702 [00:03<00:08, 33777.80ex/s] 27%|██▋       | 105486/392702 [00:03<00:08, 34677.65ex/s] 28%|██▊       | 110684/392702 [00:03<00:08, 34690.09ex/s] 28%|██▊       | 108764/392702 [00:03<00:08, 34140.75ex/s] 28%|██▊       | 109041/392702 [00:03<00:08, 34088.92ex/s] 28%|██▊       | 109013/392702 [00:03<00:08, 34853.19ex/s] 29%|██▉       | 114222/392702 [00:03<00:07, 34892.29ex/s] 29%|██▊       | 112232/392702 [00:03<00:08, 34297.98ex/s] 29%|██▊       | 112550/392702 [00:03<00:08, 34383.06ex/s] 29%|██▊       | 112500/392702 [00:03<00:08, 34730.03ex/s] 30%|██▉       | 117790/392702 [00:03<00:07, 35122.81ex/s] 29%|██▉       | 115738/392702 [00:03<00:08, 34520.54ex/s] 30%|██▉       | 116049/392702 [00:03<00:08, 34560.14ex/s] 30%|██▉       | 116024/392702 [00:03<00:07, 34880.04ex/s] 31%|███       | 121337/392702 [00:03<00:07, 35223.88ex/s] 30%|███       | 119210/392702 [00:03<00:07, 34578.05ex/s] 30%|███       | 119564/392702 [00:03<00:07, 34732.27ex/s] 30%|███       | 119526/392702 [00:03<00:07, 34919.43ex/s] 32%|███▏      | 124869/392702 [00:03<00:07, 35250.56ex/s] 31%|███       | 122715/392702 [00:03<00:07, 34717.37ex/s] 31%|███▏      | 123041/392702 [00:03<00:07, 34606.15ex/s] 31%|███▏      | 123023/392702 [00:03<00:07, 34934.07ex/s] 33%|███▎      | 128397/392702 [00:03<00:07, 35200.42ex/s] 32%|███▏      | 126189/392702 [00:03<00:07, 34715.73ex/s] 32%|███▏      | 126504/392702 [00:03<00:07, 34334.08ex/s] 32%|███▏      | 126525/392702 [00:03<00:07, 34958.39ex/s] 34%|███▎      | 131919/392702 [00:03<00:07, 35147.67ex/s] 33%|███▎      | 129680/392702 [00:03<00:07, 34772.70ex/s] 33%|███▎      | 129943/392702 [00:03<00:07, 34350.50ex/s] 33%|███▎      | 130022/392702 [00:03<00:07, 34934.41ex/s] 34%|███▍      | 133159/392702 [00:03<00:07, 34721.89ex/s] 34%|███▍      | 135436/392702 [00:03<00:07, 34922.78ex/s] 34%|███▍      | 133398/392702 [00:03<00:07, 34408.40ex/s] 34%|███▍      | 133516/392702 [00:03<00:07, 34932.67ex/s] 35%|███▍      | 136632/392702 [00:03<00:07, 34701.45ex/s] 35%|███▌      | 138968/392702 [00:03<00:07, 35040.09ex/s] 35%|███▍      | 136874/392702 [00:03<00:07, 34510.86ex/s] 35%|███▍      | 137010/392702 [00:03<00:07, 34918.22ex/s] 36%|███▋      | 142489/392702 [00:04<00:07, 35090.05ex/s] 36%|███▌      | 140103/392702 [00:04<00:07, 34561.51ex/s] 36%|███▌      | 140326/392702 [00:04<00:07, 34509.52ex/s] 36%|███▌      | 140530/392702 [00:04<00:07, 35000.23ex/s] 37%|███▋      | 146001/392702 [00:04<00:07, 35098.24ex/s] 37%|███▋      | 143560/392702 [00:04<00:07, 34550.78ex/s] 37%|███▋      | 143781/392702 [00:04<00:07, 34520.21ex/s] 37%|███▋      | 144031/392702 [00:04<00:07, 34856.13ex/s] 38%|███▊      | 149521/392702 [00:04<00:06, 35125.93ex/s] 37%|███▋      | 147016/392702 [00:04<00:07, 34553.50ex/s] 37%|███▋      | 147236/392702 [00:04<00:07, 34529.03ex/s] 38%|███▊      | 147517/392702 [00:04<00:07, 34801.57ex/s] 38%|███▊      | 150472/392702 [00:04<00:07, 34168.57ex/s] 39%|███▉      | 153034/392702 [00:04<00:06, 34244.03ex/s] 38%|███▊      | 150690/392702 [00:04<00:07, 34529.39ex/s] 38%|███▊      | 150998/392702 [00:04<00:07, 34492.10ex/s] 39%|███▉      | 153915/392702 [00:04<00:06, 34245.17ex/s] 40%|███▉      | 156553/392702 [00:04<00:06, 34522.09ex/s] 39%|███▉      | 154144/392702 [00:04<00:06, 34508.74ex/s] 39%|███▉      | 154455/392702 [00:04<00:06, 34514.16ex/s] 40%|████      | 157341/392702 [00:04<00:06, 34197.77ex/s] 41%|████      | 160057/392702 [00:04<00:06, 34675.00ex/s] 40%|████      | 157602/392702 [00:04<00:06, 34528.32ex/s] 40%|████      | 157939/392702 [00:04<00:06, 34610.64ex/s] 41%|████      | 160769/392702 [00:04<00:06, 34220.28ex/s] 42%|████▏     | 163556/392702 [00:04<00:06, 34767.48ex/s] 41%|████      | 161055/392702 [00:04<00:06, 34427.90ex/s] 41%|████      | 161401/392702 [00:04<00:06, 34553.46ex/s] 42%|████▏     | 164192/392702 [00:04<00:06, 34173.03ex/s] 43%|████▎     | 167040/392702 [00:04<00:06, 34788.67ex/s] 42%|████▏     | 164498/392702 [00:04<00:06, 34070.45ex/s] 42%|████▏     | 164857/392702 [00:04<00:06, 34548.10ex/s] 43%|████▎     | 167610/392702 [00:04<00:06, 34174.57ex/s] 43%|████▎     | 170542/392702 [00:04<00:06, 34856.94ex/s] 43%|████▎     | 167933/392702 [00:04<00:06, 34152.34ex/s] 43%|████▎     | 168313/392702 [00:04<00:06, 34505.44ex/s] 44%|████▎     | 171028/392702 [00:04<00:06, 34097.32ex/s] 44%|████▍     | 174029/392702 [00:04<00:06, 33320.28ex/s] 44%|████▎     | 171349/392702 [00:04<00:06, 34109.06ex/s] 44%|████▎     | 171768/392702 [00:04<00:06, 34517.89ex/s] 44%|████▍     | 174438/392702 [00:05<00:06, 34067.12ex/s] 45%|████▌     | 177514/392702 [00:05<00:06, 33762.88ex/s] 45%|████▍     | 174777/392702 [00:05<00:06, 34157.32ex/s] 45%|████▍     | 175220/392702 [00:05<00:06, 34405.09ex/s] 45%|████▌     | 177853/392702 [00:05<00:06, 34090.22ex/s] 46%|████▌     | 180995/392702 [00:05<00:06, 34069.87ex/s] 45%|████▌     | 178200/392702 [00:05<00:06, 34178.22ex/s] 45%|████▌     | 178661/392702 [00:05<00:06, 34380.36ex/s] 46%|████▌     | 181263/392702 [00:05<00:06, 33992.58ex/s] 47%|████▋     | 184451/392702 [00:05<00:06, 34215.26ex/s] 46%|████▋     | 181640/392702 [00:05<00:06, 34242.03ex/s] 46%|████▋     | 182100/392702 [00:05<00:06, 34084.14ex/s] 47%|████▋     | 184663/392702 [00:05<00:06, 33968.77ex/s] 48%|████▊     | 187930/392702 [00:05<00:05, 34383.99ex/s] 47%|████▋     | 185065/392702 [00:05<00:06, 34231.82ex/s] 47%|████▋     | 185510/392702 [00:05<00:06, 32856.82ex/s] 48%|████▊     | 188060/392702 [00:05<00:06, 33922.30ex/s] 49%|████▊     | 191381/392702 [00:05<00:05, 34420.45ex/s] 48%|████▊     | 188501/392702 [00:05<00:05, 34270.14ex/s] 48%|████▊     | 188956/392702 [00:05<00:06, 33320.20ex/s] 49%|████▉     | 191456/392702 [00:05<00:05, 33932.37ex/s] 50%|████▉     | 194843/392702 [00:05<00:05, 34479.81ex/s] 49%|████▉     | 191929/392702 [00:05<00:05, 34219.64ex/s] 49%|████▉     | 192297/392702 [00:05<00:06, 33211.38ex/s] 50%|████▉     | 194850/392702 [00:05<00:05, 33932.41ex/s] 50%|█████     | 198294/392702 [00:05<00:05, 34472.04ex/s] 50%|████▉     | 195352/392702 [00:05<00:05, 34139.60ex/s] 50%|████▉     | 195720/392702 [00:05<00:05, 33507.99ex/s] 50%|█████     | 198244/392702 [00:05<00:05, 33876.31ex/s] 51%|█████▏    | 201754/392702 [00:05<00:05, 34509.63ex/s] 51%|█████     | 198767/392702 [00:05<00:05, 34098.08ex/s] 51%|█████     | 199129/392702 [00:05<00:05, 33679.06ex/s] 51%|█████▏    | 201651/392702 [00:05<00:05, 33933.71ex/s] 52%|█████▏    | 205207/392702 [00:05<00:05, 34470.60ex/s] 51%|█████▏    | 202177/392702 [00:05<00:05, 34001.03ex/s] 52%|█████▏    | 202550/392702 [00:05<00:05, 33835.51ex/s] 52%|█████▏    | 205045/392702 [00:05<00:05, 33834.08ex/s] 53%|█████▎    | 208662/392702 [00:05<00:05, 34493.58ex/s] 52%|█████▏    | 205578/392702 [00:05<00:05, 33733.28ex/s] 52%|█████▏    | 205963/392702 [00:05<00:05, 33922.61ex/s] 53%|█████▎    | 208435/392702 [00:06<00:05, 33853.50ex/s] 54%|█████▍    | 212112/392702 [00:06<00:05, 34441.99ex/s] 53%|█████▎    | 208996/392702 [00:06<00:05, 33863.29ex/s] 53%|█████▎    | 209366/392702 [00:06<00:05, 33952.44ex/s] 54%|█████▍    | 211821/392702 [00:06<00:05, 33843.99ex/s] 54%|█████▍    | 212383/392702 [00:06<00:05, 33815.09ex/s] 55%|█████▍    | 215557/392702 [00:06<00:05, 33361.41ex/s] 54%|█████▍    | 212775/392702 [00:06<00:05, 33991.85ex/s] 55%|█████▍    | 215206/392702 [00:06<00:05, 33729.05ex/s] 55%|█████▍    | 215775/392702 [00:06<00:05, 33843.92ex/s] 56%|█████▌    | 219000/392702 [00:06<00:05, 33625.08ex/s] 55%|█████▌    | 216176/392702 [00:06<00:05, 33956.57ex/s] 56%|█████▌    | 218580/392702 [00:06<00:05, 33349.18ex/s] 56%|█████▌    | 219160/392702 [00:06<00:05, 33841.59ex/s] 57%|█████▋    | 222426/392702 [00:06<00:05, 33811.93ex/s] 56%|█████▌    | 219606/392702 [00:06<00:05, 34058.18ex/s] 57%|█████▋    | 221916/392702 [00:06<00:05, 32636.45ex/s] 57%|█████▋    | 222559/392702 [00:06<00:05, 33883.91ex/s] 58%|█████▊    | 225849/392702 [00:06<00:04, 33933.95ex/s] 57%|█████▋    | 223013/392702 [00:06<00:04, 34036.11ex/s] 57%|█████▋    | 225249/392702 [00:06<00:05, 32838.90ex/s] 58%|█████▊    | 225948/392702 [00:06<00:04, 33882.84ex/s] 58%|█████▊    | 229251/392702 [00:06<00:04, 33957.72ex/s] 58%|█████▊    | 226418/392702 [00:06<00:04, 34036.83ex/s] 58%|█████▊    | 228604/392702 [00:06<00:04, 33048.03ex/s] 59%|█████▉    | 232661/392702 [00:06<00:04, 33999.44ex/s] 58%|█████▊    | 229337/392702 [00:06<00:04, 33824.16ex/s] 59%|█████▊    | 229822/392702 [00:06<00:04, 32984.62ex/s] 59%|█████▉    | 231941/392702 [00:06<00:04, 33143.62ex/s] 59%|█████▉    | 232720/392702 [00:06<00:04, 33774.62ex/s] 60%|██████    | 236063/392702 [00:06<00:04, 33420.40ex/s] 59%|█████▉    | 233184/392702 [00:06<00:04, 33171.98ex/s] 60%|█████▉    | 235275/392702 [00:06<00:04, 33200.91ex/s] 60%|██████    | 236098/392702 [00:06<00:04, 33746.29ex/s] 61%|██████    | 239489/392702 [00:06<00:04, 33666.14ex/s] 60%|██████    | 236577/392702 [00:06<00:04, 33395.63ex/s] 61%|██████    | 238614/392702 [00:06<00:04, 33256.40ex/s] 61%|██████    | 239473/392702 [00:06<00:04, 33717.27ex/s] 62%|██████▏   | 242895/392702 [00:06<00:04, 33783.08ex/s] 61%|██████    | 239964/392702 [00:06<00:04, 33535.08ex/s] 62%|██████▏   | 241941/392702 [00:07<00:04, 33090.79ex/s] 62%|██████▏   | 242845/392702 [00:07<00:04, 33472.92ex/s] 63%|██████▎   | 246283/392702 [00:07<00:04, 33809.99ex/s] 62%|██████▏   | 243321/392702 [00:07<00:04, 33512.00ex/s] 62%|██████▏   | 245251/392702 [00:07<00:04, 32734.00ex/s] 63%|██████▎   | 246193/392702 [00:07<00:04, 33194.65ex/s] 64%|██████▎   | 249666/392702 [00:07<00:04, 33624.45ex/s] 63%|██████▎   | 246675/392702 [00:07<00:04, 33402.85ex/s] 63%|██████▎   | 248589/392702 [00:07<00:04, 32922.99ex/s] 64%|██████▎   | 249572/392702 [00:07<00:04, 33370.51ex/s] 64%|██████▍   | 253030/392702 [00:07<00:04, 33281.19ex/s] 64%|██████▎   | 250017/392702 [00:07<00:04, 33399.55ex/s] 64%|██████▍   | 251907/392702 [00:07<00:04, 32997.59ex/s] 64%|██████▍   | 252916/392702 [00:07<00:04, 33391.10ex/s] 65%|██████▌   | 256426/392702 [00:07<00:04, 33480.70ex/s] 65%|██████▍   | 253380/392702 [00:07<00:04, 33468.05ex/s] 65%|██████▍   | 255208/392702 [00:07<00:04, 32948.03ex/s] 65%|██████▌   | 256256/392702 [00:07<00:04, 33378.14ex/s] 66%|██████▌   | 259784/392702 [00:07<00:03, 33509.46ex/s] 65%|██████▌   | 256755/392702 [00:07<00:04, 33549.67ex/s] 66%|██████▌   | 258538/392702 [00:07<00:04, 33050.38ex/s] 66%|██████▌   | 259614/392702 [00:07<00:03, 33435.81ex/s] 67%|██████▋   | 263154/392702 [00:07<00:03, 33563.92ex/s] 66%|██████▌   | 260111/392702 [00:07<00:03, 33487.98ex/s] 67%|██████▋   | 261844/392702 [00:07<00:03, 33033.93ex/s] 67%|██████▋   | 262958/392702 [00:07<00:03, 33432.51ex/s] 68%|██████▊   | 266544/392702 [00:07<00:03, 33661.37ex/s] 67%|██████▋   | 263469/392702 [00:07<00:03, 33512.33ex/s] 68%|██████▊   | 265148/392702 [00:07<00:03, 32987.87ex/s] 68%|██████▊   | 266302/392702 [00:07<00:03, 33394.55ex/s] 69%|██████▊   | 269934/392702 [00:07<00:03, 33732.30ex/s] 68%|██████▊   | 266821/392702 [00:07<00:03, 33490.00ex/s] 68%|██████▊   | 268454/392702 [00:07<00:03, 33009.41ex/s] 69%|██████▊   | 269653/392702 [00:07<00:03, 33428.82ex/s] 70%|██████▉   | 273308/392702 [00:07<00:03, 33637.27ex/s] 69%|██████▉   | 270171/392702 [00:07<00:03, 32568.22ex/s] 69%|██████▉   | 271756/392702 [00:07<00:03, 32935.91ex/s] 70%|██████▉   | 272997/392702 [00:07<00:03, 33377.60ex/s] 70%|███████   | 276677/392702 [00:07<00:03, 33650.53ex/s] 70%|██████▉   | 273558/392702 [00:07<00:03, 32947.63ex/s] 70%|███████   | 275050/392702 [00:08<00:03, 32908.13ex/s] 70%|███████   | 276335/392702 [00:08<00:03, 33319.66ex/s] 71%|███████▏  | 280043/392702 [00:08<00:03, 33622.56ex/s] 71%|███████   | 276927/392702 [00:08<00:03, 33166.52ex/s] 71%|███████   | 278356/392702 [00:08<00:03, 32950.85ex/s] 71%|███████   | 279668/392702 [00:08<00:03, 33292.75ex/s] 72%|███████▏  | 283406/392702 [00:08<00:03, 33609.83ex/s] 71%|███████▏  | 280274/392702 [00:08<00:03, 33256.42ex/s] 72%|███████▏  | 281652/392702 [00:08<00:03, 32671.95ex/s] 73%|███████▎  | 286776/392702 [00:08<00:03, 33636.72ex/s] 72%|███████▏  | 282998/392702 [00:08<00:03, 32710.81ex/s] 72%|███████▏  | 283620/392702 [00:08<00:03, 33315.61ex/s] 73%|███████▎  | 284920/392702 [00:08<00:03, 32609.26ex/s] 74%|███████▍  | 290140/392702 [00:08<00:03, 33547.85ex/s] 73%|███████▎  | 286305/392702 [00:08<00:03, 32815.19ex/s] 73%|███████▎  | 286979/392702 [00:08<00:03, 33397.08ex/s] 73%|███████▎  | 288211/392702 [00:08<00:03, 32697.71ex/s] 75%|███████▍  | 293495/392702 [00:08<00:02, 33507.07ex/s] 74%|███████▎  | 289602/392702 [00:08<00:03, 32859.52ex/s] 74%|███████▍  | 290321/392702 [00:08<00:03, 33323.04ex/s] 74%|███████▍  | 291485/392702 [00:08<00:03, 32708.06ex/s] 76%|███████▌  | 296846/392702 [00:08<00:02, 33500.60ex/s] 75%|███████▍  | 292890/392702 [00:08<00:03, 32843.88ex/s] 75%|███████▍  | 293655/392702 [00:08<00:02, 33318.37ex/s] 75%|███████▌  | 294782/392702 [00:08<00:02, 32784.90ex/s] 75%|███████▌  | 296176/392702 [00:08<00:02, 32777.22ex/s] 76%|███████▋  | 300197/392702 [00:08<00:02, 33020.47ex/s] 76%|███████▌  | 296988/392702 [00:08<00:02, 33304.43ex/s] 76%|███████▌  | 298061/392702 [00:08<00:02, 32747.00ex/s] 76%|███████▋  | 299460/392702 [00:08<00:02, 32795.22ex/s] 77%|███████▋  | 303535/392702 [00:08<00:02, 33125.94ex/s] 76%|███████▋  | 300319/392702 [00:08<00:02, 33170.75ex/s] 77%|███████▋  | 301354/392702 [00:08<00:02, 32798.87ex/s] 77%|███████▋  | 302740/392702 [00:08<00:02, 32776.11ex/s] 78%|███████▊  | 306871/392702 [00:08<00:02, 33194.31ex/s] 77%|███████▋  | 303650/392702 [00:08<00:02, 33210.92ex/s] 78%|███████▊  | 304640/392702 [00:08<00:02, 32817.08ex/s] 78%|███████▊  | 306018/392702 [00:08<00:02, 32693.97ex/s] 79%|███████▉  | 310192/392702 [00:08<00:02, 33192.64ex/s] 78%|███████▊  | 306972/392702 [00:08<00:02, 33154.02ex/s] 78%|███████▊  | 307922/392702 [00:09<00:02, 32727.57ex/s] 79%|███████▉  | 309303/392702 [00:09<00:02, 32739.44ex/s] 80%|███████▉  | 313512/392702 [00:09<00:02, 33185.91ex/s] 79%|███████▉  | 310288/392702 [00:09<00:02, 33053.61ex/s] 79%|███████▉  | 311195/392702 [00:09<00:02, 32610.96ex/s] 80%|███████▉  | 312578/392702 [00:09<00:02, 32337.48ex/s] 81%|████████  | 316832/392702 [00:09<00:02, 32912.54ex/s] 80%|███████▉  | 313594/392702 [00:09<00:02, 31544.99ex/s] 80%|████████  | 314457/392702 [00:09<00:02, 31151.00ex/s] 80%|████████  | 315814/392702 [00:09<00:02, 31653.75ex/s] 82%|████████▏ | 320125/392702 [00:09<00:02, 31022.92ex/s] 81%|████████  | 316872/392702 [00:09<00:02, 31905.04ex/s] 81%|████████  | 317710/392702 [00:09<00:02, 31550.57ex/s] 81%|████████  | 318984/392702 [00:09<00:02, 31620.94ex/s] 82%|████████▏ | 323445/392702 [00:09<00:02, 31643.01ex/s] 82%|████████▏ | 320144/392702 [00:09<00:02, 32144.70ex/s] 82%|████████▏ | 320967/392702 [00:09<00:02, 31846.90ex/s] 82%|████████▏ | 322219/392702 [00:09<00:02, 31833.95ex/s] 83%|████████▎ | 326775/392702 [00:09<00:02, 32122.39ex/s] 82%|████████▏ | 323419/392702 [00:09<00:02, 32321.34ex/s] 83%|████████▎ | 324161/392702 [00:09<00:02, 31482.89ex/s] 83%|████████▎ | 325477/392702 [00:09<00:02, 32053.44ex/s] 84%|████████▍ | 330081/392702 [00:09<00:01, 32395.64ex/s] 83%|████████▎ | 326688/392702 [00:09<00:02, 32429.55ex/s] 83%|████████▎ | 327418/392702 [00:09<00:02, 31798.89ex/s] 84%|████████▎ | 328725/392702 [00:09<00:01, 32179.48ex/s] 85%|████████▍ | 333378/392702 [00:09<00:01, 32565.55ex/s] 84%|████████▍ | 329978/392702 [00:09<00:01, 32567.04ex/s] 84%|████████▍ | 330664/392702 [00:09<00:01, 31994.27ex/s] 85%|████████▍ | 331971/392702 [00:09<00:01, 32261.26ex/s] 86%|████████▌ | 336674/392702 [00:09<00:01, 32681.13ex/s] 85%|████████▍ | 333243/392702 [00:09<00:01, 32589.43ex/s] 85%|████████▌ | 333904/392702 [00:09<00:01, 32112.95ex/s] 85%|████████▌ | 335205/392702 [00:09<00:01, 32282.04ex/s] 87%|████████▋ | 339975/392702 [00:09<00:01, 32777.53ex/s] 86%|████████▌ | 336527/392702 [00:09<00:01, 32661.32ex/s] 86%|████████▌ | 337141/392702 [00:09<00:01, 32188.22ex/s] 86%|████████▌ | 338462/392702 [00:09<00:01, 32365.46ex/s] 87%|████████▋ | 343257/392702 [00:10<00:01, 31917.90ex/s] 87%|████████▋ | 339807/392702 [00:09<00:01, 32700.48ex/s] 87%|████████▋ | 340371/392702 [00:10<00:01, 32220.49ex/s] 87%|████████▋ | 341700/392702 [00:10<00:01, 31086.49ex/s] 88%|████████▊ | 346529/392702 [00:10<00:01, 32152.23ex/s] 87%|████████▋ | 343079/392702 [00:10<00:01, 32662.56ex/s] 87%|████████▋ | 343595/392702 [00:10<00:01, 32195.25ex/s] 88%|████████▊ | 344959/392702 [00:10<00:01, 31520.77ex/s] 89%|████████▉ | 349793/392702 [00:10<00:01, 32295.05ex/s] 88%|████████▊ | 346347/392702 [00:10<00:01, 32663.45ex/s] 88%|████████▊ | 346816/392702 [00:10<00:01, 32190.55ex/s] 89%|████████▊ | 348187/392702 [00:10<00:01, 31742.32ex/s] 90%|████████▉ | 353057/392702 [00:10<00:01, 32396.92ex/s] 89%|████████▉ | 349614/392702 [00:10<00:01, 32663.11ex/s] 89%|████████▉ | 350036/392702 [00:10<00:01, 32176.62ex/s] 89%|████████▉ | 351447/392702 [00:10<00:01, 31992.81ex/s] 91%|█████████ | 356346/392702 [00:10<00:01, 32542.72ex/s] 90%|████████▉ | 352881/392702 [00:10<00:01, 32634.85ex/s] 90%|████████▉ | 353255/392702 [00:10<00:01, 32145.24ex/s] 90%|█████████ | 354701/392702 [00:10<00:01, 32152.74ex/s] 92%|█████████▏| 359639/392702 [00:10<00:01, 32655.27ex/s] 91%|█████████ | 356145/392702 [00:10<00:01, 32604.13ex/s] 91%|█████████ | 356470/392702 [00:10<00:01, 32095.79ex/s] 91%|█████████ | 357955/392702 [00:10<00:01, 32267.86ex/s] 92%|█████████▏| 362925/392702 [00:10<00:00, 32714.94ex/s] 92%|█████████▏| 359406/392702 [00:10<00:01, 32585.28ex/s] 92%|█████████▏| 359680/392702 [00:10<00:01, 32077.17ex/s] 92%|█████████▏| 361194/392702 [00:10<00:00, 32301.65ex/s] 93%|█████████▎| 366198/392702 [00:10<00:00, 32632.78ex/s] 92%|█████████▏| 362665/392702 [00:10<00:00, 32448.62ex/s] 92%|█████████▏| 362888/392702 [00:10<00:00, 32066.03ex/s] 93%|█████████▎| 364433/392702 [00:10<00:00, 32327.65ex/s] 94%|█████████▍| 369463/392702 [00:10<00:00, 32616.59ex/s] 93%|█████████▎| 365911/392702 [00:10<00:00, 32437.04ex/s] 93%|█████████▎| 366095/392702 [00:10<00:00, 31982.97ex/s] 94%|█████████▎| 367668/392702 [00:10<00:00, 32287.25ex/s] 95%|█████████▍| 372726/392702 [00:10<00:00, 32570.18ex/s] 94%|█████████▍| 369155/392702 [00:10<00:00, 32382.91ex/s] 94%|█████████▍| 369310/392702 [00:10<00:00, 32030.05ex/s] 94%|█████████▍| 370898/392702 [00:10<00:00, 32253.31ex/s] 96%|█████████▌| 375984/392702 [00:11<00:00, 32559.79ex/s] 95%|█████████▍| 372400/392702 [00:10<00:00, 32402.81ex/s] 95%|█████████▍| 372514/392702 [00:11<00:00, 31969.97ex/s] 95%|█████████▌| 374125/392702 [00:11<00:00, 32138.97ex/s] 97%|█████████▋| 379241/392702 [00:11<00:00, 32425.13ex/s] 96%|█████████▌| 375641/392702 [00:11<00:00, 32387.56ex/s] 96%|█████████▌| 375712/392702 [00:11<00:00, 31913.22ex/s] 96%|█████████▌| 377340/392702 [00:11<00:00, 32092.05ex/s] 97%|█████████▋| 382484/392702 [00:11<00:00, 32420.87ex/s] 96%|█████████▋| 378880/392702 [00:11<00:00, 32350.93ex/s] 96%|█████████▋| 378904/392702 [00:11<00:00, 30844.46ex/s] 97%|█████████▋| 380550/392702 [00:11<00:00, 31054.87ex/s] 98%|█████████▊| 385727/392702 [00:11<00:00, 31333.06ex/s] 97%|█████████▋| 382116/392702 [00:11<00:00, 30836.02ex/s] 97%|█████████▋| 382046/392702 [00:11<00:00, 31014.55ex/s] 98%|█████████▊| 383741/392702 [00:11<00:00, 31305.12ex/s] 99%|█████████▉| 388976/392702 [00:11<00:00, 31668.98ex/s] 98%|█████████▊| 385355/392702 [00:11<00:00, 31285.24ex/s] 98%|█████████▊| 385220/392702 [00:11<00:00, 31227.94ex/s] 99%|█████████▊| 386896/392702 [00:11<00:00, 31377.25ex/s]100%|█████████▉| 392206/392702 [00:11<00:00, 31853.08ex/s]100%|██████████| 392702/392702 [00:11<00:00, 34017.40ex/s]
 99%|█████████▉| 388597/392702 [00:11<00:00, 31614.97ex/s] 99%|█████████▉| 388391/392702 [00:11<00:00, 31370.49ex/s] 99%|█████████▉| 390085/392702 [00:11<00:00, 31527.07ex/s]100%|█████████▉| 391828/392702 [00:11<00:00, 31818.78ex/s]100%|██████████| 392702/392702 [00:11<00:00, 33758.75ex/s]
100%|█████████▉| 391562/392702 [00:11<00:00, 31470.65ex/s]100%|██████████| 392702/392702 [00:11<00:00, 33629.16ex/s]
100%|██████████| 392702/392702 [00:11<00:00, 33445.70ex/s]
Downloading and preparing dataset glue/qnli (download: 10.14 MiB, generated: 27.11 MiB, post-processed: Unknown size, total: 37.24 MiB) to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
Downloading:   0%|          | 0.00/10.6M [00:00<?, ?B/s]Downloading:   0%|          | 17.4k/10.6M [00:00<02:27, 71.8kB/s]Downloading:   0%|          | 52.2k/10.6M [00:00<02:05, 84.4kB/s]Downloading:   1%|▏         | 135k/10.6M [00:00<01:36, 109kB/s]  Downloading:   3%|▎         | 314k/10.6M [00:00<01:10, 146kB/s]Downloading:   6%|▌         | 656k/10.6M [00:01<00:49, 200kB/s]Downloading:  13%|█▎        | 1.34M/10.6M [00:01<00:33, 278kB/s]Downloading:  25%|██▌       | 2.70M/10.6M [00:01<00:20, 388kB/s]Downloading:  51%|█████     | 5.43M/10.6M [00:01<00:09, 547kB/s]Downloading:  91%|█████████ | 9.66M/10.6M [00:02<00:01, 771kB/s]Downloading: 100%|██████████| 10.6M/10.6M [00:02<00:00, 4.80MB/s]
0 examples [00:00, ? examples/s]3982 examples [00:00, 39813.39 examples/s]9188 examples [00:00, 42834.62 examples/s]14125 examples [00:00, 44605.11 examples/s]19374 examples [00:00, 46708.06 examples/s]24413 examples [00:00, 47752.76 examples/s]29526 examples [00:00, 48717.09 examples/s]34620 examples [00:00, 49361.52 examples/s]39949 examples [00:00, 50477.20 examples/s]44797 examples [00:00, 47044.93 examples/s]50000 examples [00:01, 47824.91 examples/s]55190 examples [00:01, 48976.59 examples/s]60151 examples [00:01, 49164.68 examples/s]65468 examples [00:01, 50300.03 examples/s]70487 examples [00:01, 50197.87 examples/s]75499 examples [00:01, 49884.46 examples/s]80483 examples [00:01, 48770.74 examples/s]85832 examples [00:01, 50095.37 examples/s]90875 examples [00:01, 50192.31 examples/s]96191 examples [00:01, 51046.05 examples/s]101305 examples [00:02, 50728.62 examples/s]                                            0 examples [00:00, ? examples/s]4376 examples [00:00, 43753.68 examples/s]                                          0 examples [00:00, ? examples/s]5192 examples [00:00, 51917.08 examples/s]                                          Dataset glue downloaded and prepared to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/104743 [00:00<?, ?ex/s]  0%|          | 0/104743 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/104743 [00:00<?, ?ex/s]  0%|          | 0/104743 [00:00<?, ?ex/s]  3%|▎         | 3009/104743 [00:00<00:03, 30082.00ex/s]  3%|▎         | 3000/104743 [00:00<00:03, 29893.69ex/s]  3%|▎         | 2883/104743 [00:00<00:03, 28827.83ex/s]  3%|▎         | 3192/104743 [00:00<00:03, 31919.04ex/s]  6%|▋         | 6595/104743 [00:00<00:03, 31608.57ex/s]  6%|▋         | 6627/104743 [00:00<00:03, 31557.32ex/s]  6%|▌         | 6452/104743 [00:00<00:03, 30590.48ex/s]  7%|▋         | 6835/104743 [00:00<00:02, 33148.85ex/s] 10%|▉         | 10269/104743 [00:00<00:02, 32990.05ex/s] 10%|▉         | 10246/104743 [00:00<00:02, 32816.49ex/s] 10%|▉         | 9988/104743 [00:00<00:02, 31878.72ex/s] 10%|▉         | 10126/104743 [00:00<00:02, 33076.68ex/s] 13%|█▎        | 13946/104743 [00:00<00:02, 34039.13ex/s] 13%|█▎        | 13878/104743 [00:00<00:02, 33791.99ex/s] 13%|█▎        | 13553/104743 [00:00<00:02, 32921.93ex/s] 13%|█▎        | 13740/104743 [00:00<00:02, 33938.59ex/s] 17%|█▋        | 17603/104743 [00:00<00:02, 34760.13ex/s] 16%|█▌        | 16882/104743 [00:00<00:02, 32570.46ex/s] 16%|█▌        | 16324/104743 [00:00<00:02, 31162.07ex/s] 17%|█▋        | 17358/104743 [00:00<00:02, 34580.70ex/s] 20%|██        | 21259/104743 [00:00<00:02, 35280.21ex/s] 20%|█▉        | 20520/104743 [00:00<00:02, 33625.40ex/s] 19%|█▉        | 19930/104743 [00:00<00:02, 32484.98ex/s] 20%|██        | 20980/104743 [00:00<00:02, 35054.74ex/s] 24%|██▍       | 24912/104743 [00:00<00:02, 35643.72ex/s] 23%|██▎       | 24109/104743 [00:00<00:02, 34273.87ex/s] 22%|██▏       | 23497/104743 [00:00<00:02, 33376.91ex/s] 23%|██▎       | 24552/104743 [00:00<00:02, 35251.09ex/s] 27%|██▋       | 28552/104743 [00:00<00:02, 35865.40ex/s] 26%|██▋       | 27746/104743 [00:00<00:02, 34874.75ex/s] 26%|██▌       | 27057/104743 [00:00<00:02, 34012.54ex/s] 27%|██▋       | 28146/104743 [00:00<00:02, 35454.13ex/s] 31%|███       | 32198/104743 [00:00<00:02, 36040.16ex/s] 30%|██▉       | 31371/104743 [00:00<00:02, 35274.90ex/s] 29%|██▉       | 30453/104743 [00:00<00:02, 33994.75ex/s] 30%|███       | 31724/104743 [00:00<00:02, 35550.11ex/s] 34%|███▍      | 35871/104743 [00:01<00:01, 36242.62ex/s] 33%|███▎      | 34875/104743 [00:01<00:01, 35203.91ex/s] 32%|███▏      | 34014/104743 [00:01<00:02, 34462.26ex/s] 34%|███▎      | 35306/104743 [00:01<00:01, 35627.90ex/s] 38%|███▊      | 39515/104743 [00:01<00:01, 36300.30ex/s] 37%|███▋      | 38456/104743 [00:01<00:01, 35382.66ex/s] 36%|███▌      | 37591/104743 [00:01<00:01, 34843.17ex/s] 37%|███▋      | 38896/104743 [00:01<00:01, 35706.46ex/s] 41%|████      | 43148/104743 [00:01<00:01, 36307.95ex/s] 40%|████      | 42054/104743 [00:01<00:01, 35558.75ex/s] 39%|███▉      | 41134/104743 [00:01<00:01, 35015.08ex/s] 40%|████      | 42406/104743 [00:01<00:01, 35429.46ex/s] 45%|████▍     | 46758/104743 [00:01<00:01, 36243.03ex/s] 44%|████▎     | 45675/104743 [00:01<00:01, 35749.49ex/s] 43%|████▎     | 44694/104743 [00:01<00:01, 35186.12ex/s] 44%|████▍     | 45964/104743 [00:01<00:01, 35472.80ex/s] 48%|████▊     | 50371/104743 [00:01<00:01, 36206.57ex/s] 47%|████▋     | 49270/104743 [00:01<00:01, 35806.43ex/s] 46%|████▌     | 48191/104743 [00:01<00:01, 34540.69ex/s] 47%|████▋     | 49482/104743 [00:01<00:01, 35097.47ex/s] 52%|█████▏    | 53973/104743 [00:01<00:01, 35895.82ex/s] 50%|█████     | 52890/104743 [00:01<00:01, 35922.57ex/s] 49%|████▉     | 51692/104743 [00:01<00:01, 34678.03ex/s] 51%|█████     | 52981/104743 [00:01<00:01, 35064.07ex/s] 55%|█████▍    | 57587/104743 [00:01<00:01, 35967.43ex/s] 54%|█████▍    | 56487/104743 [00:01<00:01, 35936.64ex/s] 53%|█████▎    | 55216/104743 [00:01<00:01, 34842.42ex/s] 54%|█████▍    | 56502/104743 [00:01<00:01, 35105.73ex/s] 58%|█████▊    | 61181/104743 [00:01<00:01, 35956.38ex/s] 57%|█████▋    | 60078/104743 [00:01<00:01, 35927.32ex/s] 56%|█████▌    | 58725/104743 [00:01<00:01, 34914.62ex/s] 57%|█████▋    | 60020/104743 [00:01<00:01, 35126.96ex/s] 62%|██████▏   | 64771/104743 [00:01<00:01, 35425.91ex/s] 61%|██████    | 63665/104743 [00:01<00:01, 35834.60ex/s] 59%|█████▉    | 62235/104743 [00:01<00:01, 34968.54ex/s] 61%|██████    | 63564/104743 [00:01<00:01, 35218.82ex/s] 65%|██████▌   | 68324/104743 [00:01<00:01, 35456.96ex/s] 64%|██████▍   | 67248/104743 [00:01<00:01, 35832.84ex/s] 63%|██████▎   | 65767/104743 [00:01<00:01, 35071.24ex/s] 64%|██████▍   | 67082/104743 [00:01<00:01, 35193.85ex/s] 69%|██████▊   | 71938/104743 [00:02<00:00, 35659.03ex/s] 68%|██████▊   | 70847/104743 [00:02<00:00, 35878.76ex/s] 67%|██████▋   | 70609/104743 [00:02<00:00, 35213.85ex/s] 66%|██████▌   | 69273/104743 [00:02<00:01, 34432.82ex/s] 72%|███████▏  | 75529/104743 [00:02<00:00, 35731.39ex/s] 71%|███████   | 74433/104743 [00:02<00:00, 35795.63ex/s] 69%|██████▉   | 72796/104743 [00:02<00:00, 34665.53ex/s] 71%|███████   | 74129/104743 [00:02<00:00, 35108.23ex/s] 76%|███████▌  | 79116/104743 [00:02<00:00, 35771.55ex/s] 74%|███████▍  | 78012/104743 [00:02<00:00, 35650.33ex/s] 74%|███████▍  | 77639/104743 [00:02<00:00, 35095.05ex/s] 73%|███████▎  | 76264/104743 [00:02<00:00, 34433.90ex/s] 79%|███████▉  | 82736/104743 [00:02<00:00, 35898.06ex/s] 78%|███████▊  | 81577/104743 [00:02<00:00, 35301.79ex/s] 77%|███████▋  | 81148/104743 [00:02<00:00, 35030.57ex/s] 76%|███████▌  | 79752/104743 [00:02<00:00, 34564.68ex/s] 82%|████████▏ | 86326/104743 [00:02<00:00, 35881.87ex/s] 81%|████████▏ | 85138/104743 [00:02<00:00, 35391.16ex/s] 81%|████████  | 84651/104743 [00:02<00:00, 35017.79ex/s] 79%|███████▉  | 83254/104743 [00:02<00:00, 34697.43ex/s] 86%|████████▌ | 89915/104743 [00:02<00:00, 35826.28ex/s] 85%|████████▍ | 88728/104743 [00:02<00:00, 35542.12ex/s] 83%|████████▎ | 86770/104743 [00:02<00:00, 34834.01ex/s] 84%|████████▍ | 88153/104743 [00:02<00:00, 34859.91ex/s] 89%|████████▉ | 93498/104743 [00:02<00:00, 35703.64ex/s] 88%|████████▊ | 92283/104743 [00:02<00:00, 35535.81ex/s] 86%|████████▌ | 90270/104743 [00:02<00:00, 34881.19ex/s] 87%|████████▋ | 91639/104743 [00:02<00:00, 34802.77ex/s] 93%|█████████▎| 97069/104743 [00:02<00:00, 35667.19ex/s] 92%|█████████▏| 95845/104743 [00:02<00:00, 35558.85ex/s] 90%|████████▉ | 93767/104743 [00:02<00:00, 34905.58ex/s] 91%|█████████ | 95120/104743 [00:02<00:00, 34789.52ex/s] 95%|█████████▍| 99402/104743 [00:02<00:00, 35545.91ex/s] 96%|█████████▌| 100636/104743 [00:02<00:00, 35106.83ex/s] 93%|█████████▎| 97258/104743 [00:02<00:00, 34804.58ex/s] 94%|█████████▍| 98599/104743 [00:02<00:00, 34040.24ex/s] 98%|█████████▊| 102957/104743 [00:02<00:00, 35424.80ex/s] 99%|█████████▉| 104149/104743 [00:02<00:00, 34576.10ex/s]100%|██████████| 104743/104743 [00:02<00:00, 35663.33ex/s]
 96%|█████████▌| 100739/104743 [00:02<00:00, 34776.42ex/s] 97%|█████████▋| 102007/104743 [00:02<00:00, 33731.48ex/s]100%|██████████| 104743/104743 [00:02<00:00, 35397.20ex/s]
100%|██████████| 104743/104743 [00:02<00:00, 34959.79ex/s]
 99%|█████████▉| 104217/104743 [00:03<00:00, 34757.25ex/s]100%|██████████| 104743/104743 [00:03<00:00, 34559.08ex/s]
Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
Downloading:   0%|          | 0.00/377k [00:00<?, ?B/s]Downloading:   5%|▍         | 17.4k/377k [00:00<00:04, 73.8kB/s]Downloading:  14%|█▍        | 52.2k/377k [00:00<00:03, 86.9kB/s]Downloading:  38%|███▊      | 143k/377k [00:00<00:02, 113kB/s]  Downloading:  83%|████████▎ | 314k/377k [00:00<00:00, 152kB/s]Downloading: 100%|██████████| 377k/377k [00:00<00:00, 399kB/s]
0 examples [00:00, ? examples/s]8312 examples [00:00, 83117.90 examples/s]                                          0 examples [00:00, ? examples/s]                                0 examples [00:00, ? examples/s]                                Dataset glue downloaded and prepared to /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.
  0%|          | 0/8551 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/8551 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/8551 [00:00<?, ?ex/s] 37%|███▋      | 3137/8551 [00:00<00:00, 31369.21ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
 48%|████▊     | 4095/8551 [00:00<00:00, 40943.89ex/s]  0%|          | 0/8551 [00:00<?, ?ex/s] 28%|██▊       | 2393/8551 [00:00<00:00, 23929.11ex/s] 67%|██████▋   | 5750/8551 [00:00<00:00, 29586.04ex/s]100%|██████████| 8551/8551 [00:00<00:00, 43272.96ex/s]
 48%|████▊     | 4077/8551 [00:00<00:00, 40765.57ex/s] 58%|█████▊    | 4998/8551 [00:00<00:00, 24526.38ex/s] 96%|█████████▋| 8240/8551 [00:00<00:00, 28003.13ex/s]100%|██████████| 8551/8551 [00:00<00:00, 27292.61ex/s]
100%|██████████| 8551/8551 [00:00<00:00, 43369.82ex/s]
100%|██████████| 8551/8551 [00:00<00:00, 30518.76ex/s]
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/137 [00:00<?, ?ex/s]Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
100%|██████████| 137/137 [00:00<00:00, 20766.13ex/s]
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-d8dc5adba9403905.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-d8dc5adba9403905.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/rte/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-d8dc5adba9403905.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/999 [00:00<?, ?ex/s]100%|██████████| 999/999 [00:00<00:00, 39228.83ex/s]
validation
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-9ec1dd7cb8ebc877.arrow
validation
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-9ec1dd7cb8ebc877.arrow
validation
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-9ec1dd7cb8ebc877.arrow
validation
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
validation
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
validation
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
validation
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
validation
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/203 [00:00<?, ?ex/s]100%|██████████| 203/203 [00:00<00:00, 24658.09ex/s]
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-e215f8bed2aac8b3.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-e215f8bed2aac8b3.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-e215f8bed2aac8b3.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/749 [00:00<?, ?ex/s]100%|██████████| 749/749 [00:00<00:00, 22729.16ex/s]
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-2c63173409e3ccc2.arrow
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-2c63173409e3ccc2.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/stsb/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-2c63173409e3ccc2.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
  0%|          | 0/999 [00:00<?, ?ex/s]100%|██████████| 999/999 [00:00<00:00, 25080.26ex/s]
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Traceback (most recent call last):
  File "./finetune_t5_trainer.py", line 307, in <module>
    main()
  File "./finetune_t5_trainer.py", line 176, in main
    eval_datasets = ({task: dataset_class.get(task, seed=data_args.data_seed).get_dataset(
  File "./finetune_t5_trainer.py", line 176, in <dictcomp>
    eval_datasets = ({task: dataset_class.get(task, seed=data_args.data_seed).get_dataset(
  File "/home/jesus.ortizbarajas/Projects/hyperformer_alqvca/hyperformer/hyperformer/data/tasks.py", line 146, in get_dataset
    dataset = self.load_dataset(split=split)
  File "/home/jesus.ortizbarajas/Projects/hyperformer_alqvca/hyperformer/hyperformer/data/tasks.py", line 544, in load_dataset
    return datasets.load_dataset('glue', 'mnli', split=split)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/load.py", line 616, in load_dataset
    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 674, in as_dataset
    datasets = utils.map_nested(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/utils/py_utils.py", line 225, in map_nested
    return function(data_struct)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 694, in _build_single_dataset
    ds = self._as_dataset(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 762, in _as_dataset
    dataset_kwargs = ArrowReader(self._cache_dir, self.info).read(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 208, in read
    files = self.get_file_instructions(name, instructions, split_infos)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 183, in get_file_instructions
    file_instructions = make_file_instructions(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 108, in make_file_instructions
    absolute_instructions = instruction.to_absolute(name2len)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 551, in to_absolute
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 551, in <listcomp>
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 404, in _rel_to_abs_instr
    raise ValueError('Unknown split "{}". Should be one of {}.'.format(split, list(name2len)))
ValueError: Unknown split "validation". Should be one of ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'].
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-93e867205788ffb6.arrow
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-93e867205788ffb6.arrow
Loading cached processed dataset at /home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-93e867205788ffb6.arrow
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Traceback (most recent call last):
  File "./finetune_t5_trainer.py", line 307, in <module>
    main()
  File "./finetune_t5_trainer.py", line 176, in main
    eval_datasets = ({task: dataset_class.get(task, seed=data_args.data_seed).get_dataset(
  File "./finetune_t5_trainer.py", line 176, in <dictcomp>
    eval_datasets = ({task: dataset_class.get(task, seed=data_args.data_seed).get_dataset(
  File "/home/jesus.ortizbarajas/Projects/hyperformer_alqvca/hyperformer/hyperformer/data/tasks.py", line 146, in get_dataset
    dataset = self.load_dataset(split=split)
  File "/home/jesus.ortizbarajas/Projects/hyperformer_alqvca/hyperformer/hyperformer/data/tasks.py", line 544, in load_dataset
    return datasets.load_dataset('glue', 'mnli', split=split)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/load.py", line 616, in load_dataset
    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 674, in as_dataset
    datasets = utils.map_nested(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/utils/py_utils.py", line 225, in map_nested
    return function(data_struct)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 694, in _build_single_dataset
    ds = self._as_dataset(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 762, in _as_dataset
    dataset_kwargs = ArrowReader(self._cache_dir, self.info).read(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 208, in read
    files = self.get_file_instructions(name, instructions, split_infos)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 183, in get_file_instructions
    file_instructions = make_file_instructions(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 108, in make_file_instructions
    absolute_instructions = instruction.to_absolute(name2len)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 551, in to_absolute
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 551, in <listcomp>
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 404, in _rel_to_abs_instr
    raise ValueError('Unknown split "{}". Should be one of {}.'.format(split, list(name2len)))
ValueError: Unknown split "validation". Should be one of ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'].
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Traceback (most recent call last):
  File "./finetune_t5_trainer.py", line 307, in <module>
    main()
  File "./finetune_t5_trainer.py", line 176, in main
    eval_datasets = ({task: dataset_class.get(task, seed=data_args.data_seed).get_dataset(
  File "./finetune_t5_trainer.py", line 176, in <dictcomp>
    eval_datasets = ({task: dataset_class.get(task, seed=data_args.data_seed).get_dataset(
  File "/home/jesus.ortizbarajas/Projects/hyperformer_alqvca/hyperformer/hyperformer/data/tasks.py", line 146, in get_dataset
    dataset = self.load_dataset(split=split)
  File "/home/jesus.ortizbarajas/Projects/hyperformer_alqvca/hyperformer/hyperformer/data/tasks.py", line 544, in load_dataset
    return datasets.load_dataset('glue', 'mnli', split=split)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/load.py", line 616, in load_dataset
    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 674, in as_dataset
    datasets = utils.map_nested(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/utils/py_utils.py", line 225, in map_nested
    return function(data_struct)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 694, in _build_single_dataset
    ds = self._as_dataset(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 762, in _as_dataset
    dataset_kwargs = ArrowReader(self._cache_dir, self.info).read(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 208, in read
    files = self.get_file_instructions(name, instructions, split_infos)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 183, in get_file_instructions
    file_instructions = make_file_instructions(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 108, in make_file_instructions
    absolute_instructions = instruction.to_absolute(name2len)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 551, in to_absolute
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 551, in <listcomp>
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 404, in _rel_to_abs_instr
    raise ValueError('Unknown split "{}". Should be one of {}.'.format(split, list(name2len)))
ValueError: Unknown split "validation". Should be one of ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'].
Reusing dataset glue (/home/jesus.ortizbarajas/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
Traceback (most recent call last):
  File "./finetune_t5_trainer.py", line 307, in <module>
    main()
  File "./finetune_t5_trainer.py", line 176, in main
    eval_datasets = ({task: dataset_class.get(task, seed=data_args.data_seed).get_dataset(
  File "./finetune_t5_trainer.py", line 176, in <dictcomp>
    eval_datasets = ({task: dataset_class.get(task, seed=data_args.data_seed).get_dataset(
  File "/home/jesus.ortizbarajas/Projects/hyperformer_alqvca/hyperformer/hyperformer/data/tasks.py", line 146, in get_dataset
    dataset = self.load_dataset(split=split)
  File "/home/jesus.ortizbarajas/Projects/hyperformer_alqvca/hyperformer/hyperformer/data/tasks.py", line 544, in load_dataset
    return datasets.load_dataset('glue', 'mnli', split=split)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/load.py", line 616, in load_dataset
    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 674, in as_dataset
    datasets = utils.map_nested(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/utils/py_utils.py", line 225, in map_nested
    return function(data_struct)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 694, in _build_single_dataset
    ds = self._as_dataset(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/builder.py", line 762, in _as_dataset
    dataset_kwargs = ArrowReader(self._cache_dir, self.info).read(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 208, in read
    files = self.get_file_instructions(name, instructions, split_infos)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 183, in get_file_instructions
    file_instructions = make_file_instructions(
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 108, in make_file_instructions
    absolute_instructions = instruction.to_absolute(name2len)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 551, in to_absolute
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 551, in <listcomp>
    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/datasets-1.2.0-py3.8.egg/datasets/arrow_reader.py", line 404, in _rel_to_abs_instr
    raise ValueError('Unknown split "{}". Should be one of {}.'.format(split, list(name2len)))
ValueError: Unknown split "validation". Should be one of ['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'].
Traceback (most recent call last):
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/distributed/launch.py", line 260, in <module>
    main()
  File "/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/distributed/launch.py", line 255, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/jesus.ortizbarajas/.conda/envs/hyperformer/bin/python3', '-u', './finetune_t5_trainer.py', '--local_rank=3', 'configs/hyperformer++_glue.json']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
