06/08/2024 12:54:59 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
06/08/2024 12:54:59 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
06/08/2024 12:54:59 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
06/08/2024 12:54:59 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
06/08/2024 12:54:59 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='outputs/hyperformer_alqvca_v2_10++/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0003, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100, max_steps=65536, warmup_steps=500, logging_dir='runs/Jun08_12-54-53_gpu-12', logging_first_step=True, logging_steps=200, save_steps=1000, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='outputs/hyperformer_alqvca_v2_10++/', disable_tqdm=True, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=True, label_smoothing=0.1, predict_with_generate=True, adafactor=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear', temperature=10, train_adapters=True, do_test=True, eval_output_dir=None, generate_classifier_weights=False, optimize_from_scratch=False, optimize_from_scratch_with_loading_model=False, split_validation_test=True, print_num_parameters=True, compute_memory=False, compute_time=False)
06/08/2024 12:55:00 - WARNING - __main__ -   model path loaded from : t5-base
06/08/2024 12:55:00 - WARNING - __main__ -   model path loaded from : t5-base
06/08/2024 12:55:00 - WARNING - __main__ -   model path loaded from : t5-base
06/08/2024 12:55:00 - WARNING - __main__ -   model path loaded from : t5-base
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.movieTrivia', 'task_embedding_controller.task_to_embeddings.movie', 'task_embedding_controller.task_to_embeddings.restaurant', 'task_embedding_controller.task_to_embeddings.atis', 'task_embedding_controller.task_to_embeddings.snips', 'task_embedding_controller.task_to_embeddings.mtod', 'task_embedding_controller.task_to_embeddings.mtop', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.movieTrivia', 'task_embedding_controller.task_to_embeddings.movie', 'task_embedding_controller.task_to_embeddings.restaurant', 'task_embedding_controller.task_to_embeddings.atis', 'task_embedding_controller.task_to_embeddings.snips', 'task_embedding_controller.task_to_embeddings.mtod', 'task_embedding_controller.task_to_embeddings.mtop', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.movieTrivia', 'task_embedding_controller.task_to_embeddings.movie', 'task_embedding_controller.task_to_embeddings.restaurant', 'task_embedding_controller.task_to_embeddings.atis', 'task_embedding_controller.task_to_embeddings.snips', 'task_embedding_controller.task_to_embeddings.mtod', 'task_embedding_controller.task_to_embeddings.mtop', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.movieTrivia', 'task_embedding_controller.task_to_embeddings.movie', 'task_embedding_controller.task_to_embeddings.restaurant', 'task_embedding_controller.task_to_embeddings.atis', 'task_embedding_controller.task_to_embeddings.snips', 'task_embedding_controller.task_to_embeddings.mtod', 'task_embedding_controller.task_to_embeddings.mtop', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/08/2024 12:55:05 - INFO - __main__ -   T5ForConditionalGeneration(
  (task_embedding_controller): TaskEmbeddingController(
    (task_to_embeddings): ParameterDict(
        (movieTrivia): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (movie): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (restaurant): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (atis): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (snips): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (mtod): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (mtop): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
    )
  )
  (shared): Embedding(32128, 768)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 768)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (9): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (10): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (11): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (adapter_layers_hyper_net): AdapterLayersOneHyperNetController(
      (layer_id_embeddings): Embedding(12, 64)
      (adapters_block_type): Embedding(6, 64)
      (task_hypernet): TaskHyperNet(
        (task_embeding_generator): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): ReLU()
          (2): Linear(in_features=128, out_features=64, bias=True)
        )
      )
      (LayerNorm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (up_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=768, bias=True)
        )
      )
      (down_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=24, bias=True)
        )
      )
      (lora_a_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (lora_b_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (post_layernorm_hypernet): LayerNormHyperNet(
        (weight_generator): Linear(in_features=64, out_features=768, bias=True)
        (bias_generator): Linear(in_features=64, out_features=768, bias=True)
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(32128, 768)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (9): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (10): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (11): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (adapter_layers_hyper_net): AdapterLayersOneHyperNetController(
      (layer_id_embeddings): Embedding(12, 64)
      (adapters_block_type): Embedding(6, 64)
      (task_hypernet): TaskHyperNet(
        (task_embeding_generator): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): ReLU()
          (2): Linear(in_features=128, out_features=64, bias=True)
        )
      )
      (LayerNorm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (up_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=768, bias=True)
        )
      )
      (down_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=24, bias=True)
        )
      )
      (lora_a_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (lora_b_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (post_layernorm_hypernet): LayerNormHyperNet(
        (weight_generator): Linear(in_features=64, out_features=768, bias=True)
        (bias_generator): Linear(in_features=64, out_features=768, bias=True)
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=768, out_features=32128, bias=False)
)
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.movieTrivia
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.movie
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.restaurant
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.atis
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.snips
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.mtod
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.mtop
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.0.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.0.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.1.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.1.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.2.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.2.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.3.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.3.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.4.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.4.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.5.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.5.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.6.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.6.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.7.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.7.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.8.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.8.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.9.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.9.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.10.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.10.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.11.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.block.11.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.layer_id_embeddings.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.adapters_block_type.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.LayerNorm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.LayerNorm.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name encoder.final_layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.0.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.0.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.0.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.1.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.1.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.1.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.2.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.2.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.2.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.3.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.3.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.3.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.4.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.4.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.4.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.5.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.5.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.5.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.6.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.6.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.6.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.7.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.7.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.7.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.8.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.8.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.8.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.9.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.9.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.9.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.10.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.10.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.10.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.11.layer.0.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.11.layer.1.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.block.11.layer.2.layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.layer_id_embeddings.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.adapters_block_type.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.LayerNorm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.LayerNorm.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias
06/08/2024 12:55:05 - INFO - __main__ -   Parameter name decoder.final_layer_norm.weight
06/08/2024 12:55:05 - INFO - __main__ -   Total trainable parameters 6784368
06/08/2024 12:55:05 - INFO - __main__ -   Total parameters 229640688
  0%|          | 0/703 [00:00<?, ?ex/s]  0%|          | 0/703 [00:00<?, ?ex/s]  0%|          | 0/703 [00:00<?, ?ex/s]100%|| 703/703 [00:00<00:00, 31672.64ex/s]
100%|| 703/703 [00:00<00:00, 31686.25ex/s]
100%|| 703/703 [00:00<00:00, 22992.61ex/s]
  0%|          | 0/703 [00:00<?, ?ex/s]100%|| 703/703 [00:00<00:00, 25434.28ex/s]
  0%|          | 0/879 [00:00<?, ?ex/s]  0%|          | 0/879 [00:00<?, ?ex/s]  0%|          | 0/879 [00:00<?, ?ex/s]  0%|          | 0/879 [00:00<?, ?ex/s]100%|| 879/879 [00:00<00:00, 32239.90ex/s]
100%|| 879/879 [00:00<00:00, 32146.85ex/s]
100%|| 879/879 [00:00<00:00, 31887.43ex/s]
100%|| 879/879 [00:00<00:00, 27582.34ex/s]
  0%|          | 0/689 [00:00<?, ?ex/s]  0%|          | 0/689 [00:00<?, ?ex/s]  0%|          | 0/689 [00:00<?, ?ex/s]100%|| 689/689 [00:00<00:00, 18813.19ex/s]
100%|| 689/689 [00:00<00:00, 18596.85ex/s]
100%|| 689/689 [00:00<00:00, 18498.04ex/s]
  0%|          | 0/689 [00:00<?, ?ex/s]100%|| 689/689 [00:00<00:00, 21495.97ex/s]
  0%|          | 0/447 [00:00<?, ?ex/s]  0%|          | 0/447 [00:00<?, ?ex/s]  0%|          | 0/447 [00:00<?, ?ex/s]100%|| 447/447 [00:00<00:00, 32229.49ex/s]
100%|| 447/447 [00:00<00:00, 23627.35ex/s]
100%|| 447/447 [00:00<00:00, 23634.80ex/s]
  0%|          | 0/447 [00:00<?, ?ex/s]100%|| 447/447 [00:00<00:00, 17607.57ex/s]
  0%|          | 0/1308 [00:00<?, ?ex/s]  0%|          | 0/1308 [00:00<?, ?ex/s]  0%|          | 0/1308 [00:00<?, ?ex/s]100%|| 1308/1308 [00:00<00:00, 30205.58ex/s]
100%|| 1308/1308 [00:00<00:00, 31323.47ex/s]
100%|| 1308/1308 [00:00<00:00, 30640.49ex/s]
  0%|          | 0/1308 [00:00<?, ?ex/s]100%|| 1308/1308 [00:00<00:00, 24020.34ex/s]
  0%|          | 0/3052 [00:00<?, ?ex/s]  0%|          | 0/3052 [00:00<?, ?ex/s]  0%|          | 0/3052 [00:00<?, ?ex/s]100%|| 3052/3052 [00:00<00:00, 31869.96ex/s]
 85%| | 2579/3052 [00:00<00:00, 25782.77ex/s]100%|| 3052/3052 [00:00<00:00, 31287.08ex/s]
100%|| 3052/3052 [00:00<00:00, 26438.60ex/s]
  0%|          | 0/3052 [00:00<?, ?ex/s] 91%| | 2766/3052 [00:00<00:00, 27651.79ex/s]100%|| 3052/3052 [00:00<00:00, 27838.40ex/s]
  0%|          | 0/1566 [00:00<?, ?ex/s]  0%|          | 0/1566 [00:00<?, ?ex/s]  0%|          | 0/1566 [00:00<?, ?ex/s]100%|| 1566/1566 [00:00<00:00, 31484.87ex/s]
100%|| 1566/1566 [00:00<00:00, 31095.25ex/s]
100%|| 1566/1566 [00:00<00:00, 23592.62ex/s]
  0%|          | 0/1566 [00:00<?, ?ex/s]100%|| 1566/1566 [00:00<00:00, 22931.86ex/s]
  0%|          | 0/78 [00:00<?, ?ex/s]  0%|          | 0/78 [00:00<?, ?ex/s]  0%|          | 0/78 [00:00<?, ?ex/s]100%|| 78/78 [00:00<00:00, 18269.71ex/s]
  0%|          | 0/78 [00:00<?, ?ex/s]100%|| 78/78 [00:00<00:00, 13755.86ex/s]
100%|| 78/78 [00:00<00:00, 13651.97ex/s]
100%|| 78/78 [00:00<00:00, 17721.45ex/s]
  0%|          | 0/97 [00:00<?, ?ex/s]  0%|          | 0/97 [00:00<?, ?ex/s]  0%|          | 0/97 [00:00<?, ?ex/s]  0%|          | 0/97 [00:00<?, ?ex/s]100%|| 97/97 [00:00<00:00, 13816.26ex/s]
100%|| 97/97 [00:00<00:00, 17444.05ex/s]
100%|| 97/97 [00:00<00:00, 7808.52ex/s]
100%|| 97/97 [00:00<00:00, 7279.56ex/s]
  0%|          | 0/76 [00:00<?, ?ex/s]  0%|          | 0/76 [00:00<?, ?ex/s]  0%|          | 0/76 [00:00<?, ?ex/s]  0%|          | 0/76 [00:00<?, ?ex/s]100%|| 76/76 [00:00<00:00, 13959.58ex/s]
100%|| 76/76 [00:00<00:00, 13601.60ex/s]
100%|| 76/76 [00:00<00:00, 13828.18ex/s]
100%|| 76/76 [00:00<00:00, 13998.20ex/s]
  0%|          | 0/50 [00:00<?, ?ex/s]  0%|          | 0/50 [00:00<?, ?ex/s]  0%|          | 0/50 [00:00<?, ?ex/s]  0%|          | 0/50 [00:00<?, ?ex/s]100%|| 50/50 [00:00<00:00, 12584.93ex/s]
100%|| 50/50 [00:00<00:00, 12392.32ex/s]100%|| 50/50 [00:00<00:00, 12750.97ex/s]

100%|| 50/50 [00:00<00:00, 13334.72ex/s]
  0%|          | 0/70 [00:00<?, ?ex/s]  0%|          | 0/70 [00:00<?, ?ex/s]  0%|          | 0/70 [00:00<?, ?ex/s]100%|| 70/70 [00:00<00:00, 13687.07ex/s]
100%|| 70/70 [00:00<00:00, 13483.41ex/s]
100%|| 70/70 [00:00<00:00, 13605.25ex/s]
  0%|          | 0/70 [00:00<?, ?ex/s]100%|| 70/70 [00:00<00:00, 12535.81ex/s]
  0%|          | 0/418 [00:00<?, ?ex/s]  0%|          | 0/418 [00:00<?, ?ex/s]  0%|          | 0/418 [00:00<?, ?ex/s]  0%|          | 0/418 [00:00<?, ?ex/s]100%|| 418/418 [00:00<00:00, 17897.66ex/s]
100%|| 418/418 [00:00<00:00, 14082.08ex/s]
100%|| 418/418 [00:00<00:00, 14066.83ex/s]
100%|| 418/418 [00:00<00:00, 14207.38ex/s]
  0%|          | 0/223 [00:00<?, ?ex/s]  0%|          | 0/223 [00:00<?, ?ex/s]  0%|          | 0/223 [00:00<?, ?ex/s]100%|| 223/223 [00:00<00:00, 16637.25ex/s]
100%|| 223/223 [00:00<00:00, 17003.21ex/s]
100%|| 223/223 [00:00<00:00, 14126.71ex/s]
  0%|          | 0/223 [00:00<?, ?ex/s]100%|| 223/223 [00:00<00:00, 14110.31ex/s]
  0%|          | 0/1953 [00:00<?, ?ex/s]  0%|          | 0/1953 [00:00<?, ?ex/s]  0%|          | 0/1953 [00:00<?, ?ex/s]100%|| 1953/1953 [00:00<00:00, 31209.43ex/s]
100%|| 1953/1953 [00:00<00:00, 31212.88ex/s]
  0%|          | 0/1953 [00:00<?, ?ex/s]100%|| 1953/1953 [00:00<00:00, 22957.27ex/s]
100%|| 1953/1953 [00:00<00:00, 29059.12ex/s]
  0%|          | 0/2443 [00:00<?, ?ex/s]  0%|          | 0/2443 [00:00<?, ?ex/s]  0%|          | 0/2443 [00:00<?, ?ex/s]100%|| 2443/2443 [00:00<00:00, 31857.62ex/s]
100%|| 2443/2443 [00:00<00:00, 31598.94ex/s]
100%|| 2443/2443 [00:00<00:00, 31603.62ex/s]
  0%|          | 0/2443 [00:00<?, ?ex/s]100%|| 2443/2443 [00:00<00:00, 27908.41ex/s]
  0%|          | 0/1521 [00:00<?, ?ex/s]  0%|          | 0/1521 [00:00<?, ?ex/s]  0%|          | 0/1521 [00:00<?, ?ex/s]100%|| 1521/1521 [00:00<00:00, 31664.63ex/s]
100%|| 1521/1521 [00:00<00:00, 31873.30ex/s]
100%|| 1521/1521 [00:00<00:00, 24399.20ex/s]
  0%|          | 0/1521 [00:00<?, ?ex/s]100%|| 1521/1521 [00:00<00:00, 25577.38ex/s]
  0%|          | 0/893 [00:00<?, ?ex/s]  0%|          | 0/893 [00:00<?, ?ex/s]  0%|          | 0/893 [00:00<?, ?ex/s]100%|| 893/893 [00:00<00:00, 31946.31ex/s]
100%|| 893/893 [00:00<00:00, 28884.07ex/s]
100%|| 893/893 [00:00<00:00, 32132.13ex/s]
  0%|          | 0/893 [00:00<?, ?ex/s]100%|| 893/893 [00:00<00:00, 22643.95ex/s]
  0%|          | 0/700 [00:00<?, ?ex/s]  0%|          | 0/700 [00:00<?, ?ex/s]  0%|          | 0/700 [00:00<?, ?ex/s]100%|| 700/700 [00:00<00:00, 30210.24ex/s]
100%|| 700/700 [00:00<00:00, 24887.79ex/s]
100%|| 700/700 [00:00<00:00, 24729.52ex/s]
  0%|          | 0/700 [00:00<?, ?ex/s]100%|| 700/700 [00:00<00:00, 20941.01ex/s]
  0%|          | 0/8621 [00:00<?, ?ex/s]  0%|          | 0/8621 [00:00<?, ?ex/s]  0%|          | 0/8621 [00:00<?, ?ex/s] 10%|         | 894/8621 [00:00<00:00, 8938.79ex/s]  0%|          | 0/8621 [00:00<?, ?ex/s] 20%|        | 1696/8621 [00:00<00:00, 13382.86ex/s] 20%|        | 1696/8621 [00:00<00:00, 13367.87ex/s] 46%|     | 4000/8621 [00:00<00:00, 11360.82ex/s] 52%|    | 4521/8621 [00:00<00:00, 15891.24ex/s] 52%|    | 4489/8621 [00:00<00:00, 15846.11ex/s] 20%|        | 1696/8621 [00:00<00:00, 11743.07ex/s] 81%| | 7016/8621 [00:00<00:00, 13973.84ex/s] 89%| | 7661/8621 [00:00<00:00, 18654.66ex/s] 88%| | 7562/8621 [00:00<00:00, 18539.08ex/s]100%|| 8621/8621 [00:00<00:00, 24549.49ex/s]
 53%|    | 4537/8621 [00:00<00:00, 14250.92ex/s]100%|| 8621/8621 [00:00<00:00, 23490.69ex/s]
100%|| 8621/8621 [00:00<00:00, 23337.59ex/s]
 89%| | 7638/8621 [00:00<00:00, 17007.93ex/s]100%|| 8621/8621 [00:00<00:00, 22930.49ex/s]
  0%|          | 0/4386 [00:00<?, ?ex/s]  0%|          | 0/4386 [00:00<?, ?ex/s]  0%|          | 0/4386 [00:00<?, ?ex/s] 72%|  | 3179/4386 [00:00<00:00, 31787.15ex/s] 73%|  | 3189/4386 [00:00<00:00, 31884.10ex/s] 60%|    | 2653/4386 [00:00<00:00, 26525.28ex/s]100%|| 4386/4386 [00:00<00:00, 31823.57ex/s]
100%|| 4386/4386 [00:00<00:00, 31925.52ex/s]
100%|| 4386/4386 [00:00<00:00, 28243.65ex/s]
06/08/2024 12:55:19 - INFO - utils.utils -   ***** arguments metrics *****
06/08/2024 12:55:19 - INFO - utils.utils -     adafactor = False
06/08/2024 12:55:19 - INFO - utils.utils -     adam_beta1 = 0.9
06/08/2024 12:55:19 - INFO - utils.utils -     adam_beta2 = 0.999
06/08/2024 12:55:19 - INFO - utils.utils -     adam_epsilon = 1e-08
06/08/2024 12:55:19 - INFO - utils.utils -     adapter_config_name = meta-adapter
06/08/2024 12:55:19 - INFO - utils.utils -     adapters = None
06/08/2024 12:55:19 - INFO - utils.utils -     add_layer_norm_after_adapter = True
06/08/2024 12:55:19 - INFO - utils.utils -     add_layer_norm_before_adapter = False
06/08/2024 12:55:19 - INFO - utils.utils -     attention_dropout = None
06/08/2024 12:55:19 - INFO - utils.utils -     cache_dir = None
06/08/2024 12:55:19 - INFO - utils.utils -     compute_memory = False
06/08/2024 12:55:19 - INFO - utils.utils -     compute_time = False
06/08/2024 12:55:19 - INFO - utils.utils -     conditional_layer_norm = True
06/08/2024 12:55:19 - INFO - utils.utils -     config_name = None
06/08/2024 12:55:19 - INFO - utils.utils -     data_seed = 42
06/08/2024 12:55:19 - INFO - utils.utils -     dataloader_drop_last = False
06/08/2024 12:55:19 - INFO - utils.utils -     dataloader_num_workers = 0
06/08/2024 12:55:19 - INFO - utils.utils -     debug = False
06/08/2024 12:55:19 - INFO - utils.utils -     decoder_layerdrop = None
06/08/2024 12:55:19 - INFO - utils.utils -     disable_tqdm = True
06/08/2024 12:55:19 - INFO - utils.utils -     do_eval = True
06/08/2024 12:55:19 - INFO - utils.utils -     do_predict = False
06/08/2024 12:55:19 - INFO - utils.utils -     do_test = True
06/08/2024 12:55:19 - INFO - utils.utils -     do_train = True
06/08/2024 12:55:19 - INFO - utils.utils -     dropout = None
06/08/2024 12:55:19 - INFO - utils.utils -     efficient_unique_hyper_net = True
06/08/2024 12:55:19 - INFO - utils.utils -     encoder_layerdrop = None
06/08/2024 12:55:19 - INFO - utils.utils -     eval_accumulation_steps = None
06/08/2024 12:55:19 - INFO - utils.utils -     eval_beams = 1
06/08/2024 12:55:19 - INFO - utils.utils -     eval_output_dir = None
06/08/2024 12:55:19 - INFO - utils.utils -     eval_steps = 1000
06/08/2024 12:55:19 - INFO - utils.utils -     eval_tasks = ['movieTrivia', 'movie', 'restaurant', 'atis', 'snips', 'mtod', 'mtop']
06/08/2024 12:55:19 - INFO - utils.utils -     evaluate_during_training = False
06/08/2024 12:55:19 - INFO - utils.utils -     fp16 = False
06/08/2024 12:55:19 - INFO - utils.utils -     fp16_opt_level = O1
06/08/2024 12:55:19 - INFO - utils.utils -     freeze_embeds = False
06/08/2024 12:55:19 - INFO - utils.utils -     freeze_encoder = False
06/08/2024 12:55:19 - INFO - utils.utils -     freeze_model = False
06/08/2024 12:55:19 - INFO - utils.utils -     freeze_model_but_lm_head = False
06/08/2024 12:55:19 - INFO - utils.utils -     freeze_model_but_task_embeddings = False
06/08/2024 12:55:19 - INFO - utils.utils -     generate_classifier_weights = False
06/08/2024 12:55:19 - INFO - utils.utils -     gradient_accumulation_steps = 1
06/08/2024 12:55:19 - INFO - utils.utils -     greater_is_better = True
06/08/2024 12:55:19 - INFO - utils.utils -     hidden_dim = 128
06/08/2024 12:55:19 - INFO - utils.utils -     ignore_pad_token_for_loss = True
06/08/2024 12:55:19 - INFO - utils.utils -     label_names = None
06/08/2024 12:55:19 - INFO - utils.utils -     label_smoothing = 0.1
06/08/2024 12:55:19 - INFO - utils.utils -     learning_rate = 0.0003
06/08/2024 12:55:19 - INFO - utils.utils -     load_best_model_at_end = True
06/08/2024 12:55:19 - INFO - utils.utils -     local_rank = 0
06/08/2024 12:55:19 - INFO - utils.utils -     logging_dir = runs/Jun08_12-54-53_gpu-12
06/08/2024 12:55:19 - INFO - utils.utils -     logging_first_step = True
06/08/2024 12:55:19 - INFO - utils.utils -     logging_steps = 200
06/08/2024 12:55:19 - INFO - utils.utils -     lr_scheduler = linear
06/08/2024 12:55:19 - INFO - utils.utils -     max_grad_norm = 1.0
06/08/2024 12:55:19 - INFO - utils.utils -     max_source_length = 128
06/08/2024 12:55:19 - INFO - utils.utils -     max_steps = 65536
06/08/2024 12:55:19 - INFO - utils.utils -     max_target_length = 128
06/08/2024 12:55:19 - INFO - utils.utils -     metric_for_best_model = loss
06/08/2024 12:55:19 - INFO - utils.utils -     model_name_or_path = t5-base
06/08/2024 12:55:19 - INFO - utils.utils -     n_test = -1
06/08/2024 12:55:19 - INFO - utils.utils -     n_train = -1
06/08/2024 12:55:19 - INFO - utils.utils -     n_val = -1
06/08/2024 12:55:19 - INFO - utils.utils -     no_cuda = False
06/08/2024 12:55:19 - INFO - utils.utils -     non_linearity = gelu_new
06/08/2024 12:55:19 - INFO - utils.utils -     not_load_t5_checkpoint = False
06/08/2024 12:55:19 - INFO - utils.utils -     num_train_epochs = 100
06/08/2024 12:55:19 - INFO - utils.utils -     optimize_from_scratch = False
06/08/2024 12:55:19 - INFO - utils.utils -     optimize_from_scratch_with_loading_model = False
06/08/2024 12:55:19 - INFO - utils.utils -     output_dir = outputs/hyperformer_alqvca_v2_10++/
06/08/2024 12:55:19 - INFO - utils.utils -     overwrite_output_dir = True
06/08/2024 12:55:19 - INFO - utils.utils -     past_index = -1
06/08/2024 12:55:19 - INFO - utils.utils -     per_device_eval_batch_size = 32
06/08/2024 12:55:19 - INFO - utils.utils -     per_device_train_batch_size = 32
06/08/2024 12:55:19 - INFO - utils.utils -     per_gpu_eval_batch_size = None
06/08/2024 12:55:19 - INFO - utils.utils -     per_gpu_train_batch_size = None
06/08/2024 12:55:19 - INFO - utils.utils -     predict_with_generate = True
06/08/2024 12:55:19 - INFO - utils.utils -     prediction_loss_only = False
06/08/2024 12:55:19 - INFO - utils.utils -     print_num_parameters = True
06/08/2024 12:55:19 - INFO - utils.utils -     projected_task_embedding_dim = 64
06/08/2024 12:55:19 - INFO - utils.utils -     reduction_factor = 32
06/08/2024 12:55:19 - INFO - utils.utils -     remove_unused_columns = False
06/08/2024 12:55:19 - INFO - utils.utils -     run_name = outputs/hyperformer_alqvca_v2_10++/
06/08/2024 12:55:19 - INFO - utils.utils -     save_steps = 1000
06/08/2024 12:55:19 - INFO - utils.utils -     save_total_limit = 1
06/08/2024 12:55:19 - INFO - utils.utils -     seed = 42
06/08/2024 12:55:19 - INFO - utils.utils -     split_validation_test = True
06/08/2024 12:55:19 - INFO - utils.utils -     task_embedding_dim = 64
06/08/2024 12:55:19 - INFO - utils.utils -     task_embeddings = None
06/08/2024 12:55:19 - INFO - utils.utils -     task_hidden_dim = 128
06/08/2024 12:55:19 - INFO - utils.utils -     tasks = ['movieTrivia', 'movie', 'restaurant', 'atis', 'snips', 'mtod', 'mtop']
06/08/2024 12:55:19 - INFO - utils.utils -     temperature = 10
06/08/2024 12:55:19 - INFO - utils.utils -     test_max_target_length = 128
06/08/2024 12:55:19 - INFO - utils.utils -     tokenizer_name = t5-base
06/08/2024 12:55:19 - INFO - utils.utils -     tpu_metrics_debug = False
06/08/2024 12:55:19 - INFO - utils.utils -     tpu_num_cores = None
06/08/2024 12:55:19 - INFO - utils.utils -     train_adapters = True
06/08/2024 12:55:19 - INFO - utils.utils -     train_adapters_blocks = True
06/08/2024 12:55:19 - INFO - utils.utils -     train_task_embeddings = False
06/08/2024 12:55:19 - INFO - utils.utils -     unfreeze_layer_norms = True
06/08/2024 12:55:19 - INFO - utils.utils -     unfreeze_lm_head = False
06/08/2024 12:55:19 - INFO - utils.utils -     unfreeze_model = False
06/08/2024 12:55:19 - INFO - utils.utils -     unique_hyper_net = False
06/08/2024 12:55:19 - INFO - utils.utils -     unique_hyper_net_layer_norm = True
06/08/2024 12:55:19 - INFO - utils.utils -     val_max_target_length = 128
06/08/2024 12:55:19 - INFO - utils.utils -     warmup_steps = 500
06/08/2024 12:55:19 - INFO - utils.utils -     weight_decay = 0.0
  0%|          | 0/4386 [00:00<?, ?ex/s] 52%|    | 2279/4386 [00:00<00:00, 22789.86ex/s]100%|| 4386/4386 [00:00<00:00, 26179.89ex/s]
06/08/2024 12:55:19 - INFO - third_party.trainers.t5_trainer -   ***** Running training *****
06/08/2024 12:55:19 - INFO - third_party.trainers.t5_trainer -     Num examples = 8644
06/08/2024 12:55:19 - INFO - third_party.trainers.t5_trainer -     Num Epochs = 979
06/08/2024 12:55:19 - INFO - third_party.trainers.t5_trainer -     Instantaneous batch size per device = 32
06/08/2024 12:55:19 - INFO - third_party.trainers.t5_trainer -     Total train batch size (w. parallel, distributed & accumulation) = 128
06/08/2024 12:55:19 - INFO - third_party.trainers.t5_trainer -     Gradient Accumulation steps = 1
06/08/2024 12:55:19 - INFO - third_party.trainers.t5_trainer -     Total optimization steps = 65536
{'loss': 4562.5888671875, 'learning_rate': 6e-07, 'epoch': 0.014925373134328358}
{'loss': 2884.9684604663944, 'learning_rate': 0.00011999999999999999, 'epoch': 2.9850746268656714}
{'loss': 1872.17375, 'learning_rate': 0.00023999999999999998, 'epoch': 5.970149253731344}
{'loss': 1725.5478125, 'learning_rate': 0.00029953871701826677, 'epoch': 8.955223880597014}
{'loss': 1523.695625, 'learning_rate': 0.0002986161510548004, 'epoch': 11.940298507462687}
{'loss': 1433.889375, 'learning_rate': 0.000297693585091334, 'epoch': 14.925373134328359}
{'loss': 1413.753125, 'learning_rate': 0.0002967710191278676, 'epoch': 17.91044776119403}
{'loss': 1383.66625, 'learning_rate': 0.0002958484531644012, 'epoch': 20.895522388059703}
{'loss': 1387.53625, 'learning_rate': 0.00029492588720093487, 'epoch': 23.880597014925375}
{'loss': 1329.00625, 'learning_rate': 0.00029400332123746847, 'epoch': 26.865671641791046}
{'loss': 1353.08, 'learning_rate': 0.00029308075527400206, 'epoch': 29.850746268656717}
{'loss': 1379.84, 'learning_rate': 0.00029215818931053566, 'epoch': 32.83582089552239}
{'loss': 1313.31625, 'learning_rate': 0.0002912356233470693, 'epoch': 35.82089552238806}
{'loss': 1327.455, 'learning_rate': 0.0002903130573836029, 'epoch': 38.80597014925373}
{'loss': 1317.03875, 'learning_rate': 0.0002893904914201365, 'epoch': 41.791044776119406}
{'loss': 1345.6825, 'learning_rate': 0.0002884679254566701, 'epoch': 44.776119402985074}
{'loss': 1331.8325, 'learning_rate': 0.00028754535949320376, 'epoch': 47.76119402985075}
{'loss': 1324.2975, 'learning_rate': 0.00028662279352973736, 'epoch': 50.74626865671642}
{'loss': 1290.0625, 'learning_rate': 0.00028570022756627095, 'epoch': 53.73134328358209}
{'loss': 1297.865, 'learning_rate': 0.00028477766160280455, 'epoch': 56.71641791044776}
{'loss': 1298.8525, 'learning_rate': 0.0002838550956393382, 'epoch': 59.701492537313435}
{'loss': 1328.1575, 'learning_rate': 0.0002829325296758718, 'epoch': 62.6865671641791}
{'loss': 1327.61, 'learning_rate': 0.0002820099637124054, 'epoch': 65.67164179104478}
{'loss': 1315.605, 'learning_rate': 0.00028108739774893905, 'epoch': 68.65671641791045}
{'loss': 1274.35, 'learning_rate': 0.0002801648317854726, 'epoch': 71.64179104477611}
{'loss': 1279.0425, 'learning_rate': 0.00027924226582200625, 'epoch': 74.6268656716418}
{'loss': 1266.1175, 'learning_rate': 0.00027831969985853984, 'epoch': 77.61194029850746}
{'loss': 1302.9275, 'learning_rate': 0.0002773971338950735, 'epoch': 80.59701492537313}
{'loss': 1311.3775, 'learning_rate': 0.0002764745679316071, 'epoch': 83.58208955223881}
{'loss': 1264.3375, 'learning_rate': 0.0002755520019681407, 'epoch': 86.56716417910448}
{'loss': 1317.3675, 'learning_rate': 0.0002746294360046743, 'epoch': 89.55223880597015}
{'loss': 1277.755, 'learning_rate': 0.00027370687004120794, 'epoch': 92.53731343283582}
{'loss': 1235.33, 'learning_rate': 0.00027278430407774154, 'epoch': 95.5223880597015}
{'loss': 1283.705, 'learning_rate': 0.00027186173811427514, 'epoch': 98.50746268656717}
{'loss': 1284.13, 'learning_rate': 0.00027093917215080873, 'epoch': 101.49253731343283}
{'loss': 1315.785, 'learning_rate': 0.0002700166061873424, 'epoch': 104.4776119402985}
{'loss': 1357.835, 'learning_rate': 0.000269094040223876, 'epoch': 107.46268656716418}
{'loss': 1256.08, 'learning_rate': 0.0002681714742604096, 'epoch': 110.44776119402985}
{'loss': 1247.645, 'learning_rate': 0.0002672489082969432, 'epoch': 113.43283582089552}
{'loss': 1267.39, 'learning_rate': 0.00026632634233347683, 'epoch': 116.41791044776119}
{'loss': 1302.415, 'learning_rate': 0.00026540377637001043, 'epoch': 119.40298507462687}
{'loss': 1300.585, 'learning_rate': 0.000264481210406544, 'epoch': 122.38805970149254}
{'loss': 1323.45, 'learning_rate': 0.0002635586444430776, 'epoch': 125.3731343283582}
{'loss': 1325.59, 'learning_rate': 0.0002626360784796113, 'epoch': 128.3582089552239}
{'loss': 1286.455, 'learning_rate': 0.0002617135125161449, 'epoch': 131.34328358208955}
{'loss': 1263.43, 'learning_rate': 0.0002607909465526785, 'epoch': 134.32835820895522}
{'loss': 1310.86, 'learning_rate': 0.0002598683805892121, 'epoch': 137.3134328358209}
{'loss': 1275.365, 'learning_rate': 0.0002589458146257457, 'epoch': 140.29850746268656}
{'loss': 1159.4, 'learning_rate': 0.0002580232486622793, 'epoch': 143.28358208955223}
{'loss': 1258.87, 'learning_rate': 0.00025710068269881297, 'epoch': 146.26865671641792}
{'loss': 1253.335, 'learning_rate': 0.00025617811673534657, 'epoch': 149.2537313432836}
{'loss': 1260.45, 'learning_rate': 0.00025525555077188017, 'epoch': 152.23880597014926}
{'loss': 1320.185, 'learning_rate': 0.00025433298480841376, 'epoch': 155.22388059701493}
{'loss': 1255.8, 'learning_rate': 0.0002534104188449474, 'epoch': 158.2089552238806}
{'loss': 1302.025, 'learning_rate': 0.000252487852881481, 'epoch': 161.19402985074626}
{'loss': 1304.84, 'learning_rate': 0.0002515652869180146, 'epoch': 164.17910447761193}
{'loss': 1313.025, 'learning_rate': 0.0002506427209545482, 'epoch': 167.16417910447763}
{'loss': 1256.23, 'learning_rate': 0.00024972015499108186, 'epoch': 170.1492537313433}
{'loss': 1248.155, 'learning_rate': 0.00024879758902761546, 'epoch': 173.13432835820896}
{'loss': 1234.365, 'learning_rate': 0.00024787502306414905, 'epoch': 176.11940298507463}
{'loss': 1251.965, 'learning_rate': 0.00024695245710068265, 'epoch': 179.1044776119403}
{'loss': 1278.575, 'learning_rate': 0.00024602989113721625, 'epoch': 182.08955223880596}
{'loss': 1241.87, 'learning_rate': 0.0002451073251737499, 'epoch': 185.07462686567163}
{'loss': 1267.405, 'learning_rate': 0.0002441847592102835, 'epoch': 188.0597014925373}
{'loss': 1268.44, 'learning_rate': 0.00024326219324681712, 'epoch': 191.044776119403}
{'loss': 1260.38, 'learning_rate': 0.00024233962728335072, 'epoch': 194.02985074626866}
{'loss': 1307.04, 'learning_rate': 0.00024141706131988435, 'epoch': 197.01492537313433}
{'loss': 1294.48, 'learning_rate': 0.00024049449535641794, 'epoch': 200.0}
{'loss': 1302.61, 'learning_rate': 0.00023957192939295157, 'epoch': 202.98507462686567}
{'loss': 1250.67, 'learning_rate': 0.00023864936342948517, 'epoch': 205.97014925373134}
{'loss': 1285.13, 'learning_rate': 0.0002377267974660188, 'epoch': 208.955223880597}
{'loss': 1212.09, 'learning_rate': 0.0002368042315025524, 'epoch': 211.9402985074627}
{'loss': 1241.38, 'learning_rate': 0.00023588166553908604, 'epoch': 214.92537313432837}
{'loss': 1277.26, 'learning_rate': 0.0002349590995756196, 'epoch': 217.91044776119404}
{'loss': 1265.88, 'learning_rate': 0.00023403653361215326, 'epoch': 220.8955223880597}
{'loss': 1208.34, 'learning_rate': 0.00023311396764868686, 'epoch': 223.88059701492537}
{'loss': 1276.15, 'learning_rate': 0.00023219140168522049, 'epoch': 226.86567164179104}
{'loss': 1251.7, 'learning_rate': 0.00023126883572175408, 'epoch': 229.8507462686567}
{'loss': 1351.29, 'learning_rate': 0.0002303462697582877, 'epoch': 232.83582089552237}
{'loss': 1209.28, 'learning_rate': 0.0002294237037948213, 'epoch': 235.82089552238807}
{'loss': 1270.9, 'learning_rate': 0.00022850113783135493, 'epoch': 238.80597014925374}
{'loss': 1284.71, 'learning_rate': 0.00022757857186788853, 'epoch': 241.7910447761194}
{'loss': 1306.76, 'learning_rate': 0.00022665600590442215, 'epoch': 244.77611940298507}
{'loss': 1308.72, 'learning_rate': 0.00022573343994095575, 'epoch': 247.76119402985074}
{'loss': 1246.22, 'learning_rate': 0.00022481087397748938, 'epoch': 250.7462686567164}
{'loss': 1266.07, 'learning_rate': 0.00022388830801402297, 'epoch': 253.73134328358208}
{'loss': 1308.65, 'learning_rate': 0.0002229657420505566, 'epoch': 256.7164179104478}
{'loss': 1237.11, 'learning_rate': 0.0002220431760870902, 'epoch': 259.7014925373134}
{'loss': 1230.0, 'learning_rate': 0.00022112061012362382, 'epoch': 262.6865671641791}
{'loss': 1302.89, 'learning_rate': 0.00022019804416015742, 'epoch': 265.67164179104475}
{'loss': 1273.35, 'learning_rate': 0.00021927547819669104, 'epoch': 268.65671641791045}
{'loss': 1289.65, 'learning_rate': 0.00021835291223322464, 'epoch': 271.64179104477614}
{'loss': 1321.46, 'learning_rate': 0.0002174303462697583, 'epoch': 274.6268656716418}
{'loss': 1231.32, 'learning_rate': 0.0002165077803062919, 'epoch': 277.6119402985075}
{'loss': 1272.16, 'learning_rate': 0.00021558521434282552, 'epoch': 280.5970149253731}
{'loss': 1283.67, 'learning_rate': 0.0002146626483793591, 'epoch': 283.5820895522388}
{'loss': 1308.64, 'learning_rate': 0.00021374008241589274, 'epoch': 286.56716417910445}
{'loss': 1324.68, 'learning_rate': 0.00021281751645242634, 'epoch': 289.55223880597015}
{'loss': 1243.28, 'learning_rate': 0.00021189495048895996, 'epoch': 292.53731343283584}
{'loss': 1256.64, 'learning_rate': 0.00021097238452549356, 'epoch': 295.5223880597015}
{'loss': 1272.98, 'learning_rate': 0.00021004981856202716, 'epoch': 298.5074626865672}
{'loss': 1223.51, 'learning_rate': 0.00020912725259856078, 'epoch': 301.4925373134328}
{'loss': 1203.73, 'learning_rate': 0.00020820468663509438, 'epoch': 304.4776119402985}
{'loss': 1187.62, 'learning_rate': 0.000207282120671628, 'epoch': 307.46268656716416}
{'loss': 1243.82, 'learning_rate': 0.0002063595547081616, 'epoch': 310.44776119402985}
{'loss': 1233.02, 'learning_rate': 0.00020543698874469523, 'epoch': 313.43283582089555}
{'loss': 1274.1, 'learning_rate': 0.00020451442278122882, 'epoch': 316.4179104477612}
{'loss': 1264.27, 'learning_rate': 0.00020359185681776245, 'epoch': 319.4029850746269}
{'loss': 1278.73, 'learning_rate': 0.00020266929085429605, 'epoch': 322.3880597014925}
{'loss': 1258.04, 'learning_rate': 0.00020174672489082967, 'epoch': 325.3731343283582}
{'loss': 1231.33, 'learning_rate': 0.00020082415892736327, 'epoch': 328.35820895522386}
{'loss': 1274.72, 'learning_rate': 0.00019990159296389692, 'epoch': 331.34328358208955}
{'loss': 1232.11, 'learning_rate': 0.0001989790270004305, 'epoch': 334.32835820895525}
{'loss': 1289.01, 'learning_rate': 0.00019805646103696414, 'epoch': 337.3134328358209}
{'loss': 1207.15, 'learning_rate': 0.00019713389507349774, 'epoch': 340.2985074626866}
{'loss': 1198.05, 'learning_rate': 0.00019621132911003136, 'epoch': 343.2835820895522}
{'loss': 1249.46, 'learning_rate': 0.00019528876314656496, 'epoch': 346.2686567164179}
{'loss': 1349.46, 'learning_rate': 0.0001943661971830986, 'epoch': 349.25373134328356}
{'loss': 1309.57, 'learning_rate': 0.00019344363121963218, 'epoch': 352.23880597014926}
{'loss': 1279.42, 'learning_rate': 0.0001925210652561658, 'epoch': 355.2238805970149}
{'loss': 1237.08, 'learning_rate': 0.0001915984992926994, 'epoch': 358.2089552238806}
{'loss': 1316.2, 'learning_rate': 0.00019067593332923303, 'epoch': 361.1940298507463}
{'loss': 1227.12, 'learning_rate': 0.00018975336736576663, 'epoch': 364.17910447761193}
{'loss': 1257.36, 'learning_rate': 0.00018883080140230025, 'epoch': 367.1641791044776}
{'loss': 1305.15, 'learning_rate': 0.00018790823543883385, 'epoch': 370.14925373134326}
{'loss': 1249.1, 'learning_rate': 0.00018698566947536748, 'epoch': 373.13432835820896}
{'loss': 1285.94, 'learning_rate': 0.00018606310351190107, 'epoch': 376.1194029850746}
{'loss': 1234.17, 'learning_rate': 0.0001851405375484347, 'epoch': 379.1044776119403}
{'loss': 1288.59, 'learning_rate': 0.0001842179715849683, 'epoch': 382.089552238806}
{'loss': 1215.75, 'learning_rate': 0.00018329540562150192, 'epoch': 385.07462686567163}
{'loss': 1317.76, 'learning_rate': 0.00018237283965803552, 'epoch': 388.05970149253733}
{'loss': 1258.44, 'learning_rate': 0.00018145027369456917, 'epoch': 391.04477611940297}
{'loss': 1152.3, 'learning_rate': 0.00018052770773110277, 'epoch': 394.02985074626866}
{'loss': 1229.48, 'learning_rate': 0.0001796051417676364, 'epoch': 397.0149253731343}
{'loss': 1275.86, 'learning_rate': 0.00017868257580417, 'epoch': 400.0}
{'loss': 1288.64, 'learning_rate': 0.00017776000984070362, 'epoch': 402.9850746268657}
{'loss': 1272.62, 'learning_rate': 0.0001768374438772372, 'epoch': 405.97014925373134}
{'loss': 1283.36, 'learning_rate': 0.00017591487791377084, 'epoch': 408.95522388059703}
{'loss': 1216.4, 'learning_rate': 0.00017499231195030444, 'epoch': 411.94029850746267}
{'loss': 1249.46, 'learning_rate': 0.00017406974598683803, 'epoch': 414.92537313432837}
{'loss': 1230.26, 'learning_rate': 0.00017314718002337166, 'epoch': 417.910447761194}
{'loss': 1232.9, 'learning_rate': 0.00017222461405990526, 'epoch': 420.8955223880597}
{'loss': 1297.54, 'learning_rate': 0.00017130204809643888, 'epoch': 423.8805970149254}
{'loss': 1316.46, 'learning_rate': 0.00017037948213297248, 'epoch': 426.86567164179104}
{'loss': 1262.18, 'learning_rate': 0.0001694569161695061, 'epoch': 429.85074626865674}
{'loss': 1238.28, 'learning_rate': 0.0001685343502060397, 'epoch': 432.8358208955224}
{'loss': 1222.1, 'learning_rate': 0.00016761178424257333, 'epoch': 435.82089552238807}
{'loss': 1262.16, 'learning_rate': 0.00016668921827910692, 'epoch': 438.8059701492537}
{'loss': 1287.26, 'learning_rate': 0.00016576665231564055, 'epoch': 441.7910447761194}
{'loss': 1306.64, 'learning_rate': 0.00016484408635217415, 'epoch': 444.7761194029851}
{'loss': 1302.22, 'learning_rate': 0.00016392152038870777, 'epoch': 447.76119402985074}
{'loss': 1268.16, 'learning_rate': 0.00016299895442524137, 'epoch': 450.74626865671644}
{'loss': 1284.62, 'learning_rate': 0.00016207638846177502, 'epoch': 453.7313432835821}
{'loss': 1227.32, 'learning_rate': 0.00016115382249830862, 'epoch': 456.7164179104478}
{'loss': 1278.56, 'learning_rate': 0.00016023125653484224, 'epoch': 459.7014925373134}
{'loss': 1260.78, 'learning_rate': 0.00015930869057137584, 'epoch': 462.6865671641791}
{'loss': 1274.66, 'learning_rate': 0.00015838612460790946, 'epoch': 465.67164179104475}
{'loss': 1257.98, 'learning_rate': 0.00015746355864444306, 'epoch': 468.65671641791045}
{'loss': 1269.94, 'learning_rate': 0.0001565409926809767, 'epoch': 471.64179104477614}
{'loss': 1301.24, 'learning_rate': 0.00015561842671751028, 'epoch': 474.6268656716418}
{'loss': 1303.54, 'learning_rate': 0.0001546958607540439, 'epoch': 477.6119402985075}
{'loss': 1253.54, 'learning_rate': 0.0001537732947905775, 'epoch': 480.5970149253731}
{'loss': 1254.64, 'learning_rate': 0.00015285072882711113, 'epoch': 483.5820895522388}
{'loss': 1296.12, 'learning_rate': 0.00015192816286364473, 'epoch': 486.56716417910445}
{'loss': 1261.8, 'learning_rate': 0.00015100559690017835, 'epoch': 489.55223880597015}
{'loss': 1285.96, 'learning_rate': 0.00015008303093671195, 'epoch': 492.53731343283584}
{'loss': 1258.78, 'learning_rate': 0.00014916046497324558, 'epoch': 495.5223880597015}
{'loss': 1243.74, 'learning_rate': 0.00014823789900977917, 'epoch': 498.5074626865672}
{'loss': 1225.0, 'learning_rate': 0.0001473153330463128, 'epoch': 501.4925373134328}
{'loss': 1244.86, 'learning_rate': 0.0001463927670828464, 'epoch': 504.4776119402985}
{'loss': 1287.2, 'learning_rate': 0.00014547020111938002, 'epoch': 507.46268656716416}
{'loss': 1223.32, 'learning_rate': 0.00014454763515591362, 'epoch': 510.44776119402985}
{'loss': 1214.12, 'learning_rate': 0.00014362506919244724, 'epoch': 513.4328358208955}
{'loss': 1246.2, 'learning_rate': 0.00014270250322898087, 'epoch': 516.4179104477612}
{'loss': 1200.16, 'learning_rate': 0.00014177993726551447, 'epoch': 519.4029850746268}
{'loss': 1282.98, 'learning_rate': 0.0001408573713020481, 'epoch': 522.3880597014926}
{'loss': 1227.16, 'learning_rate': 0.0001399348053385817, 'epoch': 525.3731343283582}
{'loss': 1277.4, 'learning_rate': 0.00013901223937511531, 'epoch': 528.3582089552239}
{'loss': 1222.24, 'learning_rate': 0.0001380896734116489, 'epoch': 531.3432835820895}
{'loss': 1228.12, 'learning_rate': 0.00013716710744818254, 'epoch': 534.3283582089553}
{'loss': 1316.7, 'learning_rate': 0.00013624454148471613, 'epoch': 537.3134328358209}
{'loss': 1261.16, 'learning_rate': 0.00013532197552124976, 'epoch': 540.2985074626865}
{'loss': 1276.96, 'learning_rate': 0.00013439940955778338, 'epoch': 543.2835820895523}
{'loss': 1236.32, 'learning_rate': 0.00013347684359431698, 'epoch': 546.2686567164179}
{'loss': 1240.0, 'learning_rate': 0.0001325542776308506, 'epoch': 549.2537313432836}
{'loss': 1244.54, 'learning_rate': 0.0001316317116673842, 'epoch': 552.2388059701492}
{'loss': 1269.46, 'learning_rate': 0.00013070914570391783, 'epoch': 555.223880597015}
{'loss': 1254.68, 'learning_rate': 0.00012978657974045143, 'epoch': 558.2089552238806}
{'loss': 1246.3, 'learning_rate': 0.00012886401377698505, 'epoch': 561.1940298507462}
{'loss': 1248.54, 'learning_rate': 0.00012794144781351865, 'epoch': 564.179104477612}
{'loss': 1154.7, 'learning_rate': 0.00012701888185005227, 'epoch': 567.1641791044776}
{'loss': 1215.64, 'learning_rate': 0.0001260963158865859, 'epoch': 570.1492537313433}
{'loss': 1243.04, 'learning_rate': 0.0001251737499231195, 'epoch': 573.1343283582089}
{'loss': 1244.62, 'learning_rate': 0.00012425118395965312, 'epoch': 576.1194029850747}
{'loss': 1251.2, 'learning_rate': 0.00012332861799618672, 'epoch': 579.1044776119403}
{'loss': 1241.66, 'learning_rate': 0.00012240605203272034, 'epoch': 582.0895522388059}
{'loss': 1275.4, 'learning_rate': 0.00012148348606925395, 'epoch': 585.0746268656717}
{'loss': 1247.5, 'learning_rate': 0.00012056092010578755, 'epoch': 588.0597014925373}
{'loss': 1250.76, 'learning_rate': 0.00011963835414232116, 'epoch': 591.044776119403}
{'loss': 1297.6, 'learning_rate': 0.00011871578817885477, 'epoch': 594.0298507462686}
{'loss': 1254.9, 'learning_rate': 0.00011779322221538839, 'epoch': 597.0149253731344}
{'loss': 1219.92, 'learning_rate': 0.000116870656251922, 'epoch': 600.0}
{'loss': 1205.6, 'learning_rate': 0.00011594809028845561, 'epoch': 602.9850746268656}
{'loss': 1261.04, 'learning_rate': 0.00011502552432498922, 'epoch': 605.9701492537314}
{'loss': 1223.62, 'learning_rate': 0.00011410295836152283, 'epoch': 608.955223880597}
{'loss': 1249.6, 'learning_rate': 0.00011318039239805644, 'epoch': 611.9402985074627}
{'loss': 1196.42, 'learning_rate': 0.00011225782643459005, 'epoch': 614.9253731343283}
{'loss': 1253.5, 'learning_rate': 0.00011133526047112368, 'epoch': 617.9104477611941}
{'loss': 1222.88, 'learning_rate': 0.00011041269450765729, 'epoch': 620.8955223880597}
{'loss': 1268.02, 'learning_rate': 0.0001094901285441909, 'epoch': 623.8805970149253}
{'loss': 1309.92, 'learning_rate': 0.00010856756258072451, 'epoch': 626.8656716417911}
{'loss': 1237.56, 'learning_rate': 0.00010764499661725812, 'epoch': 629.8507462686567}
{'loss': 1281.38, 'learning_rate': 0.00010672243065379173, 'epoch': 632.8358208955224}
{'loss': 1281.5, 'learning_rate': 0.00010579986469032534, 'epoch': 635.820895522388}
{'loss': 1289.96, 'learning_rate': 0.00010487729872685896, 'epoch': 638.8059701492538}
{'loss': 1203.44, 'learning_rate': 0.00010395473276339257, 'epoch': 641.7910447761194}
{'loss': 1228.86, 'learning_rate': 0.00010303216679992619, 'epoch': 644.776119402985}
{'loss': 1233.78, 'learning_rate': 0.0001021096008364598, 'epoch': 647.7611940298508}
{'loss': 1230.36, 'learning_rate': 0.00010118703487299341, 'epoch': 650.7462686567164}
{'loss': 1281.62, 'learning_rate': 0.00010026446890952703, 'epoch': 653.7313432835821}
{'loss': 1249.6, 'learning_rate': 9.934190294606064e-05, 'epoch': 656.7164179104477}
{'loss': 1258.94, 'learning_rate': 9.841933698259425e-05, 'epoch': 659.7014925373135}
{'loss': 1299.66, 'learning_rate': 9.749677101912786e-05, 'epoch': 662.6865671641791}
{'loss': 1288.94, 'learning_rate': 9.657420505566147e-05, 'epoch': 665.6716417910447}
{'loss': 1259.14, 'learning_rate': 9.565163909219508e-05, 'epoch': 668.6567164179105}
{'loss': 1217.62, 'learning_rate': 9.472907312872869e-05, 'epoch': 671.6417910447761}
{'loss': 1247.92, 'learning_rate': 9.380650716526232e-05, 'epoch': 674.6268656716418}
{'loss': 1255.88, 'learning_rate': 9.288394120179593e-05, 'epoch': 677.6119402985074}
{'loss': 1219.9, 'learning_rate': 9.196137523832954e-05, 'epoch': 680.5970149253732}
{'loss': 1234.78, 'learning_rate': 9.103880927486315e-05, 'epoch': 683.5820895522388}
{'loss': 1227.02, 'learning_rate': 9.011624331139676e-05, 'epoch': 686.5671641791045}
{'loss': 1218.52, 'learning_rate': 8.919367734793037e-05, 'epoch': 689.5522388059702}
{'loss': 1280.52, 'learning_rate': 8.827111138446399e-05, 'epoch': 692.5373134328358}
{'loss': 1203.38, 'learning_rate': 8.73485454209976e-05, 'epoch': 695.5223880597015}
{'loss': 1265.44, 'learning_rate': 8.642597945753121e-05, 'epoch': 698.5074626865671}
{'loss': 1268.1, 'learning_rate': 8.550341349406483e-05, 'epoch': 701.4925373134329}
{'loss': 1235.64, 'learning_rate': 8.458084753059842e-05, 'epoch': 704.4776119402985}
{'loss': 1257.96, 'learning_rate': 8.365828156713204e-05, 'epoch': 707.4626865671642}
{'loss': 1234.88, 'learning_rate': 8.273571560366565e-05, 'epoch': 710.4477611940298}
{'loss': 1258.8, 'learning_rate': 8.181314964019926e-05, 'epoch': 713.4328358208955}
{'loss': 1281.8, 'learning_rate': 8.089058367673287e-05, 'epoch': 716.4179104477612}
{'loss': 1287.94, 'learning_rate': 7.996801771326649e-05, 'epoch': 719.4029850746268}
{'loss': 1245.4, 'learning_rate': 7.90454517498001e-05, 'epoch': 722.3880597014926}
{'loss': 1264.76, 'learning_rate': 7.812288578633371e-05, 'epoch': 725.3731343283582}
{'loss': 1271.7, 'learning_rate': 7.720031982286732e-05, 'epoch': 728.3582089552239}
{'loss': 1258.3, 'learning_rate': 7.627775385940093e-05, 'epoch': 731.3432835820895}
{'loss': 1175.94, 'learning_rate': 7.535518789593456e-05, 'epoch': 734.3283582089553}
{'loss': 1252.26, 'learning_rate': 7.443262193246817e-05, 'epoch': 737.3134328358209}
{'loss': 1244.96, 'learning_rate': 7.351005596900178e-05, 'epoch': 740.2985074626865}
{'loss': 1229.32, 'learning_rate': 7.258749000553539e-05, 'epoch': 743.2835820895523}
{'loss': 1267.6, 'learning_rate': 7.1664924042069e-05, 'epoch': 746.2686567164179}
{'loss': 1273.2, 'learning_rate': 7.074235807860261e-05, 'epoch': 749.2537313432836}
{'loss': 1268.2, 'learning_rate': 6.981979211513622e-05, 'epoch': 752.2388059701492}
{'loss': 1265.32, 'learning_rate': 6.889722615166983e-05, 'epoch': 755.223880597015}
{'loss': 1244.46, 'learning_rate': 6.797466018820345e-05, 'epoch': 758.2089552238806}
{'loss': 1277.04, 'learning_rate': 6.705209422473706e-05, 'epoch': 761.1940298507462}
{'loss': 1219.68, 'learning_rate': 6.612952826127068e-05, 'epoch': 764.179104477612}
{'loss': 1274.08, 'learning_rate': 6.520696229780429e-05, 'epoch': 767.1641791044776}
{'loss': 1283.16, 'learning_rate': 6.42843963343379e-05, 'epoch': 770.1492537313433}
{'loss': 1241.02, 'learning_rate': 6.336183037087151e-05, 'epoch': 773.1343283582089}
{'loss': 1277.9, 'learning_rate': 6.243926440740513e-05, 'epoch': 776.1194029850747}
{'loss': 1295.94, 'learning_rate': 6.151669844393874e-05, 'epoch': 779.1044776119403}
{'loss': 1217.72, 'learning_rate': 6.059413248047235e-05, 'epoch': 782.0895522388059}
{'loss': 1211.12, 'learning_rate': 5.967156651700597e-05, 'epoch': 785.0746268656717}
{'loss': 1285.44, 'learning_rate': 5.874900055353958e-05, 'epoch': 788.0597014925373}
{'loss': 1268.04, 'learning_rate': 5.782643459007318e-05, 'epoch': 791.044776119403}
{'loss': 1189.96, 'learning_rate': 5.6903868626606793e-05, 'epoch': 794.0298507462686}
{'loss': 1279.08, 'learning_rate': 5.5981302663140405e-05, 'epoch': 797.0149253731344}
{'loss': 1235.6, 'learning_rate': 5.5058736699674016e-05, 'epoch': 800.0}
{'loss': 1252.92, 'learning_rate': 5.4136170736207634e-05, 'epoch': 802.9850746268656}
{'loss': 1270.84, 'learning_rate': 5.3213604772741245e-05, 'epoch': 805.9701492537314}
{'loss': 1253.56, 'learning_rate': 5.2291038809274856e-05, 'epoch': 808.955223880597}
{'loss': 1242.12, 'learning_rate': 5.136847284580847e-05, 'epoch': 811.9402985074627}
{'loss': 1246.48, 'learning_rate': 5.0445906882342086e-05, 'epoch': 814.9253731343283}
{'loss': 1286.0, 'learning_rate': 4.95233409188757e-05, 'epoch': 817.9104477611941}
{'loss': 1201.8, 'learning_rate': 4.860077495540931e-05, 'epoch': 820.8955223880597}
{'loss': 1207.64, 'learning_rate': 4.767820899194292e-05, 'epoch': 823.8805970149253}
{'loss': 1293.12, 'learning_rate': 4.675564302847653e-05, 'epoch': 826.8656716417911}
{'loss': 1287.2, 'learning_rate': 4.583307706501015e-05, 'epoch': 829.8507462686567}
{'loss': 1243.64, 'learning_rate': 4.491051110154376e-05, 'epoch': 832.8358208955224}
{'loss': 1206.0, 'learning_rate': 4.398794513807737e-05, 'epoch': 835.820895522388}
{'loss': 1301.52, 'learning_rate': 4.306537917461098e-05, 'epoch': 838.8059701492538}
{'loss': 1227.2, 'learning_rate': 4.214281321114459e-05, 'epoch': 841.7910447761194}
{'loss': 1246.8, 'learning_rate': 4.122024724767821e-05, 'epoch': 844.776119402985}
{'loss': 1252.4, 'learning_rate': 4.0297681284211816e-05, 'epoch': 847.7611940298508}
{'loss': 1208.36, 'learning_rate': 3.937511532074543e-05, 'epoch': 850.7462686567164}
{'loss': 1306.32, 'learning_rate': 3.845254935727904e-05, 'epoch': 853.7313432835821}
{'loss': 1190.32, 'learning_rate': 3.752998339381265e-05, 'epoch': 856.7164179104477}
{'loss': 1275.76, 'learning_rate': 3.660741743034627e-05, 'epoch': 859.7014925373135}
{'loss': 1219.36, 'learning_rate': 3.568485146687988e-05, 'epoch': 862.6865671641791}
{'loss': 1262.76, 'learning_rate': 3.476228550341349e-05, 'epoch': 865.6716417910447}
{'loss': 1251.48, 'learning_rate': 3.38397195399471e-05, 'epoch': 868.6567164179105}
{'loss': 1197.2, 'learning_rate': 3.291715357648071e-05, 'epoch': 871.6417910447761}
{'loss': 1194.24, 'learning_rate': 3.199458761301433e-05, 'epoch': 874.6268656716418}
{'loss': 1253.52, 'learning_rate': 3.107202164954794e-05, 'epoch': 877.6119402985074}
{'loss': 1273.76, 'learning_rate': 3.0149455686081553e-05, 'epoch': 880.5970149253732}
{'loss': 1261.2, 'learning_rate': 2.9226889722615164e-05, 'epoch': 883.5820895522388}
{'loss': 1193.68, 'learning_rate': 2.830432375914878e-05, 'epoch': 886.5671641791045}
{'loss': 1317.64, 'learning_rate': 2.738175779568239e-05, 'epoch': 889.5522388059702}
{'loss': 1235.96, 'learning_rate': 2.6459191832216e-05, 'epoch': 892.5373134328358}
{'loss': 1239.52, 'learning_rate': 2.5536625868749612e-05, 'epoch': 895.5223880597015}
{'loss': 1269.68, 'learning_rate': 2.4614059905283223e-05, 'epoch': 898.5074626865671}
{'loss': 1249.08, 'learning_rate': 2.3691493941816838e-05, 'epoch': 901.4925373134329}
{'loss': 1247.08, 'learning_rate': 2.276892797835045e-05, 'epoch': 904.4776119402985}
{'loss': 1240.64, 'learning_rate': 2.1846362014884064e-05, 'epoch': 907.4626865671642}
{'loss': 1266.56, 'learning_rate': 2.0923796051417675e-05, 'epoch': 910.4477611940298}
{'loss': 1249.88, 'learning_rate': 2.000123008795129e-05, 'epoch': 913.4328358208955}
{'loss': 1292.68, 'learning_rate': 1.90786641244849e-05, 'epoch': 916.4179104477612}
{'loss': 1294.4, 'learning_rate': 1.8156098161018512e-05, 'epoch': 919.4029850746268}
{'loss': 1278.28, 'learning_rate': 1.7233532197552123e-05, 'epoch': 922.3880597014926}
{'loss': 1246.92, 'learning_rate': 1.6310966234085738e-05, 'epoch': 925.3731343283582}
{'loss': 1187.68, 'learning_rate': 1.5388400270619346e-05, 'epoch': 928.3582089552239}
{'loss': 1271.24, 'learning_rate': 1.446583430715296e-05, 'epoch': 931.3432835820895}
{'loss': 1302.88, 'learning_rate': 1.3543268343686573e-05, 'epoch': 934.3283582089553}
{'loss': 1232.96, 'learning_rate': 1.2620702380220184e-05, 'epoch': 937.3134328358209}
{'loss': 1232.76, 'learning_rate': 1.1698136416753797e-05, 'epoch': 940.2985074626865}
{'loss': 1259.96, 'learning_rate': 1.0775570453287409e-05, 'epoch': 943.2835820895523}
{'loss': 1235.08, 'learning_rate': 9.853004489821021e-06, 'epoch': 946.2686567164179}
{'loss': 1248.6, 'learning_rate': 8.930438526354634e-06, 'epoch': 949.2537313432836}
{'loss': 1275.68, 'learning_rate': 8.007872562888246e-06, 'epoch': 952.2388059701492}
{'loss': 1247.04, 'learning_rate': 7.085306599421858e-06, 'epoch': 955.223880597015}
{'loss': 1249.2, 'learning_rate': 6.1627406359554705e-06, 'epoch': 958.2089552238806}
{'loss': 1269.04, 'learning_rate': 5.240174672489083e-06, 'epoch': 961.1940298507462}
{'loss': 1301.6, 'learning_rate': 4.317608709022695e-06, 'epoch': 964.179104477612}
{'loss': 1253.24, 'learning_rate': 3.395042745556307e-06, 'epoch': 967.1641791044776}
{'loss': 1234.28, 'learning_rate': 2.4724767820899192e-06, 'epoch': 970.1492537313433}
{'loss': 1223.72, 'learning_rate': 1.5499108186235313e-06, 'epoch': 973.1343283582089}
{'loss': 1235.96, 'learning_rate': 6.273448551571438e-07, 'epoch': 976.1194029850747}
06/08/2024 20:31:36 - INFO - third_party.trainers.t5_trainer -   

Training completed. Do not forget to share your model on huggingface.co/models =)


{'epoch': 978.1492537313433}
06/08/2024 20:31:42 - INFO - utils.utils -   using task specific params for movieTrivia: {'max_length': 128}
06/08/2024 20:31:49 - INFO - third_party.trainers.t5_trainer -     movieTrivia_eval_loss = 1554.94189453125
06/08/2024 20:31:49 - INFO - third_party.trainers.t5_trainer -     movieTrivia_eval_micro_f1_score = 0.6626016260162602
06/08/2024 20:31:49 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:31:49 - INFO - utils.utils -   using task specific params for movie: {'max_length': 128}
06/08/2024 20:31:58 - INFO - third_party.trainers.t5_trainer -     movie_eval_loss = 1020.2781982421875
06/08/2024 20:31:58 - INFO - third_party.trainers.t5_trainer -     movie_eval_micro_f1_score = 0.867132867132867
06/08/2024 20:31:58 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:31:58 - INFO - utils.utils -   using task specific params for restaurant: {'max_length': 128}
06/08/2024 20:32:04 - INFO - third_party.trainers.t5_trainer -     restaurant_eval_loss = 680.2574462890625
06/08/2024 20:32:04 - INFO - third_party.trainers.t5_trainer -     restaurant_eval_micro_f1_score = 0.7799999999999999
06/08/2024 20:32:04 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:32:04 - INFO - utils.utils -   using task specific params for atis: {'max_length': 128}
06/08/2024 20:32:12 - INFO - third_party.trainers.t5_trainer -     atis_eval_loss = 993.6559448242188
06/08/2024 20:32:12 - INFO - third_party.trainers.t5_trainer -     atis_eval_micro_f1_score = 0.8999999999999999
06/08/2024 20:32:12 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:32:12 - INFO - utils.utils -   using task specific params for snips: {'max_length': 128}
06/08/2024 20:32:19 - INFO - third_party.trainers.t5_trainer -     snips_eval_loss = 653.956787109375
06/08/2024 20:32:19 - INFO - third_party.trainers.t5_trainer -     snips_eval_micro_f1_score = 0.9126760563380282
06/08/2024 20:32:19 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:32:19 - INFO - utils.utils -   using task specific params for mtod: {'max_length': 128}
06/08/2024 20:32:46 - INFO - third_party.trainers.t5_trainer -     mtod_eval_loss = 668.2552490234375
06/08/2024 20:32:46 - INFO - third_party.trainers.t5_trainer -     mtod_eval_micro_f1_score = 0.9558011049723757
06/08/2024 20:32:46 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:32:46 - INFO - utils.utils -   using task specific params for mtop: {'max_length': 128}
06/08/2024 20:33:03 - INFO - third_party.trainers.t5_trainer -     mtop_eval_loss = 1060.26220703125
06/08/2024 20:33:03 - INFO - third_party.trainers.t5_trainer -     mtop_eval_micro_f1_score = 0.8207792207792208
06/08/2024 20:33:03 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:33:03 - INFO - utils.utils -   ***** val metrics *****
06/08/2024 20:33:03 - INFO - utils.utils -     atis_eval_loss = 993.6559448242188
06/08/2024 20:33:03 - INFO - utils.utils -     atis_eval_micro_f1_score = 0.8999999999999999
06/08/2024 20:33:03 - INFO - utils.utils -     eval_average_metrics = 0.8427129821769644
06/08/2024 20:33:03 - INFO - utils.utils -     movieTrivia_eval_loss = 1554.94189453125
06/08/2024 20:33:03 - INFO - utils.utils -     movieTrivia_eval_micro_f1_score = 0.6626016260162602
06/08/2024 20:33:03 - INFO - utils.utils -     movie_eval_loss = 1020.2781982421875
06/08/2024 20:33:03 - INFO - utils.utils -     movie_eval_micro_f1_score = 0.867132867132867
06/08/2024 20:33:03 - INFO - utils.utils -     mtod_eval_loss = 668.2552490234375
06/08/2024 20:33:03 - INFO - utils.utils -     mtod_eval_micro_f1_score = 0.9558011049723757
06/08/2024 20:33:03 - INFO - utils.utils -     mtop_eval_loss = 1060.26220703125
06/08/2024 20:33:03 - INFO - utils.utils -     mtop_eval_micro_f1_score = 0.8207792207792208
06/08/2024 20:33:03 - INFO - utils.utils -     restaurant_eval_loss = 680.2574462890625
06/08/2024 20:33:03 - INFO - utils.utils -     restaurant_eval_micro_f1_score = 0.7799999999999999
06/08/2024 20:33:03 - INFO - utils.utils -     snips_eval_loss = 653.956787109375
06/08/2024 20:33:03 - INFO - utils.utils -     snips_eval_micro_f1_score = 0.9126760563380282
06/08/2024 20:33:03 - INFO - utils.utils -   using task specific params for movieTrivia: {'max_length': 128}
06/08/2024 20:35:54 - INFO - third_party.trainers.t5_trainer -     movieTrivia_eval_loss = 2298.48095703125
06/08/2024 20:35:54 - INFO - third_party.trainers.t5_trainer -     movieTrivia_eval_micro_f1_score = 0.6694685228964189
06/08/2024 20:35:54 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:35:54 - INFO - utils.utils -   using task specific params for movie: {'max_length': 128}
06/08/2024 20:38:56 - INFO - third_party.trainers.t5_trainer -     movie_eval_loss = 1287.14013671875
06/08/2024 20:38:56 - INFO - third_party.trainers.t5_trainer -     movie_eval_micro_f1_score = 0.8367974648149874
06/08/2024 20:38:56 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:38:56 - INFO - utils.utils -   using task specific params for restaurant: {'max_length': 128}
06/08/2024 20:40:40 - INFO - third_party.trainers.t5_trainer -     restaurant_eval_loss = 1211.4307861328125
06/08/2024 20:40:40 - INFO - third_party.trainers.t5_trainer -     restaurant_eval_micro_f1_score = 0.7499217527386542
06/08/2024 20:40:40 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:40:40 - INFO - utils.utils -   using task specific params for atis: {'max_length': 128}
06/08/2024 20:41:59 - INFO - third_party.trainers.t5_trainer -     atis_eval_loss = 1787.3126220703125
06/08/2024 20:41:59 - INFO - third_party.trainers.t5_trainer -     atis_eval_micro_f1_score = 0.917224374779852
06/08/2024 20:41:59 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:41:59 - INFO - utils.utils -   using task specific params for snips: {'max_length': 128}
06/08/2024 20:42:50 - INFO - third_party.trainers.t5_trainer -     snips_eval_loss = 1097.50439453125
06/08/2024 20:42:50 - INFO - third_party.trainers.t5_trainer -     snips_eval_micro_f1_score = 0.8969933184855234
06/08/2024 20:42:50 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:42:50 - INFO - utils.utils -   using task specific params for mtod: {'max_length': 128}
06/08/2024 20:51:37 - INFO - third_party.trainers.t5_trainer -     mtod_eval_loss = 796.78173828125
06/08/2024 20:51:37 - INFO - third_party.trainers.t5_trainer -     mtod_eval_micro_f1_score = 0.9499796112545875
06/08/2024 20:51:37 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:51:37 - INFO - utils.utils -   using task specific params for mtop: {'max_length': 128}
06/08/2024 20:57:01 - INFO - third_party.trainers.t5_trainer -     mtop_eval_loss = 1186.227294921875
06/08/2024 20:57:01 - INFO - third_party.trainers.t5_trainer -     mtop_eval_micro_f1_score = 0.8113860069167331
06/08/2024 20:57:01 - INFO - utils.utils -   config is reset to the initial values.
06/08/2024 20:57:01 - INFO - utils.utils -   ***** test metrics *****
06/08/2024 20:57:01 - INFO - utils.utils -     atis_eval_loss = 1787.3126220703125
06/08/2024 20:57:01 - INFO - utils.utils -     atis_eval_micro_f1_score = 0.917224374779852
06/08/2024 20:57:01 - INFO - utils.utils -     eval_average_metrics = 0.8331101502695366
06/08/2024 20:57:01 - INFO - utils.utils -     movieTrivia_eval_loss = 2298.48095703125
06/08/2024 20:57:01 - INFO - utils.utils -     movieTrivia_eval_micro_f1_score = 0.6694685228964189
06/08/2024 20:57:01 - INFO - utils.utils -     movie_eval_loss = 1287.14013671875
06/08/2024 20:57:01 - INFO - utils.utils -     movie_eval_micro_f1_score = 0.8367974648149874
06/08/2024 20:57:01 - INFO - utils.utils -     mtod_eval_loss = 796.78173828125
06/08/2024 20:57:01 - INFO - utils.utils -     mtod_eval_micro_f1_score = 0.9499796112545875
06/08/2024 20:57:01 - INFO - utils.utils -     mtop_eval_loss = 1186.227294921875
06/08/2024 20:57:01 - INFO - utils.utils -     mtop_eval_micro_f1_score = 0.8113860069167331
06/08/2024 20:57:01 - INFO - utils.utils -     restaurant_eval_loss = 1211.4307861328125
06/08/2024 20:57:01 - INFO - utils.utils -     restaurant_eval_micro_f1_score = 0.7499217527386542
06/08/2024 20:57:01 - INFO - utils.utils -     snips_eval_loss = 1097.50439453125
06/08/2024 20:57:01 - INFO - utils.utils -     snips_eval_micro_f1_score = 0.8969933184855234
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
