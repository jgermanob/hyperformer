06/06/2024 17:55:37 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
06/06/2024 17:55:37 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
06/06/2024 17:55:37 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
06/06/2024 17:55:37 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
06/06/2024 17:55:37 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='outputs/hyperformer_alqvca_v2_20++/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0003, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100, max_steps=65536, warmup_steps=500, logging_dir='runs/Jun06_17-55-33_gpu-07', logging_first_step=True, logging_steps=200, save_steps=1000, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='outputs/hyperformer_alqvca_v2_20++/', disable_tqdm=True, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=True, label_smoothing=0.1, predict_with_generate=True, adafactor=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear', temperature=10, train_adapters=True, do_test=True, eval_output_dir=None, generate_classifier_weights=False, optimize_from_scratch=False, optimize_from_scratch_with_loading_model=False, split_validation_test=True, print_num_parameters=True, compute_memory=False, compute_time=False)
06/06/2024 17:55:38 - WARNING - __main__ -   model path loaded from : t5-base
06/06/2024 17:55:38 - WARNING - __main__ -   model path loaded from : t5-base
06/06/2024 17:55:38 - WARNING - __main__ -   model path loaded from : t5-base
06/06/2024 17:55:38 - WARNING - __main__ -   model path loaded from : t5-base
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
/home/jesus.ortizbarajas/.conda/envs/hyperformer/lib/python3.8/site-packages/torch-1.7.0+cu110-py3.8-linux-x86_64.egg/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.
  warnings.warn("Setting attributes on ParameterDict is not supported.")
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.movieTrivia', 'task_embedding_controller.task_to_embeddings.movie', 'task_embedding_controller.task_to_embeddings.restaurant', 'task_embedding_controller.task_to_embeddings.atis', 'task_embedding_controller.task_to_embeddings.snips', 'task_embedding_controller.task_to_embeddings.mtod', 'task_embedding_controller.task_to_embeddings.mtop', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.movieTrivia', 'task_embedding_controller.task_to_embeddings.movie', 'task_embedding_controller.task_to_embeddings.restaurant', 'task_embedding_controller.task_to_embeddings.atis', 'task_embedding_controller.task_to_embeddings.snips', 'task_embedding_controller.task_to_embeddings.mtod', 'task_embedding_controller.task_to_embeddings.mtop', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.movieTrivia', 'task_embedding_controller.task_to_embeddings.movie', 'task_embedding_controller.task_to_embeddings.restaurant', 'task_embedding_controller.task_to_embeddings.atis', 'task_embedding_controller.task_to_embeddings.snips', 'task_embedding_controller.task_to_embeddings.mtod', 'task_embedding_controller.task_to_embeddings.mtop', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/06/2024 17:55:45 - INFO - __main__ -   T5ForConditionalGeneration(
  (task_embedding_controller): TaskEmbeddingController(
    (task_to_embeddings): ParameterDict(
        (movieTrivia): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (movie): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (restaurant): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (atis): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (snips): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (mtod): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (mtop): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
    )
  )
  (shared): Embedding(32128, 768)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 768)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (9): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (10): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (11): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (adapter_layers_hyper_net): AdapterLayersOneHyperNetController(
      (layer_id_embeddings): Embedding(12, 64)
      (adapters_block_type): Embedding(6, 64)
      (task_hypernet): TaskHyperNet(
        (task_embeding_generator): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): ReLU()
          (2): Linear(in_features=128, out_features=64, bias=True)
        )
      )
      (LayerNorm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (up_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=768, bias=True)
        )
      )
      (down_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=24, bias=True)
        )
      )
      (lora_a_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (lora_b_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (post_layernorm_hypernet): LayerNormHyperNet(
        (weight_generator): Linear(in_features=64, out_features=768, bias=True)
        (bias_generator): Linear(in_features=64, out_features=768, bias=True)
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(32128, 768)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (9): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (10): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (11): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=768, out_features=768, bias=False)
              (k): Linear(in_features=768, out_features=768, bias=False)
              (v): Linear(in_features=768, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=768, bias=False)
              (meta_lora): MetaLoRALayer()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseReluDense(
              (wi): Linear(in_features=768, out_features=3072, bias=False)
              (wo): Linear(in_features=3072, out_features=768, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_hyper_net): MetaLayersAdapterController()
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (adapter_layers_hyper_net): AdapterLayersOneHyperNetController(
      (layer_id_embeddings): Embedding(12, 64)
      (adapters_block_type): Embedding(6, 64)
      (task_hypernet): TaskHyperNet(
        (task_embeding_generator): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): ReLU()
          (2): Linear(in_features=128, out_features=64, bias=True)
        )
      )
      (LayerNorm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (up_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=768, bias=True)
        )
      )
      (down_sampler_hyper_net): AdapterLayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=18432, bias=True)
        )
        (bias_generator): Sequential(
          (0): Linear(in_features=64, out_features=24, bias=True)
        )
      )
      (lora_a_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (lora_b_hyper_net): LoRALayersHyperNet(
        (weight_generator): Sequential(
          (0): Linear(in_features=64, out_features=6144, bias=False)
        )
      )
      (post_layernorm_hypernet): LayerNormHyperNet(
        (weight_generator): Linear(in_features=64, out_features=768, bias=True)
        (bias_generator): Linear(in_features=64, out_features=768, bias=True)
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=768, out_features=32128, bias=False)
)
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.movieTrivia
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.movie
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.restaurant
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.atis
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.snips
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.mtod
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name task_embedding_controller.task_to_embeddings.mtop
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.0.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.0.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.1.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.1.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.2.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.2.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.3.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.3.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.4.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.4.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.5.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.5.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.6.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.6.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.7.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.7.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.8.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.8.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.9.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.9.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.10.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.10.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.11.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.block.11.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.layer_id_embeddings.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.adapters_block_type.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.LayerNorm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.LayerNorm.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name encoder.final_layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.0.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.0.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.0.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.1.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.1.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.1.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.2.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.2.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.2.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.3.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.3.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.3.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.4.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.4.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.4.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.5.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.5.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.5.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.6.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.6.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.6.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.7.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.7.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.7.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.8.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.8.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.8.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.9.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.9.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.9.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.10.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.10.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.10.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.11.layer.0.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.11.layer.1.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.block.11.layer.2.layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.layer_id_embeddings.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.adapters_block_type.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.LayerNorm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.LayerNorm.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias
06/06/2024 17:55:45 - INFO - __main__ -   Parameter name decoder.final_layer_norm.weight
06/06/2024 17:55:45 - INFO - __main__ -   Total trainable parameters 6784368
06/06/2024 17:55:45 - INFO - __main__ -   Total parameters 229640688
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['task_embedding_controller.task_to_embeddings.movieTrivia', 'task_embedding_controller.task_to_embeddings.movie', 'task_embedding_controller.task_to_embeddings.restaurant', 'task_embedding_controller.task_to_embeddings.atis', 'task_embedding_controller.task_to_embeddings.snips', 'task_embedding_controller.task_to_embeddings.mtod', 'task_embedding_controller.task_to_embeddings.mtop', 'encoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'encoder.adapter_layers_hyper_net.adapters_block_type.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'encoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'encoder.adapter_layers_hyper_net.LayerNorm.weight', 'encoder.adapter_layers_hyper_net.LayerNorm.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'encoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'encoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'encoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias', 'decoder.adapter_layers_hyper_net.layer_id_embeddings.weight', 'decoder.adapter_layers_hyper_net.adapters_block_type.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.0.bias', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.weight', 'decoder.adapter_layers_hyper_net.task_hypernet.task_embeding_generator.2.bias', 'decoder.adapter_layers_hyper_net.LayerNorm.weight', 'decoder.adapter_layers_hyper_net.LayerNorm.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.up_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.weight_generator.0.bias', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.weight', 'decoder.adapter_layers_hyper_net.down_sampler_hyper_net.bias_generator.0.bias', 'decoder.adapter_layers_hyper_net.lora_a_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.lora_b_hyper_net.weight_generator.0.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.weight_generator.bias', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.weight', 'decoder.adapter_layers_hyper_net.post_layernorm_hypernet.bias_generator.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/1406 [00:00<?, ?ex/s]  0%|          | 0/1406 [00:00<?, ?ex/s]  0%|          | 0/1406 [00:00<?, ?ex/s]  0%|          | 0/1406 [00:00<?, ?ex/s]100%|██████████| 1406/1406 [00:00<00:00, 24335.58ex/s]
100%|██████████| 1406/1406 [00:00<00:00, 24142.10ex/s]
100%|██████████| 1406/1406 [00:00<00:00, 19439.07ex/s]
100%|██████████| 1406/1406 [00:00<00:00, 19209.03ex/s]
  0%|          | 0/1759 [00:00<?, ?ex/s]  0%|          | 0/1759 [00:00<?, ?ex/s]  0%|          | 0/1759 [00:00<?, ?ex/s]  0%|          | 0/1759 [00:00<?, ?ex/s]100%|██████████| 1759/1759 [00:00<00:00, 29044.67ex/s]
100%|██████████| 1759/1759 [00:00<00:00, 29736.29ex/s]
100%|██████████| 1759/1759 [00:00<00:00, 28893.50ex/s]
100%|██████████| 1759/1759 [00:00<00:00, 20054.53ex/s]
  0%|          | 0/1378 [00:00<?, ?ex/s]  0%|          | 0/1378 [00:00<?, ?ex/s]  0%|          | 0/1378 [00:00<?, ?ex/s]  0%|          | 0/1378 [00:00<?, ?ex/s]100%|██████████| 1378/1378 [00:00<00:00, 27950.40ex/s]
100%|██████████| 1378/1378 [00:00<00:00, 22110.33ex/s]
100%|██████████| 1378/1378 [00:00<00:00, 20754.86ex/s]
100%|██████████| 1378/1378 [00:00<00:00, 20715.51ex/s]
  0%|          | 0/895 [00:00<?, ?ex/s]  0%|          | 0/895 [00:00<?, ?ex/s]  0%|          | 0/895 [00:00<?, ?ex/s]  0%|          | 0/895 [00:00<?, ?ex/s]100%|██████████| 895/895 [00:00<00:00, 25387.02ex/s]
100%|██████████| 895/895 [00:00<00:00, 28325.14ex/s]
100%|██████████| 895/895 [00:00<00:00, 20927.91ex/s]
100%|██████████| 895/895 [00:00<00:00, 19225.74ex/s]
  0%|          | 0/2616 [00:00<?, ?ex/s]  0%|          | 0/2616 [00:00<?, ?ex/s]  0%|          | 0/2616 [00:00<?, ?ex/s]  0%|          | 0/2616 [00:00<?, ?ex/s]100%|██████████| 2616/2616 [00:00<00:00, 29397.99ex/s]
100%|██████████| 2616/2616 [00:00<00:00, 28447.53ex/s]
 96%|█████████▋| 2519/2616 [00:00<00:00, 25185.82ex/s]100%|██████████| 2616/2616 [00:00<00:00, 25358.04ex/s]
 93%|█████████▎| 2440/2616 [00:00<00:00, 24395.20ex/s]100%|██████████| 2616/2616 [00:00<00:00, 24703.82ex/s]
  0%|          | 0/6104 [00:00<?, ?ex/s]  0%|          | 0/6104 [00:00<?, ?ex/s]  0%|          | 0/6104 [00:00<?, ?ex/s]  0%|          | 0/6104 [00:00<?, ?ex/s] 48%|████▊     | 2929/6104 [00:00<00:00, 29281.44ex/s] 49%|████▊     | 2965/6104 [00:00<00:00, 29641.41ex/s] 47%|████▋     | 2894/6104 [00:00<00:00, 28934.03ex/s] 48%|████▊     | 2917/6104 [00:00<00:00, 29161.75ex/s] 97%|█████████▋| 5908/6104 [00:00<00:00, 29429.56ex/s] 96%|█████████▌| 5837/6104 [00:00<00:00, 29079.48ex/s] 96%|█████████▌| 5851/6104 [00:00<00:00, 29401.02ex/s]100%|██████████| 6104/6104 [00:00<00:00, 29495.77ex/s]
100%|██████████| 6104/6104 [00:00<00:00, 29179.36ex/s]
100%|██████████| 6104/6104 [00:00<00:00, 29172.65ex/s]
 97%|█████████▋| 5946/6104 [00:00<00:00, 29489.48ex/s]100%|██████████| 6104/6104 [00:00<00:00, 29673.85ex/s]
  0%|          | 0/3133 [00:00<?, ?ex/s]  0%|          | 0/3133 [00:00<?, ?ex/s]  0%|          | 0/3133 [00:00<?, ?ex/s]  0%|          | 0/3133 [00:00<?, ?ex/s] 93%|█████████▎| 2929/3133 [00:00<00:00, 29283.26ex/s] 94%|█████████▍| 2946/3133 [00:00<00:00, 29454.06ex/s] 95%|█████████▍| 2976/3133 [00:00<00:00, 29759.32ex/s] 85%|████████▍ | 2656/3133 [00:00<00:00, 26546.10ex/s]100%|██████████| 3133/3133 [00:00<00:00, 29675.90ex/s]
100%|██████████| 3133/3133 [00:00<00:00, 29418.85ex/s]
100%|██████████| 3133/3133 [00:00<00:00, 29257.92ex/s]
100%|██████████| 3133/3133 [00:00<00:00, 26894.54ex/s]
  0%|          | 0/156 [00:00<?, ?ex/s]  0%|          | 0/156 [00:00<?, ?ex/s]  0%|          | 0/156 [00:00<?, ?ex/s]  0%|          | 0/156 [00:00<?, ?ex/s]100%|██████████| 156/156 [00:00<00:00, 16945.37ex/s]
100%|██████████| 156/156 [00:00<00:00, 17692.22ex/s]
100%|██████████| 156/156 [00:00<00:00, 16439.16ex/s]
100%|██████████| 156/156 [00:00<00:00, 13475.12ex/s]
  0%|          | 0/195 [00:00<?, ?ex/s]  0%|          | 0/195 [00:00<?, ?ex/s]  0%|          | 0/195 [00:00<?, ?ex/s]  0%|          | 0/195 [00:00<?, ?ex/s]100%|██████████| 195/195 [00:00<00:00, 26415.05ex/s]
100%|██████████| 195/195 [00:00<00:00, 17183.63ex/s]
100%|██████████| 195/195 [00:00<00:00, 17258.69ex/s]
  0%|          | 0/153 [00:00<?, ?ex/s]100%|██████████| 195/195 [00:00<00:00, 12828.43ex/s]
  0%|          | 0/153 [00:00<?, ?ex/s]  0%|          | 0/153 [00:00<?, ?ex/s]  0%|          | 0/153 [00:00<?, ?ex/s]100%|██████████| 153/153 [00:00<00:00, 29105.97ex/s]
100%|██████████| 153/153 [00:00<00:00, 30125.27ex/s]
100%|██████████| 153/153 [00:00<00:00, 28323.63ex/s]
  0%|          | 0/100 [00:00<?, ?ex/s]100%|██████████| 153/153 [00:00<00:00, 21232.42ex/s]
  0%|          | 0/100 [00:00<?, ?ex/s]100%|██████████| 100/100 [00:00<00:00, 29390.40ex/s]
  0%|          | 0/100 [00:00<?, ?ex/s]  0%|          | 0/100 [00:00<?, ?ex/s]100%|██████████| 100/100 [00:00<00:00, 29765.84ex/s]
100%|██████████| 100/100 [00:00<00:00, 29004.25ex/s]
100%|██████████| 100/100 [00:00<00:00, 27333.36ex/s]
  0%|          | 0/140 [00:00<?, ?ex/s]  0%|          | 0/140 [00:00<?, ?ex/s]  0%|          | 0/140 [00:00<?, ?ex/s]  0%|          | 0/140 [00:00<?, ?ex/s]100%|██████████| 140/140 [00:00<00:00, 29561.14ex/s]
100%|██████████| 140/140 [00:00<00:00, 29825.40ex/s]
100%|██████████| 140/140 [00:00<00:00, 27435.53ex/s]
100%|██████████| 140/140 [00:00<00:00, 15710.68ex/s]
  0%|          | 0/836 [00:00<?, ?ex/s]  0%|          | 0/836 [00:00<?, ?ex/s]  0%|          | 0/836 [00:00<?, ?ex/s]  0%|          | 0/836 [00:00<?, ?ex/s] 68%|██████▊   | 571/836 [00:00<00:00, 5709.09ex/s] 28%|██▊       | 236/836 [00:00<00:00, 2359.32ex/s] 34%|███▍      | 285/836 [00:00<00:00, 2849.49ex/s] 80%|████████  | 669/836 [00:00<00:00, 6689.16ex/s]100%|██████████| 836/836 [00:00<00:00, 7681.42ex/s]
100%|██████████| 836/836 [00:00<00:00, 7923.31ex/s]
100%|██████████| 836/836 [00:00<00:00, 7030.48ex/s]
100%|██████████| 836/836 [00:00<00:00, 6505.10ex/s]
  0%|          | 0/447 [00:00<?, ?ex/s]  0%|          | 0/447 [00:00<?, ?ex/s]  0%|          | 0/447 [00:00<?, ?ex/s]  0%|          | 0/447 [00:00<?, ?ex/s]100%|██████████| 447/447 [00:00<00:00, 20845.61ex/s]
100%|██████████| 447/447 [00:00<00:00, 20565.05ex/s]
100%|██████████| 447/447 [00:00<00:00, 13514.80ex/s]
100%|██████████| 447/447 [00:00<00:00, 13002.66ex/s]
  0%|          | 0/1953 [00:00<?, ?ex/s]  0%|          | 0/1953 [00:00<?, ?ex/s]  0%|          | 0/1953 [00:00<?, ?ex/s]  0%|          | 0/1953 [00:00<?, ?ex/s]100%|██████████| 1953/1953 [00:00<00:00, 28577.57ex/s]
100%|██████████| 1953/1953 [00:00<00:00, 28876.78ex/s]
100%|██████████| 1953/1953 [00:00<00:00, 28770.69ex/s]
100%|██████████| 1953/1953 [00:00<00:00, 28417.36ex/s]
  0%|          | 0/2443 [00:00<?, ?ex/s]  0%|          | 0/2443 [00:00<?, ?ex/s]  0%|          | 0/2443 [00:00<?, ?ex/s]  0%|          | 0/2443 [00:00<?, ?ex/s]100%|██████████| 2443/2443 [00:00<00:00, 28277.56ex/s]
100%|██████████| 2443/2443 [00:00<00:00, 28332.84ex/s]
100%|██████████| 2443/2443 [00:00<00:00, 28082.57ex/s]
100%|██████████| 2443/2443 [00:00<00:00, 28699.06ex/s]
  0%|          | 0/1521 [00:00<?, ?ex/s]  0%|          | 0/1521 [00:00<?, ?ex/s]  0%|          | 0/1521 [00:00<?, ?ex/s]  0%|          | 0/1521 [00:00<?, ?ex/s]100%|██████████| 1521/1521 [00:00<00:00, 28844.62ex/s]
100%|██████████| 1521/1521 [00:00<00:00, 29501.29ex/s]
100%|██████████| 1521/1521 [00:00<00:00, 23020.18ex/s]
100%|██████████| 1521/1521 [00:00<00:00, 23000.76ex/s]
  0%|          | 0/893 [00:00<?, ?ex/s]  0%|          | 0/893 [00:00<?, ?ex/s]  0%|          | 0/893 [00:00<?, ?ex/s]  0%|          | 0/893 [00:00<?, ?ex/s]100%|██████████| 893/893 [00:00<00:00, 29893.08ex/s]
100%|██████████| 893/893 [00:00<00:00, 30165.13ex/s]
100%|██████████| 893/893 [00:00<00:00, 28954.86ex/s]
100%|██████████| 893/893 [00:00<00:00, 29071.95ex/s]
  0%|          | 0/700 [00:00<?, ?ex/s]  0%|          | 0/700 [00:00<?, ?ex/s]  0%|          | 0/700 [00:00<?, ?ex/s]  0%|          | 0/700 [00:00<?, ?ex/s]100%|██████████| 700/700 [00:00<00:00, 21914.47ex/s]
100%|██████████| 700/700 [00:00<00:00, 21340.71ex/s]
100%|██████████| 700/700 [00:00<00:00, 22270.02ex/s]
100%|██████████| 700/700 [00:00<00:00, 17809.02ex/s]
  0%|          | 0/8621 [00:00<?, ?ex/s]  0%|          | 0/8621 [00:00<?, ?ex/s]  0%|          | 0/8621 [00:00<?, ?ex/s]  0%|          | 0/8621 [00:00<?, ?ex/s] 28%|██▊       | 2373/8621 [00:00<00:00, 23727.31ex/s] 28%|██▊       | 2449/8621 [00:00<00:00, 24486.11ex/s] 29%|██▊       | 2458/8621 [00:00<00:00, 24574.81ex/s] 26%|██▌       | 2247/8621 [00:00<00:00, 22464.45ex/s] 61%|██████    | 5275/8621 [00:00<00:00, 25100.24ex/s] 63%|██████▎   | 5427/8621 [00:00<00:00, 25865.12ex/s] 63%|██████▎   | 5418/8621 [00:00<00:00, 25891.41ex/s] 60%|█████▉    | 5138/8621 [00:00<00:00, 24073.42ex/s] 94%|█████████▍| 8111/8621 [00:00<00:00, 25996.14ex/s] 98%|█████████▊| 8440/8621 [00:00<00:00, 27011.95ex/s] 97%|█████████▋| 8366/8621 [00:00<00:00, 26871.17ex/s] 93%|█████████▎| 8004/8621 [00:00<00:00, 25286.57ex/s]100%|██████████| 8621/8621 [00:00<00:00, 28185.46ex/s]
100%|██████████| 8621/8621 [00:00<00:00, 27972.02ex/s]
100%|██████████| 8621/8621 [00:00<00:00, 27218.07ex/s]
100%|██████████| 8621/8621 [00:00<00:00, 26939.61ex/s]
  0%|          | 0/4386 [00:00<?, ?ex/s]  0%|          | 0/4386 [00:00<?, ?ex/s]  0%|          | 0/4386 [00:00<?, ?ex/s]  0%|          | 0/4386 [00:00<?, ?ex/s] 65%|██████▌   | 2863/4386 [00:00<00:00, 28624.98ex/s] 67%|██████▋   | 2952/4386 [00:00<00:00, 29517.14ex/s] 60%|█████▉    | 2628/4386 [00:00<00:00, 26273.89ex/s] 47%|████▋     | 2064/4386 [00:00<00:00, 20633.08ex/s]100%|██████████| 4386/4386 [00:00<00:00, 29073.76ex/s]
100%|██████████| 4386/4386 [00:00<00:00, 29848.29ex/s]
100%|██████████| 4386/4386 [00:00<00:00, 27527.75ex/s]
100%|██████████| 4386/4386 [00:00<00:00, 24513.81ex/s]
06/06/2024 17:55:48 - INFO - utils.utils -   ***** arguments metrics *****
06/06/2024 17:55:48 - INFO - utils.utils -     adafactor = False
06/06/2024 17:55:48 - INFO - utils.utils -     adam_beta1 = 0.9
06/06/2024 17:55:48 - INFO - utils.utils -     adam_beta2 = 0.999
06/06/2024 17:55:48 - INFO - utils.utils -     adam_epsilon = 1e-08
06/06/2024 17:55:48 - INFO - utils.utils -     adapter_config_name = meta-adapter
06/06/2024 17:55:48 - INFO - utils.utils -     adapters = None
06/06/2024 17:55:48 - INFO - utils.utils -     add_layer_norm_after_adapter = True
06/06/2024 17:55:48 - INFO - utils.utils -     add_layer_norm_before_adapter = False
06/06/2024 17:55:48 - INFO - utils.utils -     attention_dropout = None
06/06/2024 17:55:48 - INFO - utils.utils -     cache_dir = None
06/06/2024 17:55:48 - INFO - utils.utils -     compute_memory = False
06/06/2024 17:55:48 - INFO - utils.utils -     compute_time = False
06/06/2024 17:55:48 - INFO - utils.utils -     conditional_layer_norm = True
06/06/2024 17:55:48 - INFO - utils.utils -     config_name = None
06/06/2024 17:55:48 - INFO - utils.utils -     data_seed = 42
06/06/2024 17:55:48 - INFO - utils.utils -     dataloader_drop_last = False
06/06/2024 17:55:48 - INFO - utils.utils -     dataloader_num_workers = 0
06/06/2024 17:55:48 - INFO - utils.utils -     debug = False
06/06/2024 17:55:48 - INFO - utils.utils -     decoder_layerdrop = None
06/06/2024 17:55:48 - INFO - utils.utils -     disable_tqdm = True
06/06/2024 17:55:48 - INFO - utils.utils -     do_eval = True
06/06/2024 17:55:48 - INFO - utils.utils -     do_predict = False
06/06/2024 17:55:48 - INFO - utils.utils -     do_test = True
06/06/2024 17:55:48 - INFO - utils.utils -     do_train = True
06/06/2024 17:55:48 - INFO - utils.utils -     dropout = None
06/06/2024 17:55:48 - INFO - utils.utils -     efficient_unique_hyper_net = True
06/06/2024 17:55:48 - INFO - utils.utils -     encoder_layerdrop = None
06/06/2024 17:55:48 - INFO - utils.utils -     eval_accumulation_steps = None
06/06/2024 17:55:48 - INFO - utils.utils -     eval_beams = 1
06/06/2024 17:55:48 - INFO - utils.utils -     eval_output_dir = None
06/06/2024 17:55:48 - INFO - utils.utils -     eval_steps = 1000
06/06/2024 17:55:48 - INFO - utils.utils -     eval_tasks = ['movieTrivia', 'movie', 'restaurant', 'atis', 'snips', 'mtod', 'mtop']
06/06/2024 17:55:48 - INFO - utils.utils -     evaluate_during_training = False
06/06/2024 17:55:48 - INFO - utils.utils -     fp16 = False
06/06/2024 17:55:48 - INFO - utils.utils -     fp16_opt_level = O1
06/06/2024 17:55:48 - INFO - utils.utils -     freeze_embeds = False
06/06/2024 17:55:48 - INFO - utils.utils -     freeze_encoder = False
06/06/2024 17:55:48 - INFO - utils.utils -     freeze_model = False
06/06/2024 17:55:48 - INFO - utils.utils -     freeze_model_but_lm_head = False
06/06/2024 17:55:48 - INFO - utils.utils -     freeze_model_but_task_embeddings = False
06/06/2024 17:55:48 - INFO - utils.utils -     generate_classifier_weights = False
06/06/2024 17:55:48 - INFO - utils.utils -     gradient_accumulation_steps = 1
06/06/2024 17:55:48 - INFO - utils.utils -     greater_is_better = True
06/06/2024 17:55:48 - INFO - utils.utils -     hidden_dim = 128
06/06/2024 17:55:48 - INFO - utils.utils -     ignore_pad_token_for_loss = True
06/06/2024 17:55:48 - INFO - utils.utils -     label_names = None
06/06/2024 17:55:48 - INFO - utils.utils -     label_smoothing = 0.1
06/06/2024 17:55:48 - INFO - utils.utils -     learning_rate = 0.0003
06/06/2024 17:55:48 - INFO - utils.utils -     load_best_model_at_end = True
06/06/2024 17:55:48 - INFO - utils.utils -     local_rank = 0
06/06/2024 17:55:48 - INFO - utils.utils -     logging_dir = runs/Jun06_17-55-33_gpu-07
06/06/2024 17:55:48 - INFO - utils.utils -     logging_first_step = True
06/06/2024 17:55:48 - INFO - utils.utils -     logging_steps = 200
06/06/2024 17:55:48 - INFO - utils.utils -     lr_scheduler = linear
06/06/2024 17:55:48 - INFO - utils.utils -     max_grad_norm = 1.0
06/06/2024 17:55:48 - INFO - utils.utils -     max_source_length = 128
06/06/2024 17:55:48 - INFO - utils.utils -     max_steps = 65536
06/06/2024 17:55:48 - INFO - utils.utils -     max_target_length = 128
06/06/2024 17:55:48 - INFO - utils.utils -     metric_for_best_model = loss
06/06/2024 17:55:48 - INFO - utils.utils -     model_name_or_path = t5-base
06/06/2024 17:55:48 - INFO - utils.utils -     n_test = -1
06/06/2024 17:55:48 - INFO - utils.utils -     n_train = -1
06/06/2024 17:55:48 - INFO - utils.utils -     n_val = -1
06/06/2024 17:55:48 - INFO - utils.utils -     no_cuda = False
06/06/2024 17:55:48 - INFO - utils.utils -     non_linearity = gelu_new
06/06/2024 17:55:48 - INFO - utils.utils -     not_load_t5_checkpoint = False
06/06/2024 17:55:48 - INFO - utils.utils -     num_train_epochs = 100
06/06/2024 17:55:48 - INFO - utils.utils -     optimize_from_scratch = False
06/06/2024 17:55:48 - INFO - utils.utils -     optimize_from_scratch_with_loading_model = False
06/06/2024 17:55:48 - INFO - utils.utils -     output_dir = outputs/hyperformer_alqvca_v2_20++/
06/06/2024 17:55:48 - INFO - utils.utils -     overwrite_output_dir = True
06/06/2024 17:55:48 - INFO - utils.utils -     past_index = -1
06/06/2024 17:55:48 - INFO - utils.utils -     per_device_eval_batch_size = 32
06/06/2024 17:55:48 - INFO - utils.utils -     per_device_train_batch_size = 32
06/06/2024 17:55:48 - INFO - utils.utils -     per_gpu_eval_batch_size = None
06/06/2024 17:55:48 - INFO - utils.utils -     per_gpu_train_batch_size = None
06/06/2024 17:55:48 - INFO - utils.utils -     predict_with_generate = True
06/06/2024 17:55:48 - INFO - utils.utils -     prediction_loss_only = False
06/06/2024 17:55:48 - INFO - utils.utils -     print_num_parameters = True
06/06/2024 17:55:48 - INFO - utils.utils -     projected_task_embedding_dim = 64
06/06/2024 17:55:48 - INFO - utils.utils -     reduction_factor = 32
06/06/2024 17:55:48 - INFO - utils.utils -     remove_unused_columns = False
06/06/2024 17:55:48 - INFO - utils.utils -     run_name = outputs/hyperformer_alqvca_v2_20++/
06/06/2024 17:55:48 - INFO - utils.utils -     save_steps = 1000
06/06/2024 17:55:48 - INFO - utils.utils -     save_total_limit = 1
06/06/2024 17:55:48 - INFO - utils.utils -     seed = 42
06/06/2024 17:55:48 - INFO - utils.utils -     split_validation_test = True
06/06/2024 17:55:48 - INFO - utils.utils -     task_embedding_dim = 64
06/06/2024 17:55:48 - INFO - utils.utils -     task_embeddings = None
06/06/2024 17:55:48 - INFO - utils.utils -     task_hidden_dim = 128
06/06/2024 17:55:48 - INFO - utils.utils -     tasks = ['movieTrivia', 'movie', 'restaurant', 'atis', 'snips', 'mtod', 'mtop']
06/06/2024 17:55:48 - INFO - utils.utils -     temperature = 10
06/06/2024 17:55:48 - INFO - utils.utils -     test_max_target_length = 128
06/06/2024 17:55:48 - INFO - utils.utils -     tokenizer_name = t5-base
06/06/2024 17:55:48 - INFO - utils.utils -     tpu_metrics_debug = False
06/06/2024 17:55:48 - INFO - utils.utils -     tpu_num_cores = None
06/06/2024 17:55:48 - INFO - utils.utils -     train_adapters = True
06/06/2024 17:55:48 - INFO - utils.utils -     train_adapters_blocks = True
06/06/2024 17:55:48 - INFO - utils.utils -     train_task_embeddings = False
06/06/2024 17:55:48 - INFO - utils.utils -     unfreeze_layer_norms = True
06/06/2024 17:55:48 - INFO - utils.utils -     unfreeze_lm_head = False
06/06/2024 17:55:48 - INFO - utils.utils -     unfreeze_model = False
06/06/2024 17:55:48 - INFO - utils.utils -     unique_hyper_net = False
06/06/2024 17:55:48 - INFO - utils.utils -     unique_hyper_net_layer_norm = True
06/06/2024 17:55:48 - INFO - utils.utils -     val_max_target_length = 128
06/06/2024 17:55:48 - INFO - utils.utils -     warmup_steps = 500
06/06/2024 17:55:48 - INFO - utils.utils -     weight_decay = 0.0
06/06/2024 17:55:48 - INFO - third_party.trainers.t5_trainer -   ***** Running training *****
06/06/2024 17:55:48 - INFO - third_party.trainers.t5_trainer -     Num examples = 17291
06/06/2024 17:55:48 - INFO - third_party.trainers.t5_trainer -     Num Epochs = 486
06/06/2024 17:55:48 - INFO - third_party.trainers.t5_trainer -     Instantaneous batch size per device = 32
06/06/2024 17:55:48 - INFO - third_party.trainers.t5_trainer -     Total train batch size (w. parallel, distributed & accumulation) = 128
06/06/2024 17:55:48 - INFO - third_party.trainers.t5_trainer -     Gradient Accumulation steps = 1
06/06/2024 17:55:48 - INFO - third_party.trainers.t5_trainer -     Total optimization steps = 65536
{'loss': 4579.236328125, 'learning_rate': 6e-07, 'epoch': 0.007407407407407408}
{'loss': 2932.196362672739, 'learning_rate': 0.00011999999999999999, 'epoch': 1.4814814814814814}
{'loss': 1941.0815625, 'learning_rate': 0.00023999999999999998, 'epoch': 2.962962962962963}
{'loss': 1779.049375, 'learning_rate': 0.00029953871701826677, 'epoch': 4.444444444444445}
{'loss': 1534.06375, 'learning_rate': 0.0002986161510548004, 'epoch': 5.925925925925926}
{'loss': 1474.993125, 'learning_rate': 0.000297693585091334, 'epoch': 7.407407407407407}
{'loss': 1489.189375, 'learning_rate': 0.0002967710191278676, 'epoch': 8.88888888888889}
{'loss': 1488.73125, 'learning_rate': 0.0002958484531644012, 'epoch': 10.37037037037037}
{'loss': 1427.4075, 'learning_rate': 0.00029492588720093487, 'epoch': 11.851851851851851}
{'loss': 1404.26875, 'learning_rate': 0.00029400332123746847, 'epoch': 13.333333333333334}
{'loss': 1335.785, 'learning_rate': 0.00029308075527400206, 'epoch': 14.814814814814815}
{'loss': 1421.44875, 'learning_rate': 0.00029215818931053566, 'epoch': 16.296296296296298}
{'loss': 1379.5075, 'learning_rate': 0.0002912356233470693, 'epoch': 17.77777777777778}
{'loss': 1357.1075, 'learning_rate': 0.0002903130573836029, 'epoch': 19.25925925925926}
{'loss': 1337.125, 'learning_rate': 0.0002893904914201365, 'epoch': 20.74074074074074}
{'loss': 1302.77, 'learning_rate': 0.0002884679254566701, 'epoch': 22.22222222222222}
{'loss': 1386.89, 'learning_rate': 0.00028754535949320376, 'epoch': 23.703703703703702}
{'loss': 1353.9725, 'learning_rate': 0.00028662279352973736, 'epoch': 25.185185185185187}
{'loss': 1339.2725, 'learning_rate': 0.00028570022756627095, 'epoch': 26.666666666666668}
{'loss': 1353.41, 'learning_rate': 0.00028477766160280455, 'epoch': 28.14814814814815}
{'loss': 1282.3525, 'learning_rate': 0.0002838550956393382, 'epoch': 29.62962962962963}
{'loss': 1324.95, 'learning_rate': 0.0002829325296758718, 'epoch': 31.11111111111111}
{'loss': 1348.375, 'learning_rate': 0.0002820099637124054, 'epoch': 32.592592592592595}
{'loss': 1286.58, 'learning_rate': 0.00028108739774893905, 'epoch': 34.074074074074076}
{'loss': 1287.9375, 'learning_rate': 0.0002801648317854726, 'epoch': 35.55555555555556}
{'loss': 1290.335, 'learning_rate': 0.00027924226582200625, 'epoch': 37.03703703703704}
{'loss': 1289.9575, 'learning_rate': 0.00027831969985853984, 'epoch': 38.51851851851852}
{'loss': 1315.1825, 'learning_rate': 0.0002773971338950735, 'epoch': 40.0}
{'loss': 1304.09, 'learning_rate': 0.0002764745679316071, 'epoch': 41.48148148148148}
{'loss': 1338.11, 'learning_rate': 0.0002755520019681407, 'epoch': 42.96296296296296}
{'loss': 1347.885, 'learning_rate': 0.0002746294360046743, 'epoch': 44.44444444444444}
{'loss': 1347.81, 'learning_rate': 0.00027370687004120794, 'epoch': 45.925925925925924}
{'loss': 1250.68, 'learning_rate': 0.00027278430407774154, 'epoch': 47.407407407407405}
{'loss': 1305.225, 'learning_rate': 0.00027186173811427514, 'epoch': 48.888888888888886}
{'loss': 1276.45, 'learning_rate': 0.00027093917215080873, 'epoch': 50.370370370370374}
{'loss': 1295.695, 'learning_rate': 0.0002700166061873424, 'epoch': 51.851851851851855}
{'loss': 1249.27, 'learning_rate': 0.000269094040223876, 'epoch': 53.333333333333336}
{'loss': 1267.11, 'learning_rate': 0.0002681714742604096, 'epoch': 54.81481481481482}
{'loss': 1255.49, 'learning_rate': 0.0002672489082969432, 'epoch': 56.2962962962963}
{'loss': 1282.035, 'learning_rate': 0.00026632634233347683, 'epoch': 57.77777777777778}
{'loss': 1293.765, 'learning_rate': 0.00026540377637001043, 'epoch': 59.25925925925926}
{'loss': 1281.695, 'learning_rate': 0.000264481210406544, 'epoch': 60.74074074074074}
{'loss': 1282.215, 'learning_rate': 0.0002635586444430776, 'epoch': 62.22222222222222}
{'loss': 1265.605, 'learning_rate': 0.0002626360784796113, 'epoch': 63.7037037037037}
{'loss': 1279.355, 'learning_rate': 0.0002617135125161449, 'epoch': 65.18518518518519}
{'loss': 1221.005, 'learning_rate': 0.0002607909465526785, 'epoch': 66.66666666666667}
{'loss': 1302.21, 'learning_rate': 0.0002598683805892121, 'epoch': 68.14814814814815}
{'loss': 1236.93, 'learning_rate': 0.0002589458146257457, 'epoch': 69.62962962962963}
{'loss': 1316.405, 'learning_rate': 0.0002580232486622793, 'epoch': 71.11111111111111}
{'loss': 1313.56, 'learning_rate': 0.00025710068269881297, 'epoch': 72.5925925925926}
{'loss': 1267.57, 'learning_rate': 0.00025617811673534657, 'epoch': 74.07407407407408}
{'loss': 1242.885, 'learning_rate': 0.00025525555077188017, 'epoch': 75.55555555555556}
{'loss': 1236.64, 'learning_rate': 0.00025433298480841376, 'epoch': 77.03703703703704}
{'loss': 1289.2, 'learning_rate': 0.0002534104188449474, 'epoch': 78.51851851851852}
{'loss': 1289.965, 'learning_rate': 0.000252487852881481, 'epoch': 80.0}
{'loss': 1246.865, 'learning_rate': 0.0002515652869180146, 'epoch': 81.48148148148148}
{'loss': 1278.635, 'learning_rate': 0.0002506427209545482, 'epoch': 82.96296296296296}
{'loss': 1286.48, 'learning_rate': 0.00024972015499108186, 'epoch': 84.44444444444444}
{'loss': 1249.135, 'learning_rate': 0.00024879758902761546, 'epoch': 85.92592592592592}
{'loss': 1277.67, 'learning_rate': 0.00024787502306414905, 'epoch': 87.4074074074074}
{'loss': 1247.405, 'learning_rate': 0.00024695245710068265, 'epoch': 88.88888888888889}
{'loss': 1275.47, 'learning_rate': 0.00024602989113721625, 'epoch': 90.37037037037037}
{'loss': 1233.74, 'learning_rate': 0.0002451073251737499, 'epoch': 91.85185185185185}
{'loss': 1317.5, 'learning_rate': 0.0002441847592102835, 'epoch': 93.33333333333333}
{'loss': 1323.25, 'learning_rate': 0.00024326219324681712, 'epoch': 94.81481481481481}
{'loss': 1266.08, 'learning_rate': 0.00024233962728335072, 'epoch': 96.29629629629629}
{'loss': 1281.76, 'learning_rate': 0.00024141706131988435, 'epoch': 97.77777777777777}
{'loss': 1301.91, 'learning_rate': 0.00024049449535641794, 'epoch': 99.25925925925925}
{'loss': 1220.55, 'learning_rate': 0.00023957192939295157, 'epoch': 100.74074074074075}
{'loss': 1266.75, 'learning_rate': 0.00023864936342948517, 'epoch': 102.22222222222223}
{'loss': 1287.66, 'learning_rate': 0.0002377267974660188, 'epoch': 103.70370370370371}
{'loss': 1291.04, 'learning_rate': 0.0002368042315025524, 'epoch': 105.18518518518519}
{'loss': 1286.42, 'learning_rate': 0.00023588166553908604, 'epoch': 106.66666666666667}
{'loss': 1300.76, 'learning_rate': 0.0002349590995756196, 'epoch': 108.14814814814815}
{'loss': 1303.06, 'learning_rate': 0.00023403653361215326, 'epoch': 109.62962962962963}
{'loss': 1307.9, 'learning_rate': 0.00023311396764868686, 'epoch': 111.11111111111111}
{'loss': 1289.33, 'learning_rate': 0.00023219140168522049, 'epoch': 112.5925925925926}
{'loss': 1222.85, 'learning_rate': 0.00023126883572175408, 'epoch': 114.07407407407408}
{'loss': 1294.48, 'learning_rate': 0.0002303462697582877, 'epoch': 115.55555555555556}
{'loss': 1261.16, 'learning_rate': 0.0002294237037948213, 'epoch': 117.03703703703704}
{'loss': 1253.27, 'learning_rate': 0.00022850113783135493, 'epoch': 118.51851851851852}
{'loss': 1258.5, 'learning_rate': 0.00022757857186788853, 'epoch': 120.0}
{'loss': 1278.02, 'learning_rate': 0.00022665600590442215, 'epoch': 121.48148148148148}
{'loss': 1195.49, 'learning_rate': 0.00022573343994095575, 'epoch': 122.96296296296296}
{'loss': 1255.35, 'learning_rate': 0.00022481087397748938, 'epoch': 124.44444444444444}
{'loss': 1311.73, 'learning_rate': 0.00022388830801402297, 'epoch': 125.92592592592592}
{'loss': 1326.13, 'learning_rate': 0.0002229657420505566, 'epoch': 127.4074074074074}
{'loss': 1295.94, 'learning_rate': 0.0002220431760870902, 'epoch': 128.88888888888889}
{'loss': 1249.01, 'learning_rate': 0.00022112061012362382, 'epoch': 130.37037037037038}
{'loss': 1274.72, 'learning_rate': 0.00022019804416015742, 'epoch': 131.85185185185185}
{'loss': 1207.69, 'learning_rate': 0.00021927547819669104, 'epoch': 133.33333333333334}
{'loss': 1228.69, 'learning_rate': 0.00021835291223322464, 'epoch': 134.8148148148148}
{'loss': 1259.75, 'learning_rate': 0.0002174303462697583, 'epoch': 136.2962962962963}
{'loss': 1357.57, 'learning_rate': 0.0002165077803062919, 'epoch': 137.77777777777777}
{'loss': 1294.09, 'learning_rate': 0.00021558521434282552, 'epoch': 139.25925925925927}
{'loss': 1205.83, 'learning_rate': 0.0002146626483793591, 'epoch': 140.74074074074073}
{'loss': 1299.59, 'learning_rate': 0.00021374008241589274, 'epoch': 142.22222222222223}
{'loss': 1354.97, 'learning_rate': 0.00021281751645242634, 'epoch': 143.7037037037037}
{'loss': 1324.82, 'learning_rate': 0.00021189495048895996, 'epoch': 145.1851851851852}
{'loss': 1262.81, 'learning_rate': 0.00021097238452549356, 'epoch': 146.66666666666666}
{'loss': 1344.15, 'learning_rate': 0.00021004981856202716, 'epoch': 148.14814814814815}
{'loss': 1256.61, 'learning_rate': 0.00020912725259856078, 'epoch': 149.62962962962962}
{'loss': 1224.9, 'learning_rate': 0.00020820468663509438, 'epoch': 151.11111111111111}
{'loss': 1268.63, 'learning_rate': 0.000207282120671628, 'epoch': 152.59259259259258}
{'loss': 1334.55, 'learning_rate': 0.0002063595547081616, 'epoch': 154.07407407407408}
{'loss': 1280.64, 'learning_rate': 0.00020543698874469523, 'epoch': 155.55555555555554}
{'loss': 1252.03, 'learning_rate': 0.00020451442278122882, 'epoch': 157.03703703703704}
{'loss': 1252.23, 'learning_rate': 0.00020359185681776245, 'epoch': 158.5185185185185}
{'loss': 1298.1, 'learning_rate': 0.00020266929085429605, 'epoch': 160.0}
{'loss': 1273.17, 'learning_rate': 0.00020174672489082967, 'epoch': 161.4814814814815}
{'loss': 1219.35, 'learning_rate': 0.00020082415892736327, 'epoch': 162.96296296296296}
{'loss': 1276.34, 'learning_rate': 0.00019990159296389692, 'epoch': 164.44444444444446}
{'loss': 1310.57, 'learning_rate': 0.0001989790270004305, 'epoch': 165.92592592592592}
{'loss': 1302.67, 'learning_rate': 0.00019805646103696414, 'epoch': 167.40740740740742}
{'loss': 1220.99, 'learning_rate': 0.00019713389507349774, 'epoch': 168.88888888888889}
{'loss': 1244.28, 'learning_rate': 0.00019621132911003136, 'epoch': 170.37037037037038}
{'loss': 1285.43, 'learning_rate': 0.00019528876314656496, 'epoch': 171.85185185185185}
{'loss': 1228.89, 'learning_rate': 0.0001943661971830986, 'epoch': 173.33333333333334}
{'loss': 1205.43, 'learning_rate': 0.00019344363121963218, 'epoch': 174.8148148148148}
{'loss': 1261.47, 'learning_rate': 0.0001925210652561658, 'epoch': 176.2962962962963}
{'loss': 1279.21, 'learning_rate': 0.0001915984992926994, 'epoch': 177.77777777777777}
{'loss': 1271.61, 'learning_rate': 0.00019067593332923303, 'epoch': 179.25925925925927}
{'loss': 1281.08, 'learning_rate': 0.00018975336736576663, 'epoch': 180.74074074074073}
{'loss': 1312.65, 'learning_rate': 0.00018883080140230025, 'epoch': 182.22222222222223}
{'loss': 1282.74, 'learning_rate': 0.00018790823543883385, 'epoch': 183.7037037037037}
{'loss': 1195.09, 'learning_rate': 0.00018698566947536748, 'epoch': 185.1851851851852}
{'loss': 1288.75, 'learning_rate': 0.00018606310351190107, 'epoch': 186.66666666666666}
{'loss': 1287.32, 'learning_rate': 0.0001851405375484347, 'epoch': 188.14814814814815}
{'loss': 1309.18, 'learning_rate': 0.0001842179715849683, 'epoch': 189.62962962962962}
{'loss': 1283.92, 'learning_rate': 0.00018329540562150192, 'epoch': 191.11111111111111}
{'loss': 1312.26, 'learning_rate': 0.00018237283965803552, 'epoch': 192.59259259259258}
{'loss': 1232.5, 'learning_rate': 0.00018145027369456917, 'epoch': 194.07407407407408}
{'loss': 1265.14, 'learning_rate': 0.00018052770773110277, 'epoch': 195.55555555555554}
{'loss': 1217.94, 'learning_rate': 0.0001796051417676364, 'epoch': 197.03703703703704}
{'loss': 1249.5, 'learning_rate': 0.00017868257580417, 'epoch': 198.5185185185185}
{'loss': 1332.24, 'learning_rate': 0.00017776000984070362, 'epoch': 200.0}
{'loss': 1272.22, 'learning_rate': 0.0001768374438772372, 'epoch': 201.4814814814815}
{'loss': 1260.92, 'learning_rate': 0.00017591487791377084, 'epoch': 202.96296296296296}
{'loss': 1251.32, 'learning_rate': 0.00017499231195030444, 'epoch': 204.44444444444446}
{'loss': 1222.98, 'learning_rate': 0.00017406974598683803, 'epoch': 205.92592592592592}
{'loss': 1285.94, 'learning_rate': 0.00017314718002337166, 'epoch': 207.40740740740742}
{'loss': 1251.98, 'learning_rate': 0.00017222461405990526, 'epoch': 208.88888888888889}
{'loss': 1223.5, 'learning_rate': 0.00017130204809643888, 'epoch': 210.37037037037038}
{'loss': 1243.28, 'learning_rate': 0.00017037948213297248, 'epoch': 211.85185185185185}
{'loss': 1250.84, 'learning_rate': 0.0001694569161695061, 'epoch': 213.33333333333334}
{'loss': 1276.58, 'learning_rate': 0.0001685343502060397, 'epoch': 214.8148148148148}
{'loss': 1254.42, 'learning_rate': 0.00016761178424257333, 'epoch': 216.2962962962963}
{'loss': 1257.32, 'learning_rate': 0.00016668921827910692, 'epoch': 217.77777777777777}
{'loss': 1239.42, 'learning_rate': 0.00016576665231564055, 'epoch': 219.25925925925927}
{'loss': 1244.64, 'learning_rate': 0.00016484408635217415, 'epoch': 220.74074074074073}
{'loss': 1245.92, 'learning_rate': 0.00016392152038870777, 'epoch': 222.22222222222223}
{'loss': 1228.8, 'learning_rate': 0.00016299895442524137, 'epoch': 223.7037037037037}
{'loss': 1306.66, 'learning_rate': 0.00016207638846177502, 'epoch': 225.1851851851852}
{'loss': 1223.38, 'learning_rate': 0.00016115382249830862, 'epoch': 226.66666666666666}
{'loss': 1270.34, 'learning_rate': 0.00016023125653484224, 'epoch': 228.14814814814815}
{'loss': 1240.32, 'learning_rate': 0.00015930869057137584, 'epoch': 229.62962962962962}
{'loss': 1351.18, 'learning_rate': 0.00015838612460790946, 'epoch': 231.11111111111111}
{'loss': 1178.9, 'learning_rate': 0.00015746355864444306, 'epoch': 232.59259259259258}
{'loss': 1257.6, 'learning_rate': 0.0001565409926809767, 'epoch': 234.07407407407408}
{'loss': 1262.44, 'learning_rate': 0.00015561842671751028, 'epoch': 235.55555555555554}
{'loss': 1298.54, 'learning_rate': 0.0001546958607540439, 'epoch': 237.03703703703704}
{'loss': 1309.92, 'learning_rate': 0.0001537732947905775, 'epoch': 238.5185185185185}
{'loss': 1234.52, 'learning_rate': 0.00015285072882711113, 'epoch': 240.0}
{'loss': 1241.9, 'learning_rate': 0.00015192816286364473, 'epoch': 241.4814814814815}
{'loss': 1290.42, 'learning_rate': 0.00015100559690017835, 'epoch': 242.96296296296296}
{'loss': 1325.6, 'learning_rate': 0.00015008303093671195, 'epoch': 244.44444444444446}
{'loss': 1307.68, 'learning_rate': 0.00014916046497324558, 'epoch': 245.92592592592592}
{'loss': 1256.04, 'learning_rate': 0.00014823789900977917, 'epoch': 247.40740740740742}
{'loss': 1218.84, 'learning_rate': 0.0001473153330463128, 'epoch': 248.88888888888889}
{'loss': 1188.34, 'learning_rate': 0.0001463927670828464, 'epoch': 250.37037037037038}
{'loss': 1333.22, 'learning_rate': 0.00014547020111938002, 'epoch': 251.85185185185185}
{'loss': 1265.84, 'learning_rate': 0.00014454763515591362, 'epoch': 253.33333333333334}
{'loss': 1278.98, 'learning_rate': 0.00014362506919244724, 'epoch': 254.8148148148148}
{'loss': 1250.16, 'learning_rate': 0.00014270250322898087, 'epoch': 256.2962962962963}
{'loss': 1254.2, 'learning_rate': 0.00014177993726551447, 'epoch': 257.77777777777777}
{'loss': 1286.56, 'learning_rate': 0.0001408573713020481, 'epoch': 259.25925925925924}
{'loss': 1246.68, 'learning_rate': 0.0001399348053385817, 'epoch': 260.74074074074076}
{'loss': 1278.56, 'learning_rate': 0.00013901223937511531, 'epoch': 262.22222222222223}
{'loss': 1263.02, 'learning_rate': 0.0001380896734116489, 'epoch': 263.7037037037037}
{'loss': 1293.32, 'learning_rate': 0.00013716710744818254, 'epoch': 265.18518518518516}
{'loss': 1257.62, 'learning_rate': 0.00013624454148471613, 'epoch': 266.6666666666667}
{'loss': 1252.88, 'learning_rate': 0.00013532197552124976, 'epoch': 268.14814814814815}
{'loss': 1258.32, 'learning_rate': 0.00013439940955778338, 'epoch': 269.6296296296296}
{'loss': 1254.6, 'learning_rate': 0.00013347684359431698, 'epoch': 271.1111111111111}
{'loss': 1322.4, 'learning_rate': 0.0001325542776308506, 'epoch': 272.5925925925926}
{'loss': 1234.76, 'learning_rate': 0.0001316317116673842, 'epoch': 274.0740740740741}
{'loss': 1195.34, 'learning_rate': 0.00013070914570391783, 'epoch': 275.55555555555554}
{'loss': 1264.54, 'learning_rate': 0.00012978657974045143, 'epoch': 277.037037037037}
{'loss': 1259.78, 'learning_rate': 0.00012886401377698505, 'epoch': 278.51851851851853}
{'loss': 1272.08, 'learning_rate': 0.00012794144781351865, 'epoch': 280.0}
{'loss': 1306.4, 'learning_rate': 0.00012701888185005227, 'epoch': 281.48148148148147}
{'loss': 1318.46, 'learning_rate': 0.0001260963158865859, 'epoch': 282.962962962963}
{'loss': 1314.16, 'learning_rate': 0.0001251737499231195, 'epoch': 284.44444444444446}
{'loss': 1240.46, 'learning_rate': 0.00012425118395965312, 'epoch': 285.9259259259259}
{'loss': 1196.6, 'learning_rate': 0.00012332861799618672, 'epoch': 287.4074074074074}
{'loss': 1286.76, 'learning_rate': 0.00012240605203272034, 'epoch': 288.8888888888889}
{'loss': 1235.12, 'learning_rate': 0.00012148348606925395, 'epoch': 290.3703703703704}
{'loss': 1323.68, 'learning_rate': 0.00012056092010578755, 'epoch': 291.85185185185185}
{'loss': 1285.72, 'learning_rate': 0.00011963835414232116, 'epoch': 293.3333333333333}
{'loss': 1282.18, 'learning_rate': 0.00011871578817885477, 'epoch': 294.81481481481484}
{'loss': 1227.42, 'learning_rate': 0.00011779322221538839, 'epoch': 296.2962962962963}
{'loss': 1214.52, 'learning_rate': 0.000116870656251922, 'epoch': 297.77777777777777}
{'loss': 1265.46, 'learning_rate': 0.00011594809028845561, 'epoch': 299.25925925925924}
{'loss': 1283.58, 'learning_rate': 0.00011502552432498922, 'epoch': 300.74074074074076}
{'loss': 1267.44, 'learning_rate': 0.00011410295836152283, 'epoch': 302.22222222222223}
{'loss': 1263.62, 'learning_rate': 0.00011318039239805644, 'epoch': 303.7037037037037}
{'loss': 1249.64, 'learning_rate': 0.00011225782643459005, 'epoch': 305.18518518518516}
{'loss': 1254.24, 'learning_rate': 0.00011133526047112368, 'epoch': 306.6666666666667}
{'loss': 1270.94, 'learning_rate': 0.00011041269450765729, 'epoch': 308.14814814814815}
{'loss': 1246.92, 'learning_rate': 0.0001094901285441909, 'epoch': 309.6296296296296}
{'loss': 1280.48, 'learning_rate': 0.00010856756258072451, 'epoch': 311.1111111111111}
{'loss': 1276.94, 'learning_rate': 0.00010764499661725812, 'epoch': 312.5925925925926}
{'loss': 1277.98, 'learning_rate': 0.00010672243065379173, 'epoch': 314.0740740740741}
{'loss': 1197.64, 'learning_rate': 0.00010579986469032534, 'epoch': 315.55555555555554}
{'loss': 1265.14, 'learning_rate': 0.00010487729872685896, 'epoch': 317.037037037037}
{'loss': 1261.18, 'learning_rate': 0.00010395473276339257, 'epoch': 318.51851851851853}
{'loss': 1263.28, 'learning_rate': 0.00010303216679992619, 'epoch': 320.0}
{'loss': 1259.42, 'learning_rate': 0.0001021096008364598, 'epoch': 321.48148148148147}
{'loss': 1224.3, 'learning_rate': 0.00010118703487299341, 'epoch': 322.962962962963}
{'loss': 1285.74, 'learning_rate': 0.00010026446890952703, 'epoch': 324.44444444444446}
{'loss': 1254.46, 'learning_rate': 9.934190294606064e-05, 'epoch': 325.9259259259259}
{'loss': 1300.12, 'learning_rate': 9.841933698259425e-05, 'epoch': 327.4074074074074}
{'loss': 1296.0, 'learning_rate': 9.749677101912786e-05, 'epoch': 328.8888888888889}
{'loss': 1253.68, 'learning_rate': 9.657420505566147e-05, 'epoch': 330.3703703703704}
{'loss': 1282.28, 'learning_rate': 9.565163909219508e-05, 'epoch': 331.85185185185185}
{'loss': 1259.66, 'learning_rate': 9.472907312872869e-05, 'epoch': 333.3333333333333}
{'loss': 1280.76, 'learning_rate': 9.380650716526232e-05, 'epoch': 334.81481481481484}
{'loss': 1288.68, 'learning_rate': 9.288394120179593e-05, 'epoch': 336.2962962962963}
{'loss': 1230.32, 'learning_rate': 9.196137523832954e-05, 'epoch': 337.77777777777777}
{'loss': 1257.1, 'learning_rate': 9.103880927486315e-05, 'epoch': 339.25925925925924}
{'loss': 1212.18, 'learning_rate': 9.011624331139676e-05, 'epoch': 340.74074074074076}
{'loss': 1281.0, 'learning_rate': 8.919367734793037e-05, 'epoch': 342.22222222222223}
{'loss': 1312.46, 'learning_rate': 8.827111138446399e-05, 'epoch': 343.7037037037037}
{'loss': 1269.34, 'learning_rate': 8.73485454209976e-05, 'epoch': 345.18518518518516}
{'loss': 1216.06, 'learning_rate': 8.642597945753121e-05, 'epoch': 346.6666666666667}
{'loss': 1182.1, 'learning_rate': 8.550341349406483e-05, 'epoch': 348.14814814814815}
{'loss': 1258.44, 'learning_rate': 8.458084753059842e-05, 'epoch': 349.6296296296296}
{'loss': 1232.52, 'learning_rate': 8.365828156713204e-05, 'epoch': 351.1111111111111}
{'loss': 1267.58, 'learning_rate': 8.273571560366565e-05, 'epoch': 352.5925925925926}
{'loss': 1232.72, 'learning_rate': 8.181314964019926e-05, 'epoch': 354.0740740740741}
{'loss': 1226.96, 'learning_rate': 8.089058367673287e-05, 'epoch': 355.55555555555554}
{'loss': 1275.96, 'learning_rate': 7.996801771326649e-05, 'epoch': 357.037037037037}
{'loss': 1241.06, 'learning_rate': 7.90454517498001e-05, 'epoch': 358.51851851851853}
{'loss': 1275.04, 'learning_rate': 7.812288578633371e-05, 'epoch': 360.0}
{'loss': 1266.84, 'learning_rate': 7.720031982286732e-05, 'epoch': 361.48148148148147}
{'loss': 1238.8, 'learning_rate': 7.627775385940093e-05, 'epoch': 362.962962962963}
{'loss': 1265.76, 'learning_rate': 7.535518789593456e-05, 'epoch': 364.44444444444446}
{'loss': 1265.4, 'learning_rate': 7.443262193246817e-05, 'epoch': 365.9259259259259}
{'loss': 1358.62, 'learning_rate': 7.351005596900178e-05, 'epoch': 367.4074074074074}
{'loss': 1265.88, 'learning_rate': 7.258749000553539e-05, 'epoch': 368.8888888888889}
{'loss': 1297.4, 'learning_rate': 7.1664924042069e-05, 'epoch': 370.3703703703704}
{'loss': 1257.44, 'learning_rate': 7.074235807860261e-05, 'epoch': 371.85185185185185}
{'loss': 1259.42, 'learning_rate': 6.981979211513622e-05, 'epoch': 373.3333333333333}
{'loss': 1220.7, 'learning_rate': 6.889722615166983e-05, 'epoch': 374.81481481481484}
{'loss': 1279.18, 'learning_rate': 6.797466018820345e-05, 'epoch': 376.2962962962963}
{'loss': 1196.88, 'learning_rate': 6.705209422473706e-05, 'epoch': 377.77777777777777}
{'loss': 1257.24, 'learning_rate': 6.612952826127068e-05, 'epoch': 379.25925925925924}
{'loss': 1293.36, 'learning_rate': 6.520696229780429e-05, 'epoch': 380.74074074074076}
{'loss': 1236.26, 'learning_rate': 6.42843963343379e-05, 'epoch': 382.22222222222223}
{'loss': 1233.14, 'learning_rate': 6.336183037087151e-05, 'epoch': 383.7037037037037}
{'loss': 1261.82, 'learning_rate': 6.243926440740513e-05, 'epoch': 385.18518518518516}
{'loss': 1273.62, 'learning_rate': 6.151669844393874e-05, 'epoch': 386.6666666666667}
{'loss': 1328.2, 'learning_rate': 6.059413248047235e-05, 'epoch': 388.14814814814815}
{'loss': 1321.64, 'learning_rate': 5.967156651700597e-05, 'epoch': 389.6296296296296}
{'loss': 1255.88, 'learning_rate': 5.874900055353958e-05, 'epoch': 391.1111111111111}
{'loss': 1247.24, 'learning_rate': 5.782643459007318e-05, 'epoch': 392.5925925925926}
{'loss': 1262.16, 'learning_rate': 5.6903868626606793e-05, 'epoch': 394.0740740740741}
{'loss': 1261.08, 'learning_rate': 5.5981302663140405e-05, 'epoch': 395.55555555555554}
{'loss': 1250.52, 'learning_rate': 5.5058736699674016e-05, 'epoch': 397.037037037037}
{'loss': 1313.0, 'learning_rate': 5.4136170736207634e-05, 'epoch': 398.51851851851853}
{'loss': 1306.36, 'learning_rate': 5.3213604772741245e-05, 'epoch': 400.0}
{'loss': 1231.44, 'learning_rate': 5.2291038809274856e-05, 'epoch': 401.48148148148147}
{'loss': 1269.2, 'learning_rate': 5.136847284580847e-05, 'epoch': 402.962962962963}
{'loss': 1206.76, 'learning_rate': 5.0445906882342086e-05, 'epoch': 404.44444444444446}
{'loss': 1268.56, 'learning_rate': 4.95233409188757e-05, 'epoch': 405.9259259259259}
{'loss': 1241.4, 'learning_rate': 4.860077495540931e-05, 'epoch': 407.4074074074074}
{'loss': 1286.04, 'learning_rate': 4.767820899194292e-05, 'epoch': 408.8888888888889}
{'loss': 1244.92, 'learning_rate': 4.675564302847653e-05, 'epoch': 410.3703703703704}
{'loss': 1222.8, 'learning_rate': 4.583307706501015e-05, 'epoch': 411.85185185185185}
{'loss': 1213.64, 'learning_rate': 4.491051110154376e-05, 'epoch': 413.3333333333333}
{'loss': 1257.24, 'learning_rate': 4.398794513807737e-05, 'epoch': 414.81481481481484}
{'loss': 1263.68, 'learning_rate': 4.306537917461098e-05, 'epoch': 416.2962962962963}
{'loss': 1218.04, 'learning_rate': 4.214281321114459e-05, 'epoch': 417.77777777777777}
{'loss': 1300.76, 'learning_rate': 4.122024724767821e-05, 'epoch': 419.25925925925924}
{'loss': 1333.64, 'learning_rate': 4.0297681284211816e-05, 'epoch': 420.74074074074076}
{'loss': 1338.08, 'learning_rate': 3.937511532074543e-05, 'epoch': 422.22222222222223}
{'loss': 1261.44, 'learning_rate': 3.845254935727904e-05, 'epoch': 423.7037037037037}
{'loss': 1245.76, 'learning_rate': 3.752998339381265e-05, 'epoch': 425.18518518518516}
{'loss': 1266.16, 'learning_rate': 3.660741743034627e-05, 'epoch': 426.6666666666667}
{'loss': 1219.72, 'learning_rate': 3.568485146687988e-05, 'epoch': 428.14814814814815}
{'loss': 1241.2, 'learning_rate': 3.476228550341349e-05, 'epoch': 429.6296296296296}
{'loss': 1223.84, 'learning_rate': 3.38397195399471e-05, 'epoch': 431.1111111111111}
{'loss': 1318.88, 'learning_rate': 3.291715357648071e-05, 'epoch': 432.5925925925926}
{'loss': 1271.64, 'learning_rate': 3.199458761301433e-05, 'epoch': 434.0740740740741}
{'loss': 1285.04, 'learning_rate': 3.107202164954794e-05, 'epoch': 435.55555555555554}
{'loss': 1264.96, 'learning_rate': 3.0149455686081553e-05, 'epoch': 437.037037037037}
{'loss': 1262.12, 'learning_rate': 2.9226889722615164e-05, 'epoch': 438.51851851851853}
{'loss': 1300.32, 'learning_rate': 2.830432375914878e-05, 'epoch': 440.0}
{'loss': 1244.88, 'learning_rate': 2.738175779568239e-05, 'epoch': 441.48148148148147}
{'loss': 1273.64, 'learning_rate': 2.6459191832216e-05, 'epoch': 442.962962962963}
{'loss': 1237.84, 'learning_rate': 2.5536625868749612e-05, 'epoch': 444.44444444444446}
{'loss': 1263.28, 'learning_rate': 2.4614059905283223e-05, 'epoch': 445.9259259259259}
{'loss': 1243.76, 'learning_rate': 2.3691493941816838e-05, 'epoch': 447.4074074074074}
{'loss': 1283.2, 'learning_rate': 2.276892797835045e-05, 'epoch': 448.8888888888889}
{'loss': 1258.32, 'learning_rate': 2.1846362014884064e-05, 'epoch': 450.3703703703704}
{'loss': 1222.36, 'learning_rate': 2.0923796051417675e-05, 'epoch': 451.85185185185185}
{'loss': 1269.28, 'learning_rate': 2.000123008795129e-05, 'epoch': 453.3333333333333}
{'loss': 1211.92, 'learning_rate': 1.90786641244849e-05, 'epoch': 454.81481481481484}
{'loss': 1272.24, 'learning_rate': 1.8156098161018512e-05, 'epoch': 456.2962962962963}
{'loss': 1230.16, 'learning_rate': 1.7233532197552123e-05, 'epoch': 457.77777777777777}
{'loss': 1216.16, 'learning_rate': 1.6310966234085738e-05, 'epoch': 459.25925925925924}
{'loss': 1277.56, 'learning_rate': 1.5388400270619346e-05, 'epoch': 460.74074074074076}
{'loss': 1355.64, 'learning_rate': 1.446583430715296e-05, 'epoch': 462.22222222222223}
{'loss': 1286.64, 'learning_rate': 1.3543268343686573e-05, 'epoch': 463.7037037037037}
{'loss': 1283.96, 'learning_rate': 1.2620702380220184e-05, 'epoch': 465.18518518518516}
{'loss': 1277.72, 'learning_rate': 1.1698136416753797e-05, 'epoch': 466.6666666666667}
{'loss': 1255.68, 'learning_rate': 1.0775570453287409e-05, 'epoch': 468.14814814814815}
{'loss': 1276.88, 'learning_rate': 9.853004489821021e-06, 'epoch': 469.6296296296296}
{'loss': 1263.36, 'learning_rate': 8.930438526354634e-06, 'epoch': 471.1111111111111}
{'loss': 1283.56, 'learning_rate': 8.007872562888246e-06, 'epoch': 472.5925925925926}
{'loss': 1225.52, 'learning_rate': 7.085306599421858e-06, 'epoch': 474.0740740740741}
{'loss': 1252.88, 'learning_rate': 6.1627406359554705e-06, 'epoch': 475.55555555555554}
{'loss': 1199.48, 'learning_rate': 5.240174672489083e-06, 'epoch': 477.037037037037}
{'loss': 1295.2, 'learning_rate': 4.317608709022695e-06, 'epoch': 478.51851851851853}
{'loss': 1279.24, 'learning_rate': 3.395042745556307e-06, 'epoch': 480.0}
{'loss': 1282.76, 'learning_rate': 2.4724767820899192e-06, 'epoch': 481.48148148148147}
{'loss': 1202.36, 'learning_rate': 1.5499108186235313e-06, 'epoch': 482.962962962963}
{'loss': 1289.64, 'learning_rate': 6.273448551571438e-07, 'epoch': 484.44444444444446}
06/07/2024 01:49:18 - INFO - third_party.trainers.t5_trainer -   

Training completed. Do not forget to share your model on huggingface.co/models =)


{'epoch': 485.45185185185187}
06/07/2024 01:49:24 - INFO - utils.utils -   using task specific params for movieTrivia: {'max_length': 128}
06/07/2024 01:49:40 - INFO - third_party.trainers.t5_trainer -     movieTrivia_eval_loss = 1609.187255859375
06/07/2024 01:49:40 - INFO - third_party.trainers.t5_trainer -     movieTrivia_eval_micro_f1_score = 0.6666666666666666
06/07/2024 01:49:40 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:49:40 - INFO - utils.utils -   using task specific params for movie: {'max_length': 128}
06/07/2024 01:49:57 - INFO - third_party.trainers.t5_trainer -     movie_eval_loss = 961.4859008789062
06/07/2024 01:49:57 - INFO - third_party.trainers.t5_trainer -     movie_eval_micro_f1_score = 0.8621495327102804
06/07/2024 01:49:57 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:49:57 - INFO - utils.utils -   using task specific params for restaurant: {'max_length': 128}
06/07/2024 01:50:09 - INFO - third_party.trainers.t5_trainer -     restaurant_eval_loss = 757.5418090820312
06/07/2024 01:50:09 - INFO - third_party.trainers.t5_trainer -     restaurant_eval_micro_f1_score = 0.7804878048780487
06/07/2024 01:50:09 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:50:09 - INFO - utils.utils -   using task specific params for atis: {'max_length': 128}
06/07/2024 01:50:20 - INFO - third_party.trainers.t5_trainer -     atis_eval_loss = 1578.7830810546875
06/07/2024 01:50:20 - INFO - third_party.trainers.t5_trainer -     atis_eval_micro_f1_score = 0.964336661911555
06/07/2024 01:50:20 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:50:20 - INFO - utils.utils -   using task specific params for snips: {'max_length': 128}
06/07/2024 01:50:32 - INFO - third_party.trainers.t5_trainer -     snips_eval_loss = 670.5992431640625
06/07/2024 01:50:32 - INFO - third_party.trainers.t5_trainer -     snips_eval_micro_f1_score = 0.9053857350800583
06/07/2024 01:50:32 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:50:32 - INFO - utils.utils -   using task specific params for mtod: {'max_length': 128}
06/07/2024 01:51:26 - INFO - third_party.trainers.t5_trainer -     mtod_eval_loss = 760.48681640625
06/07/2024 01:51:26 - INFO - third_party.trainers.t5_trainer -     mtod_eval_micro_f1_score = 0.9531795946890287
06/07/2024 01:51:26 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:51:26 - INFO - utils.utils -   using task specific params for mtop: {'max_length': 128}
06/07/2024 01:52:00 - INFO - third_party.trainers.t5_trainer -     mtop_eval_loss = 1025.145751953125
06/07/2024 01:52:00 - INFO - third_party.trainers.t5_trainer -     mtop_eval_micro_f1_score = 0.8432010409889394
06/07/2024 01:52:00 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:52:00 - INFO - utils.utils -   ***** val metrics *****
06/07/2024 01:52:00 - INFO - utils.utils -     atis_eval_loss = 1578.7830810546875
06/07/2024 01:52:00 - INFO - utils.utils -     atis_eval_micro_f1_score = 0.964336661911555
06/07/2024 01:52:00 - INFO - utils.utils -     eval_average_metrics = 0.8536295767035108
06/07/2024 01:52:00 - INFO - utils.utils -     movieTrivia_eval_loss = 1609.187255859375
06/07/2024 01:52:00 - INFO - utils.utils -     movieTrivia_eval_micro_f1_score = 0.6666666666666666
06/07/2024 01:52:00 - INFO - utils.utils -     movie_eval_loss = 961.4859008789062
06/07/2024 01:52:00 - INFO - utils.utils -     movie_eval_micro_f1_score = 0.8621495327102804
06/07/2024 01:52:00 - INFO - utils.utils -     mtod_eval_loss = 760.48681640625
06/07/2024 01:52:00 - INFO - utils.utils -     mtod_eval_micro_f1_score = 0.9531795946890287
06/07/2024 01:52:00 - INFO - utils.utils -     mtop_eval_loss = 1025.145751953125
06/07/2024 01:52:00 - INFO - utils.utils -     mtop_eval_micro_f1_score = 0.8432010409889394
06/07/2024 01:52:00 - INFO - utils.utils -     restaurant_eval_loss = 757.5418090820312
06/07/2024 01:52:00 - INFO - utils.utils -     restaurant_eval_micro_f1_score = 0.7804878048780487
06/07/2024 01:52:00 - INFO - utils.utils -     snips_eval_loss = 670.5992431640625
06/07/2024 01:52:00 - INFO - utils.utils -     snips_eval_micro_f1_score = 0.9053857350800583
06/07/2024 01:52:00 - INFO - utils.utils -   using task specific params for movieTrivia: {'max_length': 128}
06/07/2024 01:54:50 - INFO - third_party.trainers.t5_trainer -     movieTrivia_eval_loss = 2233.97265625
06/07/2024 01:54:50 - INFO - third_party.trainers.t5_trainer -     movieTrivia_eval_micro_f1_score = 0.6862951279355065
06/07/2024 01:54:50 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:54:50 - INFO - utils.utils -   using task specific params for movie: {'max_length': 128}
06/07/2024 01:57:55 - INFO - third_party.trainers.t5_trainer -     movie_eval_loss = 1252.812744140625
06/07/2024 01:57:55 - INFO - third_party.trainers.t5_trainer -     movie_eval_micro_f1_score = 0.8546132339235788
06/07/2024 01:57:55 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:57:55 - INFO - utils.utils -   using task specific params for restaurant: {'max_length': 128}
06/07/2024 01:59:41 - INFO - third_party.trainers.t5_trainer -     restaurant_eval_loss = 1176.858642578125
06/07/2024 01:59:41 - INFO - third_party.trainers.t5_trainer -     restaurant_eval_micro_f1_score = 0.7600563468461419
06/07/2024 01:59:41 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 01:59:41 - INFO - utils.utils -   using task specific params for atis: {'max_length': 128}
06/07/2024 02:01:01 - INFO - third_party.trainers.t5_trainer -     atis_eval_loss = 1757.941162109375
06/07/2024 02:01:01 - INFO - third_party.trainers.t5_trainer -     atis_eval_micro_f1_score = 0.9401408450704226
06/07/2024 02:01:01 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 02:01:01 - INFO - utils.utils -   using task specific params for snips: {'max_length': 128}
06/07/2024 02:01:53 - INFO - third_party.trainers.t5_trainer -     snips_eval_loss = 1067.5064697265625
06/07/2024 02:01:53 - INFO - third_party.trainers.t5_trainer -     snips_eval_micro_f1_score = 0.9237618252643294
06/07/2024 02:01:53 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 02:01:53 - INFO - utils.utils -   using task specific params for mtod: {'max_length': 128}
06/07/2024 02:10:46 - INFO - third_party.trainers.t5_trainer -     mtod_eval_loss = 788.9430541992188
06/07/2024 02:10:46 - INFO - third_party.trainers.t5_trainer -     mtod_eval_micro_f1_score = 0.9540847561390382
06/07/2024 02:10:46 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 02:10:46 - INFO - utils.utils -   using task specific params for mtop: {'max_length': 128}
06/07/2024 02:16:11 - INFO - third_party.trainers.t5_trainer -     mtop_eval_loss = 1145.8824462890625
06/07/2024 02:16:11 - INFO - third_party.trainers.t5_trainer -     mtop_eval_micro_f1_score = 0.8473989209351895
06/07/2024 02:16:11 - INFO - utils.utils -   config is reset to the initial values.
06/07/2024 02:16:11 - INFO - utils.utils -   ***** test metrics *****
06/07/2024 02:16:11 - INFO - utils.utils -     atis_eval_loss = 1757.941162109375
06/07/2024 02:16:11 - INFO - utils.utils -     atis_eval_micro_f1_score = 0.9401408450704226
06/07/2024 02:16:11 - INFO - utils.utils -     eval_average_metrics = 0.8523358651591725
06/07/2024 02:16:11 - INFO - utils.utils -     movieTrivia_eval_loss = 2233.97265625
06/07/2024 02:16:11 - INFO - utils.utils -     movieTrivia_eval_micro_f1_score = 0.6862951279355065
06/07/2024 02:16:11 - INFO - utils.utils -     movie_eval_loss = 1252.812744140625
06/07/2024 02:16:11 - INFO - utils.utils -     movie_eval_micro_f1_score = 0.8546132339235788
06/07/2024 02:16:11 - INFO - utils.utils -     mtod_eval_loss = 788.9430541992188
06/07/2024 02:16:11 - INFO - utils.utils -     mtod_eval_micro_f1_score = 0.9540847561390382
06/07/2024 02:16:11 - INFO - utils.utils -     mtop_eval_loss = 1145.8824462890625
06/07/2024 02:16:11 - INFO - utils.utils -     mtop_eval_micro_f1_score = 0.8473989209351895
06/07/2024 02:16:11 - INFO - utils.utils -     restaurant_eval_loss = 1176.858642578125
06/07/2024 02:16:11 - INFO - utils.utils -     restaurant_eval_micro_f1_score = 0.7600563468461419
06/07/2024 02:16:11 - INFO - utils.utils -     snips_eval_loss = 1067.5064697265625
06/07/2024 02:16:11 - INFO - utils.utils -     snips_eval_micro_f1_score = 0.9237618252643294
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
